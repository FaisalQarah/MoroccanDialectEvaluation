{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d804ae66-9435-44be-8aad-beacbdeec0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-18 15:01:49.541536: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-18 15:01:49.728668: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-18 15:01:50.395873: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Twits', 'label'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Twits</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>what happens</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>😞😞</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ان شاء الله</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>بالتوفيق ان شاء الله</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 Twits label\n",
       "0           0          what happens   neu\n",
       "1           1                    😞😞   neg\n",
       "2           2           ان شاء الله   neu\n",
       "3           3  بالتوفيق ان شاء الله   pos"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Index(['Twits', 'dialect'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Twits</th>\n",
       "      <th>dialect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13393</th>\n",
       "      <td>slawiya msikina tb9a f darha 😐😕😕</td>\n",
       "      <td>Morocco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13394</th>\n",
       "      <td>ناس د الرباط أش واقع عندكم تما ... ياكما شي شبه عائلية 😐</td>\n",
       "      <td>Morocco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13395</th>\n",
       "      <td>لي دوا مشاا 😂😂</td>\n",
       "      <td>Morocco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13396</th>\n",
       "      <td>: وزراء تقلقوا</td>\n",
       "      <td>Morocco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          Twits  dialect\n",
       "13393                          slawiya msikina tb9a f darha 😐😕😕  Morocco\n",
       "13394  ناس د الرباط أش واقع عندكم تما ... ياكما شي شبه عائلية 😐  Morocco\n",
       "13395                                            لي دوا مشاا 😂😂  Morocco\n",
       "13396                                            : وزراء تقلقوا  Morocco"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9965"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Twits</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ههههههههه</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>هههه ضروري بإذن الله , لفقر را عيب ؤ عار ؤ حشومة</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>مع فاق كيم جون من المرض وهو اكتب لينا مشروع قانون شكرا كيم جون ، عاشت كوريا</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>الوادافاكية: زوجة اليوسفي تعزي جلالة الملك في وفاة زوجها</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>: ها علاش مكنقراوش للقصص للاطفال قبل النوم😂😂😂</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0   \n",
       "8            8  \\\n",
       "9            9   \n",
       "10          10   \n",
       "11          11   \n",
       "12          12   \n",
       "\n",
       "                                                                          Twits   \n",
       "8                                                                     ههههههههه  \\\n",
       "9                              هههه ضروري بإذن الله , لفقر را عيب ؤ عار ؤ حشومة   \n",
       "10  مع فاق كيم جون من المرض وهو اكتب لينا مشروع قانون شكرا كيم جون ، عاشت كوريا   \n",
       "11                     الوادافاكية: زوجة اليوسفي تعزي جلالة الملك في وفاة زوجها   \n",
       "12                                : ها علاش مكنقراوش للقصص للاطفال قبل النوم😂😂😂   \n",
       "\n",
       "   label  \n",
       "8    pos  \n",
       "9    neg  \n",
       "10   pos  \n",
       "11   neg  \n",
       "12   neu  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8520"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "neu    6923\n",
       "neg    1200\n",
       "pos     397\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'neg', 'neu', 'pos'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "8520"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 6816\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 1704\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:03, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.889100</td>\n",
       "      <td>0.857410</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.643600</td>\n",
       "      <td>0.603907</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.498700</td>\n",
       "      <td>0.439402</td>\n",
       "      <td>0.867958</td>\n",
       "      <td>0.519924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.422500</td>\n",
       "      <td>0.406103</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.523945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.346900</td>\n",
       "      <td>0.355258</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.544987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.289600</td>\n",
       "      <td>0.349073</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.645420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.255500</td>\n",
       "      <td>0.355938</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.693030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.197800</td>\n",
       "      <td>0.349968</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.702558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.172900</td>\n",
       "      <td>0.377242</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.704766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.375217</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.698224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.133300</td>\n",
       "      <td>0.412630</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.709419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.431705</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.721792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.112400</td>\n",
       "      <td>0.457575</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.716342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.475982</td>\n",
       "      <td>0.872066</td>\n",
       "      <td>0.721355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.451720</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.719404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.480375</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.735773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.055200</td>\n",
       "      <td>0.498874</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.709877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.501515</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.735305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.518227</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.729681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.544576</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.704876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.039000</td>\n",
       "      <td>0.528575</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.719959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.528632</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.718392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.540474</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.720254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.554226</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.712184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.548715</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.714224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:04, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.869500</td>\n",
       "      <td>0.669616</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>0.479465</td>\n",
       "      <td>0.843310</td>\n",
       "      <td>0.453963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.427100</td>\n",
       "      <td>0.416493</td>\n",
       "      <td>0.857394</td>\n",
       "      <td>0.529053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.387100</td>\n",
       "      <td>0.870892</td>\n",
       "      <td>0.530265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.482104</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.373062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.550058</td>\n",
       "      <td>0.819836</td>\n",
       "      <td>0.383508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.513300</td>\n",
       "      <td>0.727582</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.729891</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.484600</td>\n",
       "      <td>0.682203</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.452900</td>\n",
       "      <td>0.640700</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.655356</td>\n",
       "      <td>0.806338</td>\n",
       "      <td>0.297596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.612818</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.430200</td>\n",
       "      <td>0.535417</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.513770</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.417600</td>\n",
       "      <td>0.502590</td>\n",
       "      <td>0.806338</td>\n",
       "      <td>0.297693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.461553</td>\n",
       "      <td>0.802230</td>\n",
       "      <td>0.297239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.361800</td>\n",
       "      <td>0.465290</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.300094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.456899</td>\n",
       "      <td>0.803404</td>\n",
       "      <td>0.297286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.438646</td>\n",
       "      <td>0.804577</td>\n",
       "      <td>0.297429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.452508</td>\n",
       "      <td>0.804577</td>\n",
       "      <td>0.305850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.302300</td>\n",
       "      <td>0.445520</td>\n",
       "      <td>0.807512</td>\n",
       "      <td>0.306393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>0.444144</td>\n",
       "      <td>0.807512</td>\n",
       "      <td>0.306125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.436822</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>0.528315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>0.426824</td>\n",
       "      <td>0.859742</td>\n",
       "      <td>0.525279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.431303</td>\n",
       "      <td>0.859742</td>\n",
       "      <td>0.524606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:03, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.869500</td>\n",
       "      <td>0.669616</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.567400</td>\n",
       "      <td>0.479465</td>\n",
       "      <td>0.843310</td>\n",
       "      <td>0.453963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.427100</td>\n",
       "      <td>0.416493</td>\n",
       "      <td>0.857394</td>\n",
       "      <td>0.529053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.401900</td>\n",
       "      <td>0.387100</td>\n",
       "      <td>0.870892</td>\n",
       "      <td>0.530265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.427500</td>\n",
       "      <td>0.482104</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.373062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.550058</td>\n",
       "      <td>0.819836</td>\n",
       "      <td>0.383508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.513300</td>\n",
       "      <td>0.727582</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>0.729891</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.484600</td>\n",
       "      <td>0.682203</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.452900</td>\n",
       "      <td>0.640700</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.440800</td>\n",
       "      <td>0.655356</td>\n",
       "      <td>0.806338</td>\n",
       "      <td>0.297596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.612818</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.430200</td>\n",
       "      <td>0.535417</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.513770</td>\n",
       "      <td>0.806925</td>\n",
       "      <td>0.297716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.417600</td>\n",
       "      <td>0.502590</td>\n",
       "      <td>0.806338</td>\n",
       "      <td>0.297693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>0.461553</td>\n",
       "      <td>0.802230</td>\n",
       "      <td>0.297239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.361800</td>\n",
       "      <td>0.465290</td>\n",
       "      <td>0.802817</td>\n",
       "      <td>0.300094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.456899</td>\n",
       "      <td>0.803404</td>\n",
       "      <td>0.297286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.438646</td>\n",
       "      <td>0.804577</td>\n",
       "      <td>0.297429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.452508</td>\n",
       "      <td>0.804577</td>\n",
       "      <td>0.305850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.302300</td>\n",
       "      <td>0.445520</td>\n",
       "      <td>0.807512</td>\n",
       "      <td>0.306393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>0.444144</td>\n",
       "      <td>0.807512</td>\n",
       "      <td>0.306125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.436822</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>0.528315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>0.426824</td>\n",
       "      <td>0.859742</td>\n",
       "      <td>0.525279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.431303</td>\n",
       "      <td>0.859742</td>\n",
       "      <td>0.524606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:03, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.877300</td>\n",
       "      <td>0.686404</td>\n",
       "      <td>0.789319</td>\n",
       "      <td>0.294085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.479641</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>0.524055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.445180</td>\n",
       "      <td>0.862089</td>\n",
       "      <td>0.517453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.366980</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.554513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>0.386369</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.543415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>0.373368</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.545357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.421323</td>\n",
       "      <td>0.866784</td>\n",
       "      <td>0.526831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.378798</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.669191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.412622</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.666938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.687443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>0.398107</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.668891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.462416</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.668475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.426279</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.720166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.466575</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.677036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>0.478969</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.694482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.447583</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.708049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.538513</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.679847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.450242</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.711648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.535781</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.681731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.553451</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.689978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.571750</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.680189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.514081</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.690748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.526953</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.697131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.562722</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.681222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.541348</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.697949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:03, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.877300</td>\n",
       "      <td>0.686404</td>\n",
       "      <td>0.789319</td>\n",
       "      <td>0.294085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.479641</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>0.524055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.445180</td>\n",
       "      <td>0.862089</td>\n",
       "      <td>0.517453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.366980</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.554513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>0.386369</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.543415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>0.373368</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.545357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.421323</td>\n",
       "      <td>0.866784</td>\n",
       "      <td>0.526831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.378798</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.669191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.412622</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.666938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.687443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>0.398107</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.668891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.462416</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.668475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.426279</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.720166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.466575</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.677036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>0.478969</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.694482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.447583</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.708049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.538513</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.679847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.450242</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.711648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.535781</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.681731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.553451</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.689978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.571750</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.680189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.514081</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.690748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.526953</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.697131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.562722</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.681222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.541348</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.697949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:03, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.877300</td>\n",
       "      <td>0.686404</td>\n",
       "      <td>0.789319</td>\n",
       "      <td>0.294085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.523900</td>\n",
       "      <td>0.479641</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>0.524055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.445180</td>\n",
       "      <td>0.862089</td>\n",
       "      <td>0.517453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.366980</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.554513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>0.386369</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.543415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>0.373368</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.545357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.421323</td>\n",
       "      <td>0.866784</td>\n",
       "      <td>0.526831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.378798</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.669191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.412622</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.666938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.194400</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.687443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.161800</td>\n",
       "      <td>0.398107</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.668891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.462416</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.668475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.426279</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.720166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.466575</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.677036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.106500</td>\n",
       "      <td>0.478969</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.694482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.447583</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.708049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.081700</td>\n",
       "      <td>0.538513</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.679847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.076000</td>\n",
       "      <td>0.450242</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.711648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.535781</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.681731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.553451</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.689978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.571750</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.680189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.514081</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.690748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.058600</td>\n",
       "      <td>0.526953</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.697131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.562722</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.681222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.053800</td>\n",
       "      <td>0.541348</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.697949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.672155</td>\n",
       "      <td>0.808685</td>\n",
       "      <td>0.298075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.455994</td>\n",
       "      <td>0.869718</td>\n",
       "      <td>0.529325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.405100</td>\n",
       "      <td>0.380221</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.512531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.351707</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.551294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.354009</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.549024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.378109</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.515274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.375711</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.649807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.333038</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.736059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.365533</td>\n",
       "      <td>0.894953</td>\n",
       "      <td>0.732920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.362827</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.717293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.381047</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.725248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.408898</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.717796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.425918</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.714405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.441823</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.705093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>0.444360</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.711366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.725536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.484030</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.703833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.494830</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.714072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.486876</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.724416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.494897</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.721649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.501822</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.729384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.523106</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.718406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.518559</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.715212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.514217</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.719548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.518157</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.718708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682f12278d4a4c8fb74e3c6ddf7974ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dbc99391f14beaa87933797c2cc8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.672155</td>\n",
       "      <td>0.808685</td>\n",
       "      <td>0.298075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.455994</td>\n",
       "      <td>0.869718</td>\n",
       "      <td>0.529325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.405100</td>\n",
       "      <td>0.380221</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.512531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.351707</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.551294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.354009</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.549024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.378109</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.515274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.375711</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.649807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.333038</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.736059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.365533</td>\n",
       "      <td>0.894953</td>\n",
       "      <td>0.732920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.362827</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.717293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.381047</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.725248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.408898</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.717796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.425918</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.714405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.441823</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.705093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>0.444360</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.711366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.725536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.484030</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.703833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.494830</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.714072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.486876</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.724416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.494897</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.721649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.501822</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.729384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.523106</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.718406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.518559</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.715212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.514217</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.719548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.518157</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.718708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56925c295f854ade88148587a78689ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626c7f92fd694f8e90e9f2cce69bc378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.672155</td>\n",
       "      <td>0.808685</td>\n",
       "      <td>0.298075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.556200</td>\n",
       "      <td>0.455994</td>\n",
       "      <td>0.869718</td>\n",
       "      <td>0.529325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.405100</td>\n",
       "      <td>0.380221</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.512531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.376000</td>\n",
       "      <td>0.351707</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.551294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.354009</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.549024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>0.378109</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.515274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.375711</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.649807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.214800</td>\n",
       "      <td>0.333038</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.736059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.170600</td>\n",
       "      <td>0.365533</td>\n",
       "      <td>0.894953</td>\n",
       "      <td>0.732920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.362827</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.717293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.381047</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.725248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.108000</td>\n",
       "      <td>0.408898</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.717796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.425918</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.714405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.441823</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.705093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>0.444360</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.711366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.064200</td>\n",
       "      <td>0.463800</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.725536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.484030</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.703833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.494830</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.714072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.060400</td>\n",
       "      <td>0.486876</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.724416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.494897</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.721649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.501822</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.729384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.523106</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.718406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.036900</td>\n",
       "      <td>0.518559</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.715212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.514217</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.719548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.044200</td>\n",
       "      <td>0.518157</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.718708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 6816\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 1704\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9693f1ad7f4666b719ed69410a1bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa47148fa844fd8932eb348c04cf837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.382700</td>\n",
       "      <td>0.328438</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.692269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>0.311825</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.661266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.353029</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.722167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.344856</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.677296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.493757</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.718162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.428568</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.716334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.515876</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.692981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.625575</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.724263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.603178</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.711882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.621494</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.727564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.675010</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.734416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.780463</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.723440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.718863</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.733323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.723997</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.738216</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.698164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.759619</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.713616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.764288</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.715443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.772050</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.713863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.781887</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.719489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.774230</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.719116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.779240</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.719692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.785258</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.720267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.790481</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.718844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.797051</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.723056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.799431</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.722449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc699c44576f425ea8998c405c5efbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72931cc90174ac49956a06a2b9d811c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.382700</td>\n",
       "      <td>0.328438</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.692269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>0.311825</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.661266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.353029</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.722167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.344856</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.677296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.493757</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.718162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.428568</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.716334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.515876</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.692981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.625575</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.724263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.603178</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.711882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.621494</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.727564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.675010</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.734416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.780463</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.723440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.718863</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.733323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.723997</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.738216</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.698164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.759619</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.713616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.764288</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.715443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.772050</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.713863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.781887</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.719489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.774230</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.719116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.779240</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.719692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.785258</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.720267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.790481</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.718844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.797051</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.723056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.799431</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.722449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dab214d4e3b4ea1b1cacde21184b204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28d9e7af6e147bcb1f04aa7289b4f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.382700</td>\n",
       "      <td>0.328438</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.692269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.302100</td>\n",
       "      <td>0.311825</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.661266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.353029</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.722167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.344856</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.677296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.111800</td>\n",
       "      <td>0.493757</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.718162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.428568</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.716334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.515876</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.692981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.625575</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.724263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.603178</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.711882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.621494</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.727564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.675010</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.734416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.780463</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.723440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.718863</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.733323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.723997</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.738216</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.698164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.759619</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.713616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.764288</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.715443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.772050</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.713863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.781887</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.719489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.774230</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.719116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.779240</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.719692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.785258</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.720267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.790481</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.718844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.797051</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.723056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.799431</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.722449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf7567d000945198abc1f877e2df4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fddf679ab5409aba1c6f6fa5b06211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.307945</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.715055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.315232</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.674527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.305542</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.719513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.178700</td>\n",
       "      <td>0.314741</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.435636</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.702117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.392199</td>\n",
       "      <td>0.893779</td>\n",
       "      <td>0.737604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.482718</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.720753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.521281</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.731974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.506159</td>\n",
       "      <td>0.900235</td>\n",
       "      <td>0.747883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.570948</td>\n",
       "      <td>0.896714</td>\n",
       "      <td>0.733282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.585222</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.722682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.645918</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.732463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.616688</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.742609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.626070</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.742452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.636798</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.743004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.684331</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.728113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.675811</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.740143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.691488</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.741833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.694413</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.738627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.688949</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.737310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.695679</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.740670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.694825</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.739280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.696704</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.735660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.700215</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.734278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.700738</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.733805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b290a0f5b6024d8dbed35c23ece58763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16eb50faec34ec992cf7cdee9f6e0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.307945</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.715055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.315232</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.674527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.305542</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.719513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.178700</td>\n",
       "      <td>0.314741</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.435636</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.702117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.392199</td>\n",
       "      <td>0.893779</td>\n",
       "      <td>0.737604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.482718</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.720753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.521281</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.731974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.506159</td>\n",
       "      <td>0.900235</td>\n",
       "      <td>0.747883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.570948</td>\n",
       "      <td>0.896714</td>\n",
       "      <td>0.733282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.585222</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.722682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.645918</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.732463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.616688</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.742609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.626070</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.742452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.636798</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.743004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.684331</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.728113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.675811</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.740143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.691488</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.741833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.694413</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.738627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.688949</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.737310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.695679</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.740670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.694825</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.739280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.696704</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.735660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.700215</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.734278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.700738</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.733805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041ce8a9d5de4b07bd59730e2d2a499f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd76d9275704358b993f21f0682c9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.307945</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.715055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.315232</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.674527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.225400</td>\n",
       "      <td>0.305542</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.719513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.178700</td>\n",
       "      <td>0.314741</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.122000</td>\n",
       "      <td>0.435636</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.702117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.392199</td>\n",
       "      <td>0.893779</td>\n",
       "      <td>0.737604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.482718</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.720753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.521281</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.731974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.506159</td>\n",
       "      <td>0.900235</td>\n",
       "      <td>0.747883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.570948</td>\n",
       "      <td>0.896714</td>\n",
       "      <td>0.733282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.585222</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.722682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.645918</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.732463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.616688</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.742609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.626070</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.742452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.636798</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.743004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.684331</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.728113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.675811</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.740143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.691488</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.741833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.694413</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.738627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.688949</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.737310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.695679</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.740670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.694825</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.739280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.696704</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.735660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.700215</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.734278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.700738</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.733805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e47e6b89d44189895cedd47ca1f664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76397a03de840d2852ff9a38c40cd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>0.298948</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.725839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.301646</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.738188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.286124</td>\n",
       "      <td>0.900822</td>\n",
       "      <td>0.754651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.287267</td>\n",
       "      <td>0.900235</td>\n",
       "      <td>0.746493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.405341</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.759349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>0.391175</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.726949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.465747</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.719234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.459530</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.732349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.539348</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.730044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.562580</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.724591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.600555</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.721426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.590580</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.751510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.594191</td>\n",
       "      <td>0.894953</td>\n",
       "      <td>0.728451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.606915</td>\n",
       "      <td>0.899648</td>\n",
       "      <td>0.746358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.619440</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.731416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.641301</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.731466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.628408</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.732795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.635722</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.743723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.668672</td>\n",
       "      <td>0.899648</td>\n",
       "      <td>0.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.646407</td>\n",
       "      <td>0.900235</td>\n",
       "      <td>0.744226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.665019</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.737528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.663308</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.665834</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.738523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.667809</td>\n",
       "      <td>0.899061</td>\n",
       "      <td>0.740140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.669150</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.737565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed60979a280040eea9c3b6f5d1d6d215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68ccd4c318946e8966a94bb9c285c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>0.298948</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.725839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.301646</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.738188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.286124</td>\n",
       "      <td>0.900822</td>\n",
       "      <td>0.754651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.287267</td>\n",
       "      <td>0.900235</td>\n",
       "      <td>0.746493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.405341</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.759349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>0.391175</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.726949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.465747</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.719234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.459530</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.732349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.539348</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.730044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.562580</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.724591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.600555</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.721426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.590580</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.751510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.594191</td>\n",
       "      <td>0.894953</td>\n",
       "      <td>0.728451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.606915</td>\n",
       "      <td>0.899648</td>\n",
       "      <td>0.746358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.619440</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.731416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.641301</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.731466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.628408</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.732795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.635722</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.743723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.668672</td>\n",
       "      <td>0.899648</td>\n",
       "      <td>0.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.646407</td>\n",
       "      <td>0.900235</td>\n",
       "      <td>0.744226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.665019</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.737528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.663308</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.665834</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.738523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.667809</td>\n",
       "      <td>0.899061</td>\n",
       "      <td>0.740140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.669150</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.737565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd7385780a74f9f87eb5e3b250a2f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c2925b238b45e7b6f1ec27475c81cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>0.298948</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.725839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.284300</td>\n",
       "      <td>0.301646</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.738188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.286124</td>\n",
       "      <td>0.900822</td>\n",
       "      <td>0.754651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.287267</td>\n",
       "      <td>0.900235</td>\n",
       "      <td>0.746493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.109700</td>\n",
       "      <td>0.405341</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.759349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>0.391175</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.726949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.069100</td>\n",
       "      <td>0.465747</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.719234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.037200</td>\n",
       "      <td>0.459530</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.732349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.539348</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.730044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.562580</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.724591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.600555</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.721426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.590580</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.751510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.594191</td>\n",
       "      <td>0.894953</td>\n",
       "      <td>0.728451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.606915</td>\n",
       "      <td>0.899648</td>\n",
       "      <td>0.746358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.619440</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.731416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.641301</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.731466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.628408</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.732795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.635722</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.743723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.668672</td>\n",
       "      <td>0.899648</td>\n",
       "      <td>0.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.646407</td>\n",
       "      <td>0.900235</td>\n",
       "      <td>0.744226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.665019</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.737528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.663308</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.665834</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.738523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.667809</td>\n",
       "      <td>0.899061</td>\n",
       "      <td>0.740140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.669150</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.737565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 6816\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 1704\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09881585c0254c399a2ecfec9f501919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced331943dca4a719e647c6cd54da162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.448443</td>\n",
       "      <td>0.849765</td>\n",
       "      <td>0.517302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.414336</td>\n",
       "      <td>0.860915</td>\n",
       "      <td>0.531728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.385700</td>\n",
       "      <td>0.394354</td>\n",
       "      <td>0.864437</td>\n",
       "      <td>0.534839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.398807</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.532826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>0.377105</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.529419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.402566</td>\n",
       "      <td>0.850939</td>\n",
       "      <td>0.549078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.363859</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.628934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.367512</td>\n",
       "      <td>0.867958</td>\n",
       "      <td>0.542067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.369355</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.568069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.353012</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.665578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>0.344340</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.668328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.340941</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.649178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.348526</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.665645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.281800</td>\n",
       "      <td>0.341532</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.682487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.348990</td>\n",
       "      <td>0.870892</td>\n",
       "      <td>0.675046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.362356</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.680010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.381298</td>\n",
       "      <td>0.864437</td>\n",
       "      <td>0.645971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.651551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.354796</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.662029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.355942</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.660360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.373846</td>\n",
       "      <td>0.872066</td>\n",
       "      <td>0.655703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.370742</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.663230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.364193</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.664249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.370995</td>\n",
       "      <td>0.866784</td>\n",
       "      <td>0.658817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04280cac073c4ea5adf08118f85111b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93d5b5bdbd24e2c846fd46602d61e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.448443</td>\n",
       "      <td>0.849765</td>\n",
       "      <td>0.517302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.414336</td>\n",
       "      <td>0.860915</td>\n",
       "      <td>0.531728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.385700</td>\n",
       "      <td>0.394354</td>\n",
       "      <td>0.864437</td>\n",
       "      <td>0.534839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.398807</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.532826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>0.377105</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.529419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.402566</td>\n",
       "      <td>0.850939</td>\n",
       "      <td>0.549078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.363859</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.628934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.367512</td>\n",
       "      <td>0.867958</td>\n",
       "      <td>0.542067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.369355</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.568069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.353012</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.665578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>0.344340</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.668328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.340941</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.649178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.348526</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.665645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.281800</td>\n",
       "      <td>0.341532</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.682487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.348990</td>\n",
       "      <td>0.870892</td>\n",
       "      <td>0.675046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.362356</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.680010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.381298</td>\n",
       "      <td>0.864437</td>\n",
       "      <td>0.645971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.651551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.354796</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.662029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.355942</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.660360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.373846</td>\n",
       "      <td>0.872066</td>\n",
       "      <td>0.655703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.370742</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.663230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.364193</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.664249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.370995</td>\n",
       "      <td>0.866784</td>\n",
       "      <td>0.658817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfc8322d77b4f6d83c6ab9637a42f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d11430f51b4c59a2198d2e47551993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.579800</td>\n",
       "      <td>0.448443</td>\n",
       "      <td>0.849765</td>\n",
       "      <td>0.517302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.414336</td>\n",
       "      <td>0.860915</td>\n",
       "      <td>0.531728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.385700</td>\n",
       "      <td>0.394354</td>\n",
       "      <td>0.864437</td>\n",
       "      <td>0.534839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.410200</td>\n",
       "      <td>0.398807</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.532826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.372100</td>\n",
       "      <td>0.377105</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.529419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.340600</td>\n",
       "      <td>0.402566</td>\n",
       "      <td>0.850939</td>\n",
       "      <td>0.549078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.369100</td>\n",
       "      <td>0.363859</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.628934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.348500</td>\n",
       "      <td>0.367512</td>\n",
       "      <td>0.867958</td>\n",
       "      <td>0.542067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.369355</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.568069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.353012</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.665578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>0.344340</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.668328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.280200</td>\n",
       "      <td>0.340941</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.368200</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.649178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.260200</td>\n",
       "      <td>0.348526</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.665645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.281800</td>\n",
       "      <td>0.341532</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.682487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.348990</td>\n",
       "      <td>0.870892</td>\n",
       "      <td>0.675046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.265900</td>\n",
       "      <td>0.362356</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.680010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.381298</td>\n",
       "      <td>0.864437</td>\n",
       "      <td>0.645971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.381977</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.651551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.238700</td>\n",
       "      <td>0.354796</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.662029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>0.355942</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.660360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.208800</td>\n",
       "      <td>0.373846</td>\n",
       "      <td>0.872066</td>\n",
       "      <td>0.655703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.194800</td>\n",
       "      <td>0.370742</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.663230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.364193</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.664249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.184100</td>\n",
       "      <td>0.370995</td>\n",
       "      <td>0.866784</td>\n",
       "      <td>0.658817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3493cd479748f38f01542d34b8f44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873b9d96f6ff4d5fb0b84f74aaa5a5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.544300</td>\n",
       "      <td>0.420295</td>\n",
       "      <td>0.863263</td>\n",
       "      <td>0.530884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.500606</td>\n",
       "      <td>0.795775</td>\n",
       "      <td>0.500007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>0.439226</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.537581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.443067</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.400243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.399601</td>\n",
       "      <td>0.863850</td>\n",
       "      <td>0.526229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.467465</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>0.543390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.382630</td>\n",
       "      <td>0.865023</td>\n",
       "      <td>0.604724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.374783</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.545323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.398964</td>\n",
       "      <td>0.851526</td>\n",
       "      <td>0.581264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.353206</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.626709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.359912</td>\n",
       "      <td>0.872066</td>\n",
       "      <td>0.651394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.287200</td>\n",
       "      <td>0.352999</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.653912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.290400</td>\n",
       "      <td>0.381863</td>\n",
       "      <td>0.858568</td>\n",
       "      <td>0.635449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.373836</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.661427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.383799</td>\n",
       "      <td>0.853286</td>\n",
       "      <td>0.656805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.397645</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.673245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.382222</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.666948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.230100</td>\n",
       "      <td>0.383002</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.643617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.409131</td>\n",
       "      <td>0.860329</td>\n",
       "      <td>0.643218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>0.407381</td>\n",
       "      <td>0.860329</td>\n",
       "      <td>0.661333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.399962</td>\n",
       "      <td>0.857981</td>\n",
       "      <td>0.624048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>0.397245</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.649692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.415405</td>\n",
       "      <td>0.848592</td>\n",
       "      <td>0.644614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.434477</td>\n",
       "      <td>0.863263</td>\n",
       "      <td>0.639663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.183800</td>\n",
       "      <td>0.418492</td>\n",
       "      <td>0.857981</td>\n",
       "      <td>0.645408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cc47e344844aa59a0c3736174e1d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b591947124b449f91a42420d04ecaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.544300</td>\n",
       "      <td>0.420295</td>\n",
       "      <td>0.863263</td>\n",
       "      <td>0.530884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.500606</td>\n",
       "      <td>0.795775</td>\n",
       "      <td>0.500007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>0.439226</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.537581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.443067</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.400243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.399601</td>\n",
       "      <td>0.863850</td>\n",
       "      <td>0.526229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.467465</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>0.543390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.382630</td>\n",
       "      <td>0.865023</td>\n",
       "      <td>0.604724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.374783</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.545323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.398964</td>\n",
       "      <td>0.851526</td>\n",
       "      <td>0.581264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.353206</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.626709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.359912</td>\n",
       "      <td>0.872066</td>\n",
       "      <td>0.651394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.287200</td>\n",
       "      <td>0.352999</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.653912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.290400</td>\n",
       "      <td>0.381863</td>\n",
       "      <td>0.858568</td>\n",
       "      <td>0.635449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.373836</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.661427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.383799</td>\n",
       "      <td>0.853286</td>\n",
       "      <td>0.656805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.397645</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.673245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.382222</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.666948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.230100</td>\n",
       "      <td>0.383002</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.643617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.409131</td>\n",
       "      <td>0.860329</td>\n",
       "      <td>0.643218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>0.407381</td>\n",
       "      <td>0.860329</td>\n",
       "      <td>0.661333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.399962</td>\n",
       "      <td>0.857981</td>\n",
       "      <td>0.624048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>0.397245</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.649692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.415405</td>\n",
       "      <td>0.848592</td>\n",
       "      <td>0.644614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.434477</td>\n",
       "      <td>0.863263</td>\n",
       "      <td>0.639663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.183800</td>\n",
       "      <td>0.418492</td>\n",
       "      <td>0.857981</td>\n",
       "      <td>0.645408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dec086fba4408c9ee37a8712c512a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cd0c1a31d9462c82f80517ee347852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.544300</td>\n",
       "      <td>0.420295</td>\n",
       "      <td>0.863263</td>\n",
       "      <td>0.530884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.397500</td>\n",
       "      <td>0.500606</td>\n",
       "      <td>0.795775</td>\n",
       "      <td>0.500007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>0.439226</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.537581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.382400</td>\n",
       "      <td>0.443067</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.400243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.379700</td>\n",
       "      <td>0.399601</td>\n",
       "      <td>0.863850</td>\n",
       "      <td>0.526229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.363500</td>\n",
       "      <td>0.467465</td>\n",
       "      <td>0.862676</td>\n",
       "      <td>0.543390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.367600</td>\n",
       "      <td>0.382630</td>\n",
       "      <td>0.865023</td>\n",
       "      <td>0.604724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.352100</td>\n",
       "      <td>0.374783</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.545323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.305600</td>\n",
       "      <td>0.398964</td>\n",
       "      <td>0.851526</td>\n",
       "      <td>0.581264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.329700</td>\n",
       "      <td>0.353206</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.626709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.289400</td>\n",
       "      <td>0.359912</td>\n",
       "      <td>0.872066</td>\n",
       "      <td>0.651394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.287200</td>\n",
       "      <td>0.352999</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.653912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.290400</td>\n",
       "      <td>0.381863</td>\n",
       "      <td>0.858568</td>\n",
       "      <td>0.635449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.281100</td>\n",
       "      <td>0.373836</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.661427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.383799</td>\n",
       "      <td>0.853286</td>\n",
       "      <td>0.656805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.249500</td>\n",
       "      <td>0.397645</td>\n",
       "      <td>0.868545</td>\n",
       "      <td>0.673245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.254500</td>\n",
       "      <td>0.382222</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.666948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.230100</td>\n",
       "      <td>0.383002</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.643617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.241300</td>\n",
       "      <td>0.409131</td>\n",
       "      <td>0.860329</td>\n",
       "      <td>0.643218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>0.407381</td>\n",
       "      <td>0.860329</td>\n",
       "      <td>0.661333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.228100</td>\n",
       "      <td>0.399962</td>\n",
       "      <td>0.857981</td>\n",
       "      <td>0.624048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.208200</td>\n",
       "      <td>0.397245</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.649692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.415405</td>\n",
       "      <td>0.848592</td>\n",
       "      <td>0.644614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.434477</td>\n",
       "      <td>0.863263</td>\n",
       "      <td>0.639663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.183800</td>\n",
       "      <td>0.418492</td>\n",
       "      <td>0.857981</td>\n",
       "      <td>0.645408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079fddb10ed54f1c9ce10362c4e804a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f739933ab87b44da977688e1ab18b08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>0.437086</td>\n",
       "      <td>0.854460</td>\n",
       "      <td>0.526122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.429400</td>\n",
       "      <td>0.437963</td>\n",
       "      <td>0.834507</td>\n",
       "      <td>0.511540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.406300</td>\n",
       "      <td>0.421580</td>\n",
       "      <td>0.833920</td>\n",
       "      <td>0.418469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.363573</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.540720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.395040</td>\n",
       "      <td>0.857394</td>\n",
       "      <td>0.497134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>0.364231</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.536883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.363264</td>\n",
       "      <td>0.861502</td>\n",
       "      <td>0.634207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>0.359598</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.561034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.574737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.384364</td>\n",
       "      <td>0.854460</td>\n",
       "      <td>0.649586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>0.391331</td>\n",
       "      <td>0.848592</td>\n",
       "      <td>0.600132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.335214</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.659107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.357074</td>\n",
       "      <td>0.862089</td>\n",
       "      <td>0.618284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.360422</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.616776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.354990</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.653035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.349021</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.639052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.372405</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.668720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.381736</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.668077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.237800</td>\n",
       "      <td>0.378490</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.649689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.387096</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.649807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.379404</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.663213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.379050</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.657262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.202800</td>\n",
       "      <td>0.382910</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.661858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.190600</td>\n",
       "      <td>0.383151</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.668962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.381661</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.662932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee9dbba650345d89376bf54712dfb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd22151b5f8541ee9bd5d7b1905ffa34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>0.437086</td>\n",
       "      <td>0.854460</td>\n",
       "      <td>0.526122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.429400</td>\n",
       "      <td>0.437963</td>\n",
       "      <td>0.834507</td>\n",
       "      <td>0.511540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.406300</td>\n",
       "      <td>0.421580</td>\n",
       "      <td>0.833920</td>\n",
       "      <td>0.418469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.363573</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.540720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.395040</td>\n",
       "      <td>0.857394</td>\n",
       "      <td>0.497134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>0.364231</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.536883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.363264</td>\n",
       "      <td>0.861502</td>\n",
       "      <td>0.634207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>0.359598</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.561034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.574737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.384364</td>\n",
       "      <td>0.854460</td>\n",
       "      <td>0.649586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>0.391331</td>\n",
       "      <td>0.848592</td>\n",
       "      <td>0.600132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.335214</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.659107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.357074</td>\n",
       "      <td>0.862089</td>\n",
       "      <td>0.618284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.360422</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.616776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.354990</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.653035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.349021</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.639052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.372405</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.668720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.381736</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.668077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.237800</td>\n",
       "      <td>0.378490</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.649689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.387096</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.649807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.379404</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.663213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.379050</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.657262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.202800</td>\n",
       "      <td>0.382910</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.661858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.190600</td>\n",
       "      <td>0.383151</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.668962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.381661</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.662932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d98656a5994724bd183e93abd130e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62522d88122643a9b1d4633b56e51757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:55, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>0.437086</td>\n",
       "      <td>0.854460</td>\n",
       "      <td>0.526122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.429400</td>\n",
       "      <td>0.437963</td>\n",
       "      <td>0.834507</td>\n",
       "      <td>0.511540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.406300</td>\n",
       "      <td>0.421580</td>\n",
       "      <td>0.833920</td>\n",
       "      <td>0.418469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.363573</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.540720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.395040</td>\n",
       "      <td>0.857394</td>\n",
       "      <td>0.497134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.357700</td>\n",
       "      <td>0.364231</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.536883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>0.363264</td>\n",
       "      <td>0.861502</td>\n",
       "      <td>0.634207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.337300</td>\n",
       "      <td>0.359598</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.561034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.338600</td>\n",
       "      <td>0.362500</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.574737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.384364</td>\n",
       "      <td>0.854460</td>\n",
       "      <td>0.649586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.298800</td>\n",
       "      <td>0.391331</td>\n",
       "      <td>0.848592</td>\n",
       "      <td>0.600132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.335214</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.659107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.357074</td>\n",
       "      <td>0.862089</td>\n",
       "      <td>0.618284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.276900</td>\n",
       "      <td>0.360422</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.616776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.354990</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.653035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.267600</td>\n",
       "      <td>0.349021</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.639052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.372405</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.668720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.381736</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.668077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.237800</td>\n",
       "      <td>0.378490</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.649689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.387096</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.649807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.379404</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.663213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.209800</td>\n",
       "      <td>0.379050</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.657262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.202800</td>\n",
       "      <td>0.382910</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.661858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.190600</td>\n",
       "      <td>0.383151</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.668962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.210700</td>\n",
       "      <td>0.381661</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.662932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 6816\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 1704\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bbbf1bddb146d99657b68edbade768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553630dbe891452990a7562583d0257e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:57, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.306917</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.688459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>0.298723</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.713574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.356214</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.712821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.322228</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.691644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.428804</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.731010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.547756</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.693509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.516981</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.706878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.549817</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.735829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.629247</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.744415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.615532</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.747426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.645644</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.716750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.705056</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.730404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.688178</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.722893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.727807</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.726978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.762681</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.726963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.743367</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.720696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.742349</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.716741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.743616</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.736739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.753777</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.717669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.748662</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.755708</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.738902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.762643</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.736340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.766535</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.768058</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.736464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.768407</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.735459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a01be8d9c60d4094aecaf49860177ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a8fd2a26d1484e91c28da9b08afe8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:58, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.306917</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.688459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>0.298723</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.713574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.356214</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.712821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.322228</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.691644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.428804</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.731010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.547756</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.693509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.516981</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.706878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.549817</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.735829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.629247</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.744415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.615532</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.747426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.645644</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.716750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.705056</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.730404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.688178</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.722893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.727807</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.726978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.762681</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.726963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.743367</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.720696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.742349</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.716741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.743616</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.736739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.753777</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.717669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.748662</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.755708</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.738902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.762643</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.736340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.766535</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.768058</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.736464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.768407</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.735459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdeaeb674abd4efb97c1298ac17cef3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807fe7ec64b64a31b8d6360681b0fa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:58, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.306917</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.688459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>0.298723</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.713574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.356214</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.712821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.322228</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.691644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.097600</td>\n",
       "      <td>0.428804</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.731010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.044600</td>\n",
       "      <td>0.547756</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.693509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.046300</td>\n",
       "      <td>0.516981</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.706878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.022800</td>\n",
       "      <td>0.549817</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.735829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.629247</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.744415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.615532</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.747426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.645644</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.716750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.705056</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.730404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.688178</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.722893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.727807</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.726978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.762681</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.726963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.743367</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.720696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.742349</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.716741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.743616</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.736739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.753777</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.717669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.748662</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.755708</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.738902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.762643</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.736340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.766535</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.768058</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.736464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.768407</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.735459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54780b76ea2849fda2b2eb5e498442f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73132b5c8b664aefb9fd734d2329037f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:57, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.302262</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.706477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.298480</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.691339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.339563</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.706094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.376311</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.697689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.483033</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.701527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.523173</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.702102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.601313</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.705336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.604293</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.620250</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.718007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.695194</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.700568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.696578</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.711971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.785430</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.677432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.708331</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.703069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.698091</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.721865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.727379</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.713075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.745323</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.714667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.749440</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.719051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.763512</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.721291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.774214</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.715434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.767894</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.715363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.776038</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.712462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.785634</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.711991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.793539</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.713530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.716097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.797889</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.716097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fcc511fb6084a268bb465afc8db09d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5867d92af14b4578b36d385e26e6e2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:58, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.302262</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.706477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.298480</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.691339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.339563</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.706094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.376311</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.697689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.483033</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.701527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.523173</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.702102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.601313</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.705336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.604293</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.620250</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.718007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.695194</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.700568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.696578</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.711971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.785430</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.677432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.708331</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.703069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.698091</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.721865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.727379</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.713075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.745323</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.714667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.749440</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.719051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.763512</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.721291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.774214</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.715434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.767894</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.715363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.776038</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.712462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.785634</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.711991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.793539</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.713530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.716097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.797889</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.716097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95758ee462ab44f799cdfa04b7dd0078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a371afab111e43b8b761c72f3c5ccbbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:57, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.302262</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.706477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.298480</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.691339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.196600</td>\n",
       "      <td>0.339563</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.706094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.376311</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.697689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.101600</td>\n",
       "      <td>0.483033</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.701527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.523173</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.702102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.050300</td>\n",
       "      <td>0.601313</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.705336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.604293</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.620250</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.718007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.695194</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.700568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.696578</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.711971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>0.785430</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.677432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.708331</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.703069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.698091</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.721865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.727379</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.713075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.745323</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.714667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.749440</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.719051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.763512</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.721291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.774214</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.715434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.767894</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.715363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.776038</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.712462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.785634</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.711991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.793539</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.713530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.797237</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.716097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.797889</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.716097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687b6cf2be09412ba3ae8cb5ca5d517d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c16eada5e94419aa83bbca3aff0456a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:57, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>0.298131</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.709742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.279361</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.718191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.305198</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.711949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.310676</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.746594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.473502</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.729593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.440378</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.725128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.556676</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.728710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.544017</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.718577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.612121</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.727161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.658654</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.728146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.651203</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.730902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.676980</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.719002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.704218</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.717776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.695510</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.723827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.717978</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.731417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.744428</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.713179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.742058</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.736273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.750209</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.754557</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.733519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.754520</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.737628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.756640</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.729232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.757738</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.731699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.764397</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.731662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.767971</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.734420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.765723</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.727831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a042ae0c259540c4a40d004e2e448e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70020ad3451045d59b7cedb26a08211e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:57, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>0.298131</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.709742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.279361</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.718191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.305198</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.711949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.310676</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.746594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.473502</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.729593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.440378</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.725128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.556676</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.728710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.544017</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.718577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.612121</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.727161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.658654</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.728146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.651203</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.730902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.676980</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.719002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.704218</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.717776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.695510</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.723827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.717978</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.731417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.744428</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.713179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.742058</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.736273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.750209</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.754557</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.733519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.754520</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.737628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.756640</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.729232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.757738</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.731699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.764397</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.731662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.767971</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.734420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.765723</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.727831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1b8230261b49968e8f18039b81c5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0a3683384a43af932320ccc9343ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:57, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.404600</td>\n",
       "      <td>0.298131</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.709742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.279361</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.718191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.305198</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.711949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.310676</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.746594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.473502</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.729593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.067500</td>\n",
       "      <td>0.440378</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.725128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.556676</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.728710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.544017</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.718577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.612121</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.727161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.658654</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.728146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.651203</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.730902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.676980</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.719002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.704218</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.717776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.695510</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.723827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.717978</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.731417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.744428</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.713179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.742058</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.736273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.750209</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.754557</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.733519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.754520</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.737628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.756640</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.729232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.757738</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.731699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.764397</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.731662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.767971</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.734420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.765723</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.727831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 6816\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 1704\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a7c9c36d8649cd81c88a14c109145f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4331d66c82604f61b5e26c92d0d04e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.331810</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.688338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>0.332829</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.709931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.372362</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.716345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.384803</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.667998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.428496</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.727472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.463164</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.673612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.481249</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.737731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.560135</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.715835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.628042</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.715229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.618908</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.727385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.649859</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.723607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.677252</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.710547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.676349</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.738007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.721747</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.742563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.726247</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.727934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.755607</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.720122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.793804</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.728460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.747995</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.730815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.745573</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.733370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.764679</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.729047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.755600</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.742274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.757504</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.744627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.764369</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.741620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.773439</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.732331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.773354</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.732331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de039b9fc743475bb541300a99a669c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf1755420e441138e97ad2855f54811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.331810</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.688338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>0.332829</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.709931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.372362</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.716345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.384803</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.667998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.428496</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.727472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.463164</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.673612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.481249</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.737731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.560135</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.715835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.628042</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.715229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.618908</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.727385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.649859</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.723607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.677252</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.710547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.676349</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.738007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.721747</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.742563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.726247</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.727934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.755607</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.720122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.793804</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.728460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.747995</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.730815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.745573</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.733370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.764679</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.729047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.755600</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.742274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.757504</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.744627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.764369</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.741620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.773439</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.732331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.773354</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.732331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfd1400580d4aebad1123ecfe2f10ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ee548aa636436b9dfeaf615fb867fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.398900</td>\n",
       "      <td>0.331810</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.688338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.310900</td>\n",
       "      <td>0.332829</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.709931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.372362</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.716345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.233100</td>\n",
       "      <td>0.384803</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.667998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.428496</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.727472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.109100</td>\n",
       "      <td>0.463164</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.673612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.481249</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.737731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.044800</td>\n",
       "      <td>0.560135</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.715835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.040300</td>\n",
       "      <td>0.628042</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.715229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.031700</td>\n",
       "      <td>0.618908</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.727385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.649859</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.723607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.677252</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.710547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.676349</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.738007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.721747</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.742563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.726247</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.727934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.755607</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.720122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.793804</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.728460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.747995</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.730815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.745573</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.733370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.764679</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.729047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.755600</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.742274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.757504</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.744627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.764369</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.741620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.773439</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.732331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.773354</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.732331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219ab0f61ec64d0c85aa14881d7cbbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25d93b6ee9b41688a672a3b2aef53ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.323254</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.701349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.324472</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.644563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.356653</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.701021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.359089</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.666337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.413719</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.700027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.410003</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.723536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.496991</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.723843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.507634</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.741583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.555412</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.740012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.562397</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.723551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.597263</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.713370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.609518</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.717119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.686650</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.713332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.729199</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.695844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.668283</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.736369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.695925</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.710057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.671117</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.737339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.694323</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.719110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.677376</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.736654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.729487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.686975</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.736317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.689429</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.738917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.695035</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.737758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.732805</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.731929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.721549</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.730743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f8a54b1cd9413f9f3063a618ddc451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b4d57ba5f24d50833048ab6ecae6c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.323254</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.701349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.324472</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.644563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.356653</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.701021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.359089</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.666337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.413719</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.700027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.410003</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.723536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.496991</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.723843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.507634</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.741583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.555412</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.740012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.562397</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.723551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.597263</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.713370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.609518</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.717119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.686650</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.713332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.729199</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.695844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.668283</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.736369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.695925</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.710057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.671117</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.737339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.694323</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.719110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.677376</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.736654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.729487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.686975</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.736317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.689429</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.738917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.695035</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.737758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.732805</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.731929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.721549</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.730743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8fba5495c64ee6870f361119112c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b158f4fa384d4ebd2306387ec16bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.407100</td>\n",
       "      <td>0.323254</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.701349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.312300</td>\n",
       "      <td>0.324472</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.644563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.264600</td>\n",
       "      <td>0.356653</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.701021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.359089</td>\n",
       "      <td>0.867371</td>\n",
       "      <td>0.666337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.148900</td>\n",
       "      <td>0.413719</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.700027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.410003</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.723536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.079100</td>\n",
       "      <td>0.496991</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.723843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.057000</td>\n",
       "      <td>0.507634</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.741583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.555412</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.740012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.040900</td>\n",
       "      <td>0.562397</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.723551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.597263</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.713370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.609518</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.717119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.686650</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.713332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.729199</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.695844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.019100</td>\n",
       "      <td>0.668283</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.736369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.695925</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.710057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.671117</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.737339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.694323</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.719110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.677376</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.736654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.729487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.686975</td>\n",
       "      <td>0.891432</td>\n",
       "      <td>0.736317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.689429</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.738917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.695035</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.737758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.732805</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.731929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.721549</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.730743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ffd048c43324e099f4dee3ae2b1c4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4544d55a88a54c789232409f7377edf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.336257</td>\n",
       "      <td>0.867958</td>\n",
       "      <td>0.693420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.306100</td>\n",
       "      <td>0.310603</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.740502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.238300</td>\n",
       "      <td>0.305464</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.709977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.300306</td>\n",
       "      <td>0.905516</td>\n",
       "      <td>0.760702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.452670</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.712212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.369384</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.725037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.636626</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.680370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.480514</td>\n",
       "      <td>0.900822</td>\n",
       "      <td>0.735498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.500639</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.728991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.565574</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.723148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.608614</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.727944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.621768</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.750060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.672540</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.750682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.664916</td>\n",
       "      <td>0.895540</td>\n",
       "      <td>0.737806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.659025</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.760144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.672878</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.762304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.687018</td>\n",
       "      <td>0.894953</td>\n",
       "      <td>0.756820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.669495</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.757062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.672102</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.748456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.669838</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.750306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.681183</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.750359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.683413</td>\n",
       "      <td>0.899061</td>\n",
       "      <td>0.754249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.687754</td>\n",
       "      <td>0.899648</td>\n",
       "      <td>0.759726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.692452</td>\n",
       "      <td>0.899061</td>\n",
       "      <td>0.758032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.693843</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.754603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b87e54cff04cde958df375d7a2f1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d9d7a34ee34310b89ac74abfa1d5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.336257</td>\n",
       "      <td>0.867958</td>\n",
       "      <td>0.693420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.306100</td>\n",
       "      <td>0.310603</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.740502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.238300</td>\n",
       "      <td>0.305464</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.709977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.300306</td>\n",
       "      <td>0.905516</td>\n",
       "      <td>0.760702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.452670</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.712212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.369384</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.725037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.636626</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.680370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.480514</td>\n",
       "      <td>0.900822</td>\n",
       "      <td>0.735498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.500639</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.728991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.565574</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.723148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.608614</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.727944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.621768</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.750060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.672540</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.750682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.664916</td>\n",
       "      <td>0.895540</td>\n",
       "      <td>0.737806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.659025</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.760144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.672878</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.762304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.687018</td>\n",
       "      <td>0.894953</td>\n",
       "      <td>0.756820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.669495</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.757062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.672102</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.748456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.669838</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.750306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.681183</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.750359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.683413</td>\n",
       "      <td>0.899061</td>\n",
       "      <td>0.754249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.687754</td>\n",
       "      <td>0.899648</td>\n",
       "      <td>0.759726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.692452</td>\n",
       "      <td>0.899061</td>\n",
       "      <td>0.758032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.693843</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.754603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6edba864d1b24f00ade31a29bd628322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8780ebb2e1344c028f835951b66a5706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 02:01, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.336257</td>\n",
       "      <td>0.867958</td>\n",
       "      <td>0.693420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.306100</td>\n",
       "      <td>0.310603</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.740502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.238300</td>\n",
       "      <td>0.305464</td>\n",
       "      <td>0.892019</td>\n",
       "      <td>0.709977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.224900</td>\n",
       "      <td>0.300306</td>\n",
       "      <td>0.905516</td>\n",
       "      <td>0.760702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.452670</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.712212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.122200</td>\n",
       "      <td>0.369384</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.725037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.636626</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.680370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.480514</td>\n",
       "      <td>0.900822</td>\n",
       "      <td>0.735498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.039700</td>\n",
       "      <td>0.500639</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.728991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.565574</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.723148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.608614</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.727944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.621768</td>\n",
       "      <td>0.898474</td>\n",
       "      <td>0.750060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.672540</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.750682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.664916</td>\n",
       "      <td>0.895540</td>\n",
       "      <td>0.737806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.659025</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.760144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.672878</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.762304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.687018</td>\n",
       "      <td>0.894953</td>\n",
       "      <td>0.756820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.669495</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.757062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.672102</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.748456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.669838</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.750306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.681183</td>\n",
       "      <td>0.897887</td>\n",
       "      <td>0.750359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.683413</td>\n",
       "      <td>0.899061</td>\n",
       "      <td>0.754249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.687754</td>\n",
       "      <td>0.899648</td>\n",
       "      <td>0.759726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.692452</td>\n",
       "      <td>0.899061</td>\n",
       "      <td>0.758032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.693843</td>\n",
       "      <td>0.897300</td>\n",
       "      <td>0.754603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 6816\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 1704\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a9d91251ed4552ba8a2be20cca9520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64fc50eed0842bcb95c3661b018864c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:05, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>0.325730</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.696009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.306400</td>\n",
       "      <td>0.308644</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.734313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.369309</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.705504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.342283</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.680744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.439343</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.706100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.727363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.520461</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.691450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.597873</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.717921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.597050</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.721898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.674212</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.699928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.646529</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.722007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.692883</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.719510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.685843</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.703218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.722319</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.742447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.739771</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.711069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.753798</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.744082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.748797</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.731750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.743373</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.723031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.745213</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.740581</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.715534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.766368</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.739304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.736022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.759654</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.732090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.755570</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.757197</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.730902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5c7fe6295d42ec8bfe0d83def16e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb348b6016964a789f1f5d4ea9b27b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:05, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>0.325730</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.696009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.306400</td>\n",
       "      <td>0.308644</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.734313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.369309</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.705504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.342283</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.680744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.439343</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.706100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.727363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.520461</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.691450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.597873</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.717921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.597050</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.721898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.674212</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.699928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.646529</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.722007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.692883</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.719510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.685843</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.703218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.722319</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.742447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.739771</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.711069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.753798</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.744082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.748797</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.731750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.743373</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.723031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.745213</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.740581</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.715534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.766368</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.739304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.736022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.759654</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.732090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.755570</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.757197</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.730902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a03d5b235b54763ad3576b0a902d2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3ce7f964724f69a0d232f0072604d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:05, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>0.325730</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.696009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.306400</td>\n",
       "      <td>0.308644</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.734313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.209100</td>\n",
       "      <td>0.369309</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.705504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>0.342283</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.680744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.439343</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.706100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.091000</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.890845</td>\n",
       "      <td>0.727363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.520461</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.691450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.597873</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.717921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.029700</td>\n",
       "      <td>0.597050</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.721898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.674212</td>\n",
       "      <td>0.871479</td>\n",
       "      <td>0.699928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.646529</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.722007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.692883</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.719510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.685843</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.703218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.722319</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.742447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.739771</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.711069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.753798</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.744082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.748797</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.731750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.743373</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.723031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.745213</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.740581</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.715534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.766368</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.739304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.736022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.759654</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.732090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.755570</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.734061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.757197</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.730902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7853db4212486f9421b01e44935168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48ba8463d8c243669c77c103f2a21715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:05, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.316939</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.697284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>0.302311</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.691758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.318162</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.716010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.374254</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.456359</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.719620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.399119</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.734061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.531326</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.723305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.543401</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.529996</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.731470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.621700</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.705671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.585928</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.723313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.669498</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.707721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.736323</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.685326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.690220</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.724510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.713899</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.725208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.719606</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.717003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.721794</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.731009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.746989</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.724870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.743134</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.734379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.737042</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.734764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.740528</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.731992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.747135</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.735084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.752984</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.732497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.759716</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.721361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.757718</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.721640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36046464405d486594ced15c80d61e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed283cdde8414b2ab4c284df995b417c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:05, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.316939</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.697284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>0.302311</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.691758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.318162</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.716010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.374254</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.456359</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.719620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.399119</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.734061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.531326</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.723305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.543401</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.529996</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.731470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.621700</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.705671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.585928</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.723313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.669498</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.707721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.736323</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.685326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.690220</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.724510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.713899</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.725208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.719606</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.717003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.721794</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.731009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.746989</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.724870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.743134</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.734379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.737042</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.734764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.740528</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.731992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.747135</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.735084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.752984</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.732497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.759716</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.721361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.757718</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.721640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2342c29c738b472bbaaa73c2f6b8eddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9318304ef7e244d0bbf823192fd2b731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:05, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.316939</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.697284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>0.302311</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.691758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.234800</td>\n",
       "      <td>0.318162</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.716010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.191500</td>\n",
       "      <td>0.374254</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.129200</td>\n",
       "      <td>0.456359</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.719620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.109300</td>\n",
       "      <td>0.399119</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.734061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.531326</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.723305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.543401</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.717747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.529996</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.731470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.621700</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.705671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.585928</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.723313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.669498</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.707721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.736323</td>\n",
       "      <td>0.866197</td>\n",
       "      <td>0.685326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.690220</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.724510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.713899</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.725208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>0.719606</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.717003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.721794</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.731009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.746989</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.724870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.743134</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.734379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.737042</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.734764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.740528</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.731992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.747135</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.735084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.752984</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.732497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.759716</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.721361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.757718</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.721640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41217575bcdc463dbb7891b0a82a1754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15579307975b4d35a8123157931da601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:05, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.316038</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.684096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.296718</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.721491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.312444</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.723054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.321918</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.735544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.432919</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.735379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.451701</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.698837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>0.533335</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.728885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.573404</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.730237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.662542</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.690255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.622062</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.741106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.628017</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.723206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.708721</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.726456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.699260</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.714841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.733305</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.716709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.763844</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.724015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.787981</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.727899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.789691</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.725705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.770332</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.723291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.789245</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.723857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.782060</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.722742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.795868</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.720244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.779082</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.716988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.789467</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.719926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.793371</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.717257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.797344</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.719341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c58d5a0b91e4a82ab329e68b1acbefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9294de8efa14851bda2c4be6279bcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:05, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.316038</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.684096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.296718</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.721491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.312444</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.723054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.321918</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.735544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.432919</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.735379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.451701</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.698837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>0.533335</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.728885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.573404</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.730237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.662542</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.690255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.622062</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.741106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.628017</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.723206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.708721</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.726456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.699260</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.714841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.733305</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.716709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.763844</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.724015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.787981</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.727899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.789691</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.725705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.770332</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.723291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.789245</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.723857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.782060</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.722742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.795868</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.720244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.779082</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.716988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.789467</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.719926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.793371</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.717257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.797344</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.719341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d3500552634d4ba12a72a561183412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1e93485e6347eeb85278e9a0158927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:05, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.316038</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.684096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.296718</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.721491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.312444</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.723054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.321918</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.735544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.432919</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.735379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.451701</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.698837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>0.533335</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.728885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.573404</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.730237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.031100</td>\n",
       "      <td>0.662542</td>\n",
       "      <td>0.869131</td>\n",
       "      <td>0.690255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.622062</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.741106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.628017</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.723206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.708721</td>\n",
       "      <td>0.892606</td>\n",
       "      <td>0.726456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.699260</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.714841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.733305</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.716709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.763844</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.724015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>0.787981</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.727899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.789691</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.725705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.770332</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.723291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.789245</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.723857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.782060</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.722742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.795868</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.720244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.779082</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.716988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.789467</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.719926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.793371</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.717257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.797344</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.719341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 6816\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Twits', 'label', '__index_level_0__'],\n",
       "        num_rows: 1704\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d978626342a64220a9cd7a258b9f594f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f308edc8676b411caec966d3d2f8e7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:58, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.313671</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.708885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.300400</td>\n",
       "      <td>0.312895</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.694216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.347163</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.717868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.206800</td>\n",
       "      <td>0.348047</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.687802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.412404</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.703491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.454959</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.711309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.536963</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.704408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.535829</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.697890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.621454</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.705214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.646460</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.705493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.678997</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.704358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.698388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.666203</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.706494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.734723</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.688587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.737906</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.695163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.740640</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.696665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.746708</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.701824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.745610</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.705309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.754166</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.701422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.774897</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.701763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.795665</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.706128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.809974</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.707708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.815483</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.704187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.820079</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.707217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.820519</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.707217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564dedcd589a4247980e013f5b43f342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f924a751de4c39b9a5985baf9fa982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:58, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.313671</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.708885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.300400</td>\n",
       "      <td>0.312895</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.694216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.347163</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.717868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.206800</td>\n",
       "      <td>0.348047</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.687802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.412404</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.703491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.454959</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.711309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.536963</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.704408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.535829</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.697890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.621454</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.705214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.646460</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.705493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.678997</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.704358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.698388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.666203</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.706494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.734723</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.688587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.737906</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.695163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.740640</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.696665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.746708</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.701824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.745610</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.705309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.754166</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.701422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.774897</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.701763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.795665</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.706128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.809974</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.707708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.815483</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.704187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.820079</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.707217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.820519</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.707217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740f745b55f8419e919fc1ad62360f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ee7a0af6e34bbb8f3cde3b01d03cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:58, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.313671</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.708885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.300400</td>\n",
       "      <td>0.312895</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.694216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.347163</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.717868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.206800</td>\n",
       "      <td>0.348047</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.687802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.412404</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.703491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.454959</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.711309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.536963</td>\n",
       "      <td>0.884977</td>\n",
       "      <td>0.704408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.535829</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.697890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.621454</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.705214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.646460</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.705493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.678997</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.704358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.703690</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.698388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.666203</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.706494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.734723</td>\n",
       "      <td>0.876761</td>\n",
       "      <td>0.688587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.737906</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.695163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.740640</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.696665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.746708</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.701824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.745610</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.705309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.754166</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.701422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.774897</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.701763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.795665</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.706128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.809974</td>\n",
       "      <td>0.883803</td>\n",
       "      <td>0.707708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.815483</td>\n",
       "      <td>0.882042</td>\n",
       "      <td>0.704187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.820079</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.707217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.820519</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.707217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01459bce9dc742469823c25e178dacaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3a65c8639d4e30825a148c9670b0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:58, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>0.313410</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.700499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>0.293738</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.706551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.313390</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.698033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>0.344724</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.693932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.551678</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.626156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.440868</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.688218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.556709</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.700947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.608421</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.688855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.629472</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.697694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.738301</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.692228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.638364</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.691849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.731144</td>\n",
       "      <td>0.869718</td>\n",
       "      <td>0.684813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.784608</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.684599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.760201</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.694615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.736662</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.698052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.819938</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.694017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.781535</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.702479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.786100</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.698113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.827535</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.696735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.824184</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.692036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.808576</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.703369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.817553</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.696352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.837010</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.701476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.869648</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.698123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.868394</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.693728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e249d0d7f0d242b6b91c7087d9d8a46e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5aa46829d44eb6b47356aea7194637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:57, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>0.313410</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.700499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>0.293738</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.706551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.313390</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.698033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>0.344724</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.693932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.551678</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.626156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.440868</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.688218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.556709</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.700947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.608421</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.688855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.629472</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.697694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.738301</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.692228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.638364</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.691849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.731144</td>\n",
       "      <td>0.869718</td>\n",
       "      <td>0.684813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.784608</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.684599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.760201</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.694615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.736662</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.698052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.819938</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.694017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.781535</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.702479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.786100</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.698113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.827535</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.696735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.824184</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.692036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.808576</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.703369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.817553</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.696352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.837010</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.701476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.869648</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.698123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.868394</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.693728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6854c64fb38a48ceb51f267353d18be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfd2a7783a84c83b1b3448c392f14f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:57, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.389000</td>\n",
       "      <td>0.313410</td>\n",
       "      <td>0.883216</td>\n",
       "      <td>0.700499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.298200</td>\n",
       "      <td>0.293738</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.706551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.313390</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.698033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>0.344724</td>\n",
       "      <td>0.881455</td>\n",
       "      <td>0.693932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.130500</td>\n",
       "      <td>0.551678</td>\n",
       "      <td>0.870305</td>\n",
       "      <td>0.626156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.100600</td>\n",
       "      <td>0.440868</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.688218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.556709</td>\n",
       "      <td>0.879695</td>\n",
       "      <td>0.700947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.608421</td>\n",
       "      <td>0.873239</td>\n",
       "      <td>0.688855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.629472</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.697694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.738301</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.692228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.638364</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.691849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.731144</td>\n",
       "      <td>0.869718</td>\n",
       "      <td>0.684813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.784608</td>\n",
       "      <td>0.872653</td>\n",
       "      <td>0.684599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.009700</td>\n",
       "      <td>0.760201</td>\n",
       "      <td>0.877934</td>\n",
       "      <td>0.694615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.736662</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.698052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.009300</td>\n",
       "      <td>0.819938</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.694017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.781535</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.702479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>0.786100</td>\n",
       "      <td>0.873826</td>\n",
       "      <td>0.698113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.827535</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.696735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.824184</td>\n",
       "      <td>0.874413</td>\n",
       "      <td>0.692036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.808576</td>\n",
       "      <td>0.878521</td>\n",
       "      <td>0.703369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.817553</td>\n",
       "      <td>0.875587</td>\n",
       "      <td>0.696352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.837010</td>\n",
       "      <td>0.877347</td>\n",
       "      <td>0.701476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.869648</td>\n",
       "      <td>0.876174</td>\n",
       "      <td>0.698123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.008200</td>\n",
       "      <td>0.868394</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.693728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a395044d00642329d09a22e11d7f9d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1a07a511a04dfd9d80f502c5debfc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:57, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>0.310427</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.675646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.298319</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.709025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.319014</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.697351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.297875</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.724290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.407760</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.734273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.415330</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.691722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>0.485916</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.691090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.582877</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.707364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.549538</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.724016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.572886</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.712613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.594737</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.705578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.667193</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.700469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.632019</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.721474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.646112</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.705520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.685810</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.706258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.700716</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.731060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.708871</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.714203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.732993</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.720164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.718013</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.730332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.747359</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.719960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.736272</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.726578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.729696</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.717215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.732071</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.726580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.739623</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.727668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.741441</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.727918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8bbb48d14142d5abfc512711b72c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942b2f8678c54469951fc4191bf25128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:58, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>0.310427</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.675646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.298319</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.709025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.319014</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.697351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.297875</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.724290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.407760</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.734273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.415330</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.691722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>0.485916</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.691090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.582877</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.707364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.549538</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.724016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.572886</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.712613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.594737</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.705578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.667193</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.700469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.632019</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.721474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.646112</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.705520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.685810</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.706258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.700716</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.731060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.708871</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.714203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.732993</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.720164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.718013</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.730332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.747359</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.719960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.736272</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.726578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.729696</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.717215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.732071</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.726580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.739623</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.727668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.741441</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.727918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a899a9c9209243b883ce8fbce1a8392a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6816 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615dd456889b4f1d813fe8913561b50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1284' max='1284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1284/1284 01:57, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.398700</td>\n",
       "      <td>0.310427</td>\n",
       "      <td>0.880869</td>\n",
       "      <td>0.675646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.297500</td>\n",
       "      <td>0.298319</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.709025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.224700</td>\n",
       "      <td>0.319014</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.697351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.297875</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.724290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.407760</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.734273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.086900</td>\n",
       "      <td>0.415330</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.691722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>0.485916</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.691090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.042900</td>\n",
       "      <td>0.582877</td>\n",
       "      <td>0.884390</td>\n",
       "      <td>0.707364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.549538</td>\n",
       "      <td>0.890258</td>\n",
       "      <td>0.724016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.028800</td>\n",
       "      <td>0.572886</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.712613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.594737</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.705578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.667193</td>\n",
       "      <td>0.882629</td>\n",
       "      <td>0.700469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.632019</td>\n",
       "      <td>0.886150</td>\n",
       "      <td>0.721474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.646112</td>\n",
       "      <td>0.885563</td>\n",
       "      <td>0.705520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.685810</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.706258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.700716</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.731060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.708871</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.714203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.732993</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.720164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.718013</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.730332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.747359</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.719960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.736272</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.726578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.007300</td>\n",
       "      <td>0.729696</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.717215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.010200</td>\n",
       "      <td>0.732071</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.726580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.739623</td>\n",
       "      <td>0.887911</td>\n",
       "      <td>0.727668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.741441</td>\n",
       "      <td>0.888498</td>\n",
       "      <td>0.727918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SI2M-Lab/DarijaBERT</td>\n",
       "      <td>0.896127</td>\n",
       "      <td>0.762304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alger-ia/dziribert</td>\n",
       "      <td>0.889085</td>\n",
       "      <td>0.747426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>faisalq/EgyBERT</td>\n",
       "      <td>0.893192</td>\n",
       "      <td>0.736059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>faisalq/SaudiBERT</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.759349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>otmangi/MorRoBERTa</td>\n",
       "      <td>0.886737</td>\n",
       "      <td>0.744082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>otmangi/MorrBERT</td>\n",
       "      <td>0.889671</td>\n",
       "      <td>0.734273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tunis-ai/TunBERT</td>\n",
       "      <td>0.879108</td>\n",
       "      <td>0.682487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy        F1\n",
       "0   SI2M-Lab/DarijaBERT  0.896127  0.762304\n",
       "3    alger-ia/dziribert  0.889085  0.747426\n",
       "6       faisalq/EgyBERT  0.893192  0.736059\n",
       "9     faisalq/SaudiBERT  0.901408  0.759349\n",
       "12   otmangi/MorRoBERTa  0.886737  0.744082\n",
       "15     otmangi/MorrBERT  0.889671  0.734273\n",
       "18     tunis-ai/TunBERT  0.879108  0.682487"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "fname = 'Open_2'\n",
    "log_file = fname + '.txt'\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write('Model,Accuracy,F1\\n')\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('datasets/OpenAccessArDialect/Sentiment_Anaysis.csv', encoding='utf-8', engine='python') #, quotechar=\"'\"  , quoting=3\n",
    "df2 = pd.read_csv('datasets/OpenAccessArDialect/dialect.csv', encoding='utf-8', engine='python') #, quotechar=\"'\"  , quoting=3\n",
    "\n",
    "df2 = df2[df2['dialect'] == 'Morocco']\n",
    "      \n",
    "display(df1.columns)\n",
    "display(df1[:4])\n",
    "\n",
    "display(df2.columns)\n",
    "display(df2[:4])\n",
    "display(len(df2))\n",
    "\n",
    "\n",
    "\n",
    "df = df1[df1['Twits'].isin(df2['Twits'])]\n",
    "\n",
    "display(df[:5])\n",
    "display(len(df))\n",
    "\n",
    "c = df['label'].value_counts()\n",
    "display(c)\n",
    "\n",
    "classes = set(df['label'].values)\n",
    "display(classes)\n",
    "\n",
    "df['label'] = df['label'].astype('category')\n",
    "df['label'] = df['label'].cat.codes\n",
    "\n",
    "df = df[['Twits', 'label']]\n",
    "classes_num = len(classes)\n",
    "display(classes_num)\n",
    "display(len(df))\n",
    "\n",
    "\n",
    "\n",
    "max_sequence_length = 128\n",
    "\n",
    "\n",
    "\n",
    "models = [ \n",
    "        'faisalq/EgyBERT',            \n",
    "    'faisalq/SaudiBERT',            \n",
    "    'tunis-ai/TunBERT',\n",
    "    'alger-ia/dziribert',\n",
    "    'SI2M-Lab/DarijaBERT',\n",
    "    'otmangi/MorRoBERTa',\n",
    "    'otmangi/MorrBERT'\n",
    "            \n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seeds = [0, 1, 42]\n",
    "\n",
    "for model_name in models:\n",
    "    for seed in seeds:\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        ds = ds.train_test_split(test_size=0.2, seed = seed)\n",
    "        if seed==0:\n",
    "            display(ds)\n",
    "            \n",
    "        for i in range(3):\n",
    "            print(f'{model_name}, try:{i}')\n",
    "                  \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                                  num_labels=classes_num).to('cuda')                                                 \n",
    "            dataset_train = ds['train']\n",
    "            dataset_validation = ds['test']                                                    \n",
    "            \n",
    "          \n",
    "    \n",
    "            def preprocess_function(examples):\n",
    "                return tokenizer(examples['Twits'], truncation=True, padding=\"max_length\",\n",
    "                                max_length=max_sequence_length)\n",
    "            \n",
    "            \n",
    "            dataset_train = dataset_train.map(preprocess_function, batched=True)\n",
    "            dataset_validation = dataset_validation.map(preprocess_function, batched=True)\n",
    "            \n",
    "           \n",
    "            \n",
    "            def compute_metrics(eval_pred):\n",
    "                logits, labels = eval_pred\n",
    "                predictions = np.argmax(logits, axis=-1)    \n",
    "                acc = accuracy_score(labels, predictions)        \n",
    "                f1 = f1_score(labels, predictions, average='macro')   \n",
    "                with open(log_file, 'a') as f:\n",
    "                    f.write(f'{model_name},{acc},{f1}\\n')\n",
    "                return {'accuracy': acc, 'f1_score': f1}\n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "            epochs = 12\n",
    "            save_steps = 10000 #save checkpoint every 10000 steps\n",
    "            batch_size = 64\n",
    "            \n",
    "            training_args = TrainingArguments(\n",
    "                output_dir = 'bert/',\n",
    "                overwrite_output_dir=True,\n",
    "                num_train_epochs = epochs,\n",
    "                per_device_train_batch_size = batch_size,\n",
    "                per_device_eval_batch_size = batch_size,\n",
    "                save_steps = save_steps,\n",
    "                save_total_limit = 1, #only save the last 5 checkpoints\n",
    "                fp16=True,\n",
    "                learning_rate = 5e-5,  # 5e-5 is the default\n",
    "                logging_steps = 50, #50_000\n",
    "                evaluation_strategy = 'steps',\n",
    "                # evaluate_during_training = True,\n",
    "                eval_steps = 50\n",
    "                \n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model = model,\n",
    "                args = training_args,\n",
    "                # data_collator=data_collator,\n",
    "                train_dataset=dataset_train,\n",
    "                eval_dataset=dataset_validation,\n",
    "                compute_metrics = compute_metrics\n",
    "            )\n",
    "            \n",
    "            \n",
    "            trainer.train()\n",
    "\n",
    "\n",
    "results = pd.read_csv(log_file)\n",
    "\n",
    "best_results = results.groupby('Model', as_index=False)['F1'].max()\n",
    "\n",
    "best_results = pd.merge(best_results, results, on=['Model', 'F1'])\n",
    "best_results = best_results[['Model', 'Accuracy', 'F1']]\n",
    "best_results = best_results.drop_duplicates()\n",
    "best_results.to_csv(f'{fname}.csv')\n",
    "display(best_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213ac86-934f-4e82-a949-0bcdcae2188d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220784d6-b06d-4429-adb8-0026654f9d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8647cf08-3aa6-44eb-846f-4bed97554042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203fa6b-c9d7-44a4-b501-a67bfd3e4ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8794b705-31a1-45d7-8e88-4017a9c282aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
