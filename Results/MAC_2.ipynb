{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d804ae66-9435-44be-8aad-beacbdeec0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweets', 'type', 'class'], dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>type</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ماهي آرائكم متتبعي الكرام</td>\n",
       "      <td>neutral</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>عبقرية المدرب وروعة اللاعبين</td>\n",
       "      <td>positive</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>أمة عربية واحدة خالدة</td>\n",
       "      <td>positive</td>\n",
       "      <td>standard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>عاد إليكم من جديد وأخيرا درت جيم لهاد الصفحة</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dialectal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tweets      type      class\n",
       "0                     ماهي آرائكم متتبعي الكرام   neutral   standard\n",
       "1                  عبقرية المدرب وروعة اللاعبين  positive   standard\n",
       "2                        أمة عربية واحدة خالدة   positive   standard\n",
       "3  عاد إليكم من جديد وأخيرا درت جيم لهاد الصفحة   neutral  dialectal"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'mixed', 'negative', 'neutral', 'positive'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "type\n",
       "positive    9897\n",
       "neutral     4039\n",
       "negative    3508\n",
       "mixed        643\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "18087"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 14469\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 3618\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.177200</td>\n",
       "      <td>0.910012</td>\n",
       "      <td>0.772526</td>\n",
       "      <td>0.546720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.808600</td>\n",
       "      <td>0.664080</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>0.594674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.618500</td>\n",
       "      <td>0.546926</td>\n",
       "      <td>0.836650</td>\n",
       "      <td>0.609404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.476078</td>\n",
       "      <td>0.842178</td>\n",
       "      <td>0.618763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.449700</td>\n",
       "      <td>0.436467</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.627696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.387600</td>\n",
       "      <td>0.414397</td>\n",
       "      <td>0.869541</td>\n",
       "      <td>0.640759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.388091</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.648004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.397655</td>\n",
       "      <td>0.878939</td>\n",
       "      <td>0.704229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.256000</td>\n",
       "      <td>0.434100</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.756332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.391947</td>\n",
       "      <td>0.883637</td>\n",
       "      <td>0.768831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.377113</td>\n",
       "      <td>0.889994</td>\n",
       "      <td>0.803684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.159900</td>\n",
       "      <td>0.392690</td>\n",
       "      <td>0.892758</td>\n",
       "      <td>0.800254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.144100</td>\n",
       "      <td>0.404844</td>\n",
       "      <td>0.884467</td>\n",
       "      <td>0.790322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.150200</td>\n",
       "      <td>0.417014</td>\n",
       "      <td>0.887231</td>\n",
       "      <td>0.791948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.124700</td>\n",
       "      <td>0.412815</td>\n",
       "      <td>0.892758</td>\n",
       "      <td>0.813228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.123800</td>\n",
       "      <td>0.408858</td>\n",
       "      <td>0.894970</td>\n",
       "      <td>0.813380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.409703</td>\n",
       "      <td>0.894140</td>\n",
       "      <td>0.811196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.084600</td>\n",
       "      <td>0.433426</td>\n",
       "      <td>0.892482</td>\n",
       "      <td>0.807762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.097900</td>\n",
       "      <td>0.441837</td>\n",
       "      <td>0.892758</td>\n",
       "      <td>0.817476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.101400</td>\n",
       "      <td>0.429920</td>\n",
       "      <td>0.898563</td>\n",
       "      <td>0.821839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.455390</td>\n",
       "      <td>0.897734</td>\n",
       "      <td>0.817948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.088900</td>\n",
       "      <td>0.448602</td>\n",
       "      <td>0.895799</td>\n",
       "      <td>0.819204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.452909</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.814938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.463805</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.811345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.460242</td>\n",
       "      <td>0.896904</td>\n",
       "      <td>0.817605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.063200</td>\n",
       "      <td>0.464586</td>\n",
       "      <td>0.897181</td>\n",
       "      <td>0.824568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.052800</td>\n",
       "      <td>0.467172</td>\n",
       "      <td>0.896352</td>\n",
       "      <td>0.821711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.467761</td>\n",
       "      <td>0.897734</td>\n",
       "      <td>0.822292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.157800</td>\n",
       "      <td>0.882080</td>\n",
       "      <td>0.704809</td>\n",
       "      <td>0.400416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.803500</td>\n",
       "      <td>0.667633</td>\n",
       "      <td>0.806799</td>\n",
       "      <td>0.576324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>0.543904</td>\n",
       "      <td>0.840520</td>\n",
       "      <td>0.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.494300</td>\n",
       "      <td>0.475724</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.622743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.453600</td>\n",
       "      <td>0.443025</td>\n",
       "      <td>0.861249</td>\n",
       "      <td>0.632022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>0.421924</td>\n",
       "      <td>0.865119</td>\n",
       "      <td>0.634831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.384418</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.665601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.388405</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.695935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.259600</td>\n",
       "      <td>0.387907</td>\n",
       "      <td>0.883085</td>\n",
       "      <td>0.758585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.387049</td>\n",
       "      <td>0.888336</td>\n",
       "      <td>0.773624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.205900</td>\n",
       "      <td>0.375665</td>\n",
       "      <td>0.887231</td>\n",
       "      <td>0.788754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.402114</td>\n",
       "      <td>0.889718</td>\n",
       "      <td>0.791392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.403250</td>\n",
       "      <td>0.890824</td>\n",
       "      <td>0.794210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>0.404528</td>\n",
       "      <td>0.891653</td>\n",
       "      <td>0.798869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.415602</td>\n",
       "      <td>0.889718</td>\n",
       "      <td>0.792682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.420769</td>\n",
       "      <td>0.890547</td>\n",
       "      <td>0.796527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.431909</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.804840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.446917</td>\n",
       "      <td>0.892482</td>\n",
       "      <td>0.799865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.432862</td>\n",
       "      <td>0.897181</td>\n",
       "      <td>0.813205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.446383</td>\n",
       "      <td>0.894970</td>\n",
       "      <td>0.807995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.457765</td>\n",
       "      <td>0.899116</td>\n",
       "      <td>0.817990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.461564</td>\n",
       "      <td>0.897457</td>\n",
       "      <td>0.812239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.459072</td>\n",
       "      <td>0.895246</td>\n",
       "      <td>0.808135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.458502</td>\n",
       "      <td>0.899945</td>\n",
       "      <td>0.810686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.464764</td>\n",
       "      <td>0.897734</td>\n",
       "      <td>0.807886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.473730</td>\n",
       "      <td>0.897457</td>\n",
       "      <td>0.807761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.473405</td>\n",
       "      <td>0.896075</td>\n",
       "      <td>0.804268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.471206</td>\n",
       "      <td>0.897734</td>\n",
       "      <td>0.808873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.157800</td>\n",
       "      <td>0.882080</td>\n",
       "      <td>0.704809</td>\n",
       "      <td>0.400416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.803500</td>\n",
       "      <td>0.667633</td>\n",
       "      <td>0.806799</td>\n",
       "      <td>0.576324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>0.543904</td>\n",
       "      <td>0.840520</td>\n",
       "      <td>0.612400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.494300</td>\n",
       "      <td>0.475724</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.622743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.453600</td>\n",
       "      <td>0.443025</td>\n",
       "      <td>0.861249</td>\n",
       "      <td>0.632022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>0.421924</td>\n",
       "      <td>0.865119</td>\n",
       "      <td>0.634831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.384418</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.665601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.388405</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.695935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.259600</td>\n",
       "      <td>0.387907</td>\n",
       "      <td>0.883085</td>\n",
       "      <td>0.758585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.212000</td>\n",
       "      <td>0.387049</td>\n",
       "      <td>0.888336</td>\n",
       "      <td>0.773624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.205900</td>\n",
       "      <td>0.375665</td>\n",
       "      <td>0.887231</td>\n",
       "      <td>0.788754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.148200</td>\n",
       "      <td>0.402114</td>\n",
       "      <td>0.889718</td>\n",
       "      <td>0.791392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.147300</td>\n",
       "      <td>0.403250</td>\n",
       "      <td>0.890824</td>\n",
       "      <td>0.794210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.143500</td>\n",
       "      <td>0.404528</td>\n",
       "      <td>0.891653</td>\n",
       "      <td>0.798869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.415602</td>\n",
       "      <td>0.889718</td>\n",
       "      <td>0.792682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.420769</td>\n",
       "      <td>0.890547</td>\n",
       "      <td>0.796527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.103400</td>\n",
       "      <td>0.431909</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.804840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.446917</td>\n",
       "      <td>0.892482</td>\n",
       "      <td>0.799865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>0.432862</td>\n",
       "      <td>0.897181</td>\n",
       "      <td>0.813205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.446383</td>\n",
       "      <td>0.894970</td>\n",
       "      <td>0.807995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.457765</td>\n",
       "      <td>0.899116</td>\n",
       "      <td>0.817990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.076300</td>\n",
       "      <td>0.461564</td>\n",
       "      <td>0.897457</td>\n",
       "      <td>0.812239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.459072</td>\n",
       "      <td>0.895246</td>\n",
       "      <td>0.808135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.059400</td>\n",
       "      <td>0.458502</td>\n",
       "      <td>0.899945</td>\n",
       "      <td>0.810686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.464764</td>\n",
       "      <td>0.897734</td>\n",
       "      <td>0.807886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.473730</td>\n",
       "      <td>0.897457</td>\n",
       "      <td>0.807761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.473405</td>\n",
       "      <td>0.896075</td>\n",
       "      <td>0.804268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.058900</td>\n",
       "      <td>0.471206</td>\n",
       "      <td>0.897734</td>\n",
       "      <td>0.808873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.183000</td>\n",
       "      <td>0.960736</td>\n",
       "      <td>0.710061</td>\n",
       "      <td>0.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.841600</td>\n",
       "      <td>0.711530</td>\n",
       "      <td>0.752626</td>\n",
       "      <td>0.492742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.659400</td>\n",
       "      <td>0.599496</td>\n",
       "      <td>0.812604</td>\n",
       "      <td>0.585209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.536255</td>\n",
       "      <td>0.826147</td>\n",
       "      <td>0.600904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.476700</td>\n",
       "      <td>0.493341</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.606636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.465931</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.626375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.446332</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.632644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.437651</td>\n",
       "      <td>0.861802</td>\n",
       "      <td>0.686145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.451197</td>\n",
       "      <td>0.865948</td>\n",
       "      <td>0.735613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.440127</td>\n",
       "      <td>0.872582</td>\n",
       "      <td>0.753454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.434846</td>\n",
       "      <td>0.868988</td>\n",
       "      <td>0.760033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.464648</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.753396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.473899</td>\n",
       "      <td>0.869818</td>\n",
       "      <td>0.764212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.478762</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.770040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.478685</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.775189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.511362</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.770286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.498295</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.774947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.512040</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.779164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.504408</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.773237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.522915</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.783468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.518617</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.790116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.526219</td>\n",
       "      <td>0.884467</td>\n",
       "      <td>0.794836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.533996</td>\n",
       "      <td>0.882532</td>\n",
       "      <td>0.792136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.539203</td>\n",
       "      <td>0.879768</td>\n",
       "      <td>0.783314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.553641</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.784574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.550258</td>\n",
       "      <td>0.879768</td>\n",
       "      <td>0.787149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.553547</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.782664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.551505</td>\n",
       "      <td>0.879215</td>\n",
       "      <td>0.781056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.183000</td>\n",
       "      <td>0.960736</td>\n",
       "      <td>0.710061</td>\n",
       "      <td>0.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.841600</td>\n",
       "      <td>0.711530</td>\n",
       "      <td>0.752626</td>\n",
       "      <td>0.492742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.659400</td>\n",
       "      <td>0.599496</td>\n",
       "      <td>0.812604</td>\n",
       "      <td>0.585209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.536255</td>\n",
       "      <td>0.826147</td>\n",
       "      <td>0.600904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.476700</td>\n",
       "      <td>0.493341</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.606636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.465931</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.626375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.446332</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.632644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.437651</td>\n",
       "      <td>0.861802</td>\n",
       "      <td>0.686145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.451197</td>\n",
       "      <td>0.865948</td>\n",
       "      <td>0.735613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.440127</td>\n",
       "      <td>0.872582</td>\n",
       "      <td>0.753454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.434846</td>\n",
       "      <td>0.868988</td>\n",
       "      <td>0.760033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.464648</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.753396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.473899</td>\n",
       "      <td>0.869818</td>\n",
       "      <td>0.764212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.478762</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.770040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.478685</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.775189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.511362</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.770286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.498295</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.774947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.512040</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.779164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.504408</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.773237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.522915</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.783468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.518617</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.790116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.526219</td>\n",
       "      <td>0.884467</td>\n",
       "      <td>0.794836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.533996</td>\n",
       "      <td>0.882532</td>\n",
       "      <td>0.792136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.539203</td>\n",
       "      <td>0.879768</td>\n",
       "      <td>0.783314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.553641</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.784574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.550258</td>\n",
       "      <td>0.879768</td>\n",
       "      <td>0.787149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.553547</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.782664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.551505</td>\n",
       "      <td>0.879215</td>\n",
       "      <td>0.781056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.183000</td>\n",
       "      <td>0.960736</td>\n",
       "      <td>0.710061</td>\n",
       "      <td>0.416200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.841600</td>\n",
       "      <td>0.711530</td>\n",
       "      <td>0.752626</td>\n",
       "      <td>0.492742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.659400</td>\n",
       "      <td>0.599496</td>\n",
       "      <td>0.812604</td>\n",
       "      <td>0.585209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.526700</td>\n",
       "      <td>0.536255</td>\n",
       "      <td>0.826147</td>\n",
       "      <td>0.600904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.476700</td>\n",
       "      <td>0.493341</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.606636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.385300</td>\n",
       "      <td>0.465931</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.626375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.446332</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.632644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.437651</td>\n",
       "      <td>0.861802</td>\n",
       "      <td>0.686145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.250600</td>\n",
       "      <td>0.451197</td>\n",
       "      <td>0.865948</td>\n",
       "      <td>0.735613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.440127</td>\n",
       "      <td>0.872582</td>\n",
       "      <td>0.753454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.213800</td>\n",
       "      <td>0.434846</td>\n",
       "      <td>0.868988</td>\n",
       "      <td>0.760033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.464648</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.753396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.160600</td>\n",
       "      <td>0.473899</td>\n",
       "      <td>0.869818</td>\n",
       "      <td>0.764212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.141900</td>\n",
       "      <td>0.478762</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.770040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.478685</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.775189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.511362</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.770286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.498295</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.774947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.512040</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.779164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.504408</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.773237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.081100</td>\n",
       "      <td>0.522915</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.783468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.518617</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.790116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.072700</td>\n",
       "      <td>0.526219</td>\n",
       "      <td>0.884467</td>\n",
       "      <td>0.794836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.533996</td>\n",
       "      <td>0.882532</td>\n",
       "      <td>0.792136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.062700</td>\n",
       "      <td>0.539203</td>\n",
       "      <td>0.879768</td>\n",
       "      <td>0.783314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.553641</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.784574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.550258</td>\n",
       "      <td>0.879768</td>\n",
       "      <td>0.787149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.553547</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.782664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.551505</td>\n",
       "      <td>0.879215</td>\n",
       "      <td>0.781056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.156800</td>\n",
       "      <td>0.899167</td>\n",
       "      <td>0.718629</td>\n",
       "      <td>0.440839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.801300</td>\n",
       "      <td>0.676461</td>\n",
       "      <td>0.810116</td>\n",
       "      <td>0.589772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>0.562309</td>\n",
       "      <td>0.831122</td>\n",
       "      <td>0.606538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.502254</td>\n",
       "      <td>0.838861</td>\n",
       "      <td>0.618750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.454736</td>\n",
       "      <td>0.845218</td>\n",
       "      <td>0.620135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.432814</td>\n",
       "      <td>0.857933</td>\n",
       "      <td>0.633292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.304300</td>\n",
       "      <td>0.440143</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.629130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>0.424244</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.682455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.448682</td>\n",
       "      <td>0.874793</td>\n",
       "      <td>0.761626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.426428</td>\n",
       "      <td>0.876175</td>\n",
       "      <td>0.772762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.411563</td>\n",
       "      <td>0.886678</td>\n",
       "      <td>0.794140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.430515</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.793287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.416152</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.811990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.428010</td>\n",
       "      <td>0.886954</td>\n",
       "      <td>0.806319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.422609</td>\n",
       "      <td>0.892206</td>\n",
       "      <td>0.821422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.441830</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.809224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.434626</td>\n",
       "      <td>0.894693</td>\n",
       "      <td>0.824630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.449180</td>\n",
       "      <td>0.891653</td>\n",
       "      <td>0.818927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.459832</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.811641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.459037</td>\n",
       "      <td>0.893588</td>\n",
       "      <td>0.826880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.455740</td>\n",
       "      <td>0.897457</td>\n",
       "      <td>0.825172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.487389</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.816462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.478793</td>\n",
       "      <td>0.894417</td>\n",
       "      <td>0.822210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.488301</td>\n",
       "      <td>0.890547</td>\n",
       "      <td>0.812443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.481358</td>\n",
       "      <td>0.891376</td>\n",
       "      <td>0.817979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.487706</td>\n",
       "      <td>0.891376</td>\n",
       "      <td>0.815849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.489166</td>\n",
       "      <td>0.893311</td>\n",
       "      <td>0.820429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.490286</td>\n",
       "      <td>0.892482</td>\n",
       "      <td>0.821189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.156800</td>\n",
       "      <td>0.899167</td>\n",
       "      <td>0.718629</td>\n",
       "      <td>0.440839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.801300</td>\n",
       "      <td>0.676461</td>\n",
       "      <td>0.810116</td>\n",
       "      <td>0.589772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>0.562309</td>\n",
       "      <td>0.831122</td>\n",
       "      <td>0.606538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.502254</td>\n",
       "      <td>0.838861</td>\n",
       "      <td>0.618750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.454736</td>\n",
       "      <td>0.845218</td>\n",
       "      <td>0.620135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.432814</td>\n",
       "      <td>0.857933</td>\n",
       "      <td>0.633292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.304300</td>\n",
       "      <td>0.440143</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.629130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>0.424244</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.682455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.448682</td>\n",
       "      <td>0.874793</td>\n",
       "      <td>0.761626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.426428</td>\n",
       "      <td>0.876175</td>\n",
       "      <td>0.772762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.411563</td>\n",
       "      <td>0.886678</td>\n",
       "      <td>0.794140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.430515</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.793287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.416152</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.811990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.428010</td>\n",
       "      <td>0.886954</td>\n",
       "      <td>0.806319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.422609</td>\n",
       "      <td>0.892206</td>\n",
       "      <td>0.821422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.441830</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.809224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.434626</td>\n",
       "      <td>0.894693</td>\n",
       "      <td>0.824630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.449180</td>\n",
       "      <td>0.891653</td>\n",
       "      <td>0.818927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.459832</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.811641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.459037</td>\n",
       "      <td>0.893588</td>\n",
       "      <td>0.826880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.455740</td>\n",
       "      <td>0.897457</td>\n",
       "      <td>0.825172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.487389</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.816462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.478793</td>\n",
       "      <td>0.894417</td>\n",
       "      <td>0.822210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.488301</td>\n",
       "      <td>0.890547</td>\n",
       "      <td>0.812443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.481358</td>\n",
       "      <td>0.891376</td>\n",
       "      <td>0.817979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.487706</td>\n",
       "      <td>0.891376</td>\n",
       "      <td>0.815849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.489166</td>\n",
       "      <td>0.893311</td>\n",
       "      <td>0.820429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.490286</td>\n",
       "      <td>0.892482</td>\n",
       "      <td>0.821189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/EgyBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/EgyBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e47ed72a6f46988b5d55bcdee98060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.156800</td>\n",
       "      <td>0.899167</td>\n",
       "      <td>0.718629</td>\n",
       "      <td>0.440839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.801300</td>\n",
       "      <td>0.676461</td>\n",
       "      <td>0.810116</td>\n",
       "      <td>0.589772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.635100</td>\n",
       "      <td>0.562309</td>\n",
       "      <td>0.831122</td>\n",
       "      <td>0.606538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.491300</td>\n",
       "      <td>0.502254</td>\n",
       "      <td>0.838861</td>\n",
       "      <td>0.618750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>0.454736</td>\n",
       "      <td>0.845218</td>\n",
       "      <td>0.620135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.381000</td>\n",
       "      <td>0.432814</td>\n",
       "      <td>0.857933</td>\n",
       "      <td>0.633292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.304300</td>\n",
       "      <td>0.440143</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.629130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>0.424244</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.682455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.249800</td>\n",
       "      <td>0.448682</td>\n",
       "      <td>0.874793</td>\n",
       "      <td>0.761626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.426428</td>\n",
       "      <td>0.876175</td>\n",
       "      <td>0.772762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.411563</td>\n",
       "      <td>0.886678</td>\n",
       "      <td>0.794140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.178200</td>\n",
       "      <td>0.430515</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.793287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.416152</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.811990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.147600</td>\n",
       "      <td>0.428010</td>\n",
       "      <td>0.886954</td>\n",
       "      <td>0.806319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>0.422609</td>\n",
       "      <td>0.892206</td>\n",
       "      <td>0.821422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.107800</td>\n",
       "      <td>0.441830</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.809224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.106100</td>\n",
       "      <td>0.434626</td>\n",
       "      <td>0.894693</td>\n",
       "      <td>0.824630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.449180</td>\n",
       "      <td>0.891653</td>\n",
       "      <td>0.818927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.459832</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.811641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.459037</td>\n",
       "      <td>0.893588</td>\n",
       "      <td>0.826880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.455740</td>\n",
       "      <td>0.897457</td>\n",
       "      <td>0.825172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.487389</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.816462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.478793</td>\n",
       "      <td>0.894417</td>\n",
       "      <td>0.822210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.488301</td>\n",
       "      <td>0.890547</td>\n",
       "      <td>0.812443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.062900</td>\n",
       "      <td>0.481358</td>\n",
       "      <td>0.891376</td>\n",
       "      <td>0.817979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.487706</td>\n",
       "      <td>0.891376</td>\n",
       "      <td>0.815849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.489166</td>\n",
       "      <td>0.893311</td>\n",
       "      <td>0.820429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.490286</td>\n",
       "      <td>0.892482</td>\n",
       "      <td>0.821189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 14469\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 3618\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b3f4919dfee4787b2fd823ef0c5be98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612618afd4e141c6aa7b50b74aa4e7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.543701</td>\n",
       "      <td>0.795191</td>\n",
       "      <td>0.580667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.470100</td>\n",
       "      <td>0.403207</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.690439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.391685</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.777461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.393597</td>\n",
       "      <td>0.873411</td>\n",
       "      <td>0.776623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.339826</td>\n",
       "      <td>0.896352</td>\n",
       "      <td>0.820837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.176600</td>\n",
       "      <td>0.440901</td>\n",
       "      <td>0.888336</td>\n",
       "      <td>0.795433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>0.402051</td>\n",
       "      <td>0.896075</td>\n",
       "      <td>0.823885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.101700</td>\n",
       "      <td>0.451153</td>\n",
       "      <td>0.896628</td>\n",
       "      <td>0.823193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.463689</td>\n",
       "      <td>0.899668</td>\n",
       "      <td>0.824277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.433733</td>\n",
       "      <td>0.901603</td>\n",
       "      <td>0.829990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>0.457963</td>\n",
       "      <td>0.899945</td>\n",
       "      <td>0.832372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.487250</td>\n",
       "      <td>0.903814</td>\n",
       "      <td>0.830112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.508359</td>\n",
       "      <td>0.902709</td>\n",
       "      <td>0.829599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.513846</td>\n",
       "      <td>0.903261</td>\n",
       "      <td>0.828231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.536337</td>\n",
       "      <td>0.904091</td>\n",
       "      <td>0.833822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.545510</td>\n",
       "      <td>0.898563</td>\n",
       "      <td>0.821878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.539465</td>\n",
       "      <td>0.908513</td>\n",
       "      <td>0.847212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.554827</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.842476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.550313</td>\n",
       "      <td>0.907960</td>\n",
       "      <td>0.845863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.559080</td>\n",
       "      <td>0.901050</td>\n",
       "      <td>0.827591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.594602</td>\n",
       "      <td>0.906578</td>\n",
       "      <td>0.843409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.560831</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.843946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.568897</td>\n",
       "      <td>0.904920</td>\n",
       "      <td>0.837578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.572034</td>\n",
       "      <td>0.908237</td>\n",
       "      <td>0.840712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.581599</td>\n",
       "      <td>0.906025</td>\n",
       "      <td>0.839220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.577692</td>\n",
       "      <td>0.907684</td>\n",
       "      <td>0.840869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.583095</td>\n",
       "      <td>0.906578</td>\n",
       "      <td>0.839030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.581552</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.843478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397f2f9c69ea4471bea6dd0463628068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9775617786b24cbaa435f6b96be3377a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.543701</td>\n",
       "      <td>0.795191</td>\n",
       "      <td>0.580667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.470100</td>\n",
       "      <td>0.403207</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.690439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.391685</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.777461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.393597</td>\n",
       "      <td>0.873411</td>\n",
       "      <td>0.776623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.339826</td>\n",
       "      <td>0.896352</td>\n",
       "      <td>0.820837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.176600</td>\n",
       "      <td>0.440901</td>\n",
       "      <td>0.888336</td>\n",
       "      <td>0.795433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>0.402051</td>\n",
       "      <td>0.896075</td>\n",
       "      <td>0.823885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.101700</td>\n",
       "      <td>0.451153</td>\n",
       "      <td>0.896628</td>\n",
       "      <td>0.823193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.463689</td>\n",
       "      <td>0.899668</td>\n",
       "      <td>0.824277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.433733</td>\n",
       "      <td>0.901603</td>\n",
       "      <td>0.829990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>0.457963</td>\n",
       "      <td>0.899945</td>\n",
       "      <td>0.832372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.487250</td>\n",
       "      <td>0.903814</td>\n",
       "      <td>0.830112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.508359</td>\n",
       "      <td>0.902709</td>\n",
       "      <td>0.829599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.513846</td>\n",
       "      <td>0.903261</td>\n",
       "      <td>0.828231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.536337</td>\n",
       "      <td>0.904091</td>\n",
       "      <td>0.833822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.545510</td>\n",
       "      <td>0.898563</td>\n",
       "      <td>0.821878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.539465</td>\n",
       "      <td>0.908513</td>\n",
       "      <td>0.847212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.554827</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.842476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.550313</td>\n",
       "      <td>0.907960</td>\n",
       "      <td>0.845863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.559080</td>\n",
       "      <td>0.901050</td>\n",
       "      <td>0.827591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.594602</td>\n",
       "      <td>0.906578</td>\n",
       "      <td>0.843409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.560831</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.843946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.568897</td>\n",
       "      <td>0.904920</td>\n",
       "      <td>0.837578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.572034</td>\n",
       "      <td>0.908237</td>\n",
       "      <td>0.840712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.581599</td>\n",
       "      <td>0.906025</td>\n",
       "      <td>0.839220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.577692</td>\n",
       "      <td>0.907684</td>\n",
       "      <td>0.840869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.583095</td>\n",
       "      <td>0.906578</td>\n",
       "      <td>0.839030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.581552</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.843478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99993081d77b43ee8e87c0c5690ec65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bc2719371e44e4b23b0a9aa2c9dca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.644600</td>\n",
       "      <td>0.543701</td>\n",
       "      <td>0.795191</td>\n",
       "      <td>0.580667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.470100</td>\n",
       "      <td>0.403207</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.690439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.391685</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.777461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.393597</td>\n",
       "      <td>0.873411</td>\n",
       "      <td>0.776623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.222800</td>\n",
       "      <td>0.339826</td>\n",
       "      <td>0.896352</td>\n",
       "      <td>0.820837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.176600</td>\n",
       "      <td>0.440901</td>\n",
       "      <td>0.888336</td>\n",
       "      <td>0.795433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>0.402051</td>\n",
       "      <td>0.896075</td>\n",
       "      <td>0.823885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.101700</td>\n",
       "      <td>0.451153</td>\n",
       "      <td>0.896628</td>\n",
       "      <td>0.823193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.078500</td>\n",
       "      <td>0.463689</td>\n",
       "      <td>0.899668</td>\n",
       "      <td>0.824277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.433733</td>\n",
       "      <td>0.901603</td>\n",
       "      <td>0.829990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.078300</td>\n",
       "      <td>0.457963</td>\n",
       "      <td>0.899945</td>\n",
       "      <td>0.832372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.487250</td>\n",
       "      <td>0.903814</td>\n",
       "      <td>0.830112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.045100</td>\n",
       "      <td>0.508359</td>\n",
       "      <td>0.902709</td>\n",
       "      <td>0.829599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.513846</td>\n",
       "      <td>0.903261</td>\n",
       "      <td>0.828231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.536337</td>\n",
       "      <td>0.904091</td>\n",
       "      <td>0.833822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.545510</td>\n",
       "      <td>0.898563</td>\n",
       "      <td>0.821878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.539465</td>\n",
       "      <td>0.908513</td>\n",
       "      <td>0.847212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.554827</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.842476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.550313</td>\n",
       "      <td>0.907960</td>\n",
       "      <td>0.845863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.559080</td>\n",
       "      <td>0.901050</td>\n",
       "      <td>0.827591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.594602</td>\n",
       "      <td>0.906578</td>\n",
       "      <td>0.843409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.560831</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.843946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.568897</td>\n",
       "      <td>0.904920</td>\n",
       "      <td>0.837578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.572034</td>\n",
       "      <td>0.908237</td>\n",
       "      <td>0.840712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.581599</td>\n",
       "      <td>0.906025</td>\n",
       "      <td>0.839220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.577692</td>\n",
       "      <td>0.907684</td>\n",
       "      <td>0.840869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.014700</td>\n",
       "      <td>0.583095</td>\n",
       "      <td>0.906578</td>\n",
       "      <td>0.839030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.581552</td>\n",
       "      <td>0.907407</td>\n",
       "      <td>0.843478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5f2fbc16b64e208908badaac37c228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9cd1c0757b3484790430d17e19f5848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.482755</td>\n",
       "      <td>0.826976</td>\n",
       "      <td>0.632347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>0.418987</td>\n",
       "      <td>0.846600</td>\n",
       "      <td>0.710102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.418229</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.765987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.425243</td>\n",
       "      <td>0.867606</td>\n",
       "      <td>0.742411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.366208</td>\n",
       "      <td>0.886678</td>\n",
       "      <td>0.791106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>0.440347</td>\n",
       "      <td>0.890271</td>\n",
       "      <td>0.800282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.394237</td>\n",
       "      <td>0.894970</td>\n",
       "      <td>0.816988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.475081</td>\n",
       "      <td>0.883361</td>\n",
       "      <td>0.794281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.478023</td>\n",
       "      <td>0.893311</td>\n",
       "      <td>0.804918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.486449</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.812552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.550159</td>\n",
       "      <td>0.886401</td>\n",
       "      <td>0.795357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.585553</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.818599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.530426</td>\n",
       "      <td>0.900498</td>\n",
       "      <td>0.824327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.549873</td>\n",
       "      <td>0.898563</td>\n",
       "      <td>0.824468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.558683</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.830059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.541354</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.827520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.553018</td>\n",
       "      <td>0.904920</td>\n",
       "      <td>0.830825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.591496</td>\n",
       "      <td>0.900774</td>\n",
       "      <td>0.826937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.588170</td>\n",
       "      <td>0.897181</td>\n",
       "      <td>0.819150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.620550</td>\n",
       "      <td>0.895246</td>\n",
       "      <td>0.810303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.611764</td>\n",
       "      <td>0.901327</td>\n",
       "      <td>0.823768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.594758</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.826520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.598442</td>\n",
       "      <td>0.903261</td>\n",
       "      <td>0.828641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.610747</td>\n",
       "      <td>0.904643</td>\n",
       "      <td>0.833483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.616473</td>\n",
       "      <td>0.898839</td>\n",
       "      <td>0.818437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.619538</td>\n",
       "      <td>0.899668</td>\n",
       "      <td>0.819356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.618863</td>\n",
       "      <td>0.901603</td>\n",
       "      <td>0.821551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.620708</td>\n",
       "      <td>0.902156</td>\n",
       "      <td>0.823283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c960abb4d848209b35ad1827535985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e02c6dbe8564168be38aa4857b02f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.482755</td>\n",
       "      <td>0.826976</td>\n",
       "      <td>0.632347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>0.418987</td>\n",
       "      <td>0.846600</td>\n",
       "      <td>0.710102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.418229</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.765987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.425243</td>\n",
       "      <td>0.867606</td>\n",
       "      <td>0.742411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.366208</td>\n",
       "      <td>0.886678</td>\n",
       "      <td>0.791106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>0.440347</td>\n",
       "      <td>0.890271</td>\n",
       "      <td>0.800282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.394237</td>\n",
       "      <td>0.894970</td>\n",
       "      <td>0.816988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.475081</td>\n",
       "      <td>0.883361</td>\n",
       "      <td>0.794281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.478023</td>\n",
       "      <td>0.893311</td>\n",
       "      <td>0.804918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.486449</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.812552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.550159</td>\n",
       "      <td>0.886401</td>\n",
       "      <td>0.795357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.585553</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.818599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.530426</td>\n",
       "      <td>0.900498</td>\n",
       "      <td>0.824327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.549873</td>\n",
       "      <td>0.898563</td>\n",
       "      <td>0.824468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.558683</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.830059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.541354</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.827520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.553018</td>\n",
       "      <td>0.904920</td>\n",
       "      <td>0.830825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.591496</td>\n",
       "      <td>0.900774</td>\n",
       "      <td>0.826937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.588170</td>\n",
       "      <td>0.897181</td>\n",
       "      <td>0.819150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.620550</td>\n",
       "      <td>0.895246</td>\n",
       "      <td>0.810303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.611764</td>\n",
       "      <td>0.901327</td>\n",
       "      <td>0.823768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.594758</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.826520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.598442</td>\n",
       "      <td>0.903261</td>\n",
       "      <td>0.828641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.610747</td>\n",
       "      <td>0.904643</td>\n",
       "      <td>0.833483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.616473</td>\n",
       "      <td>0.898839</td>\n",
       "      <td>0.818437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.619538</td>\n",
       "      <td>0.899668</td>\n",
       "      <td>0.819356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.618863</td>\n",
       "      <td>0.901603</td>\n",
       "      <td>0.821551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.620708</td>\n",
       "      <td>0.902156</td>\n",
       "      <td>0.823283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08fcc39e2934fa4b9010ce251417920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bad3934661d46a6aa56e601fdd6b2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.482755</td>\n",
       "      <td>0.826976</td>\n",
       "      <td>0.632347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.449900</td>\n",
       "      <td>0.418987</td>\n",
       "      <td>0.846600</td>\n",
       "      <td>0.710102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.418229</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.765987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.227900</td>\n",
       "      <td>0.425243</td>\n",
       "      <td>0.867606</td>\n",
       "      <td>0.742411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.233300</td>\n",
       "      <td>0.366208</td>\n",
       "      <td>0.886678</td>\n",
       "      <td>0.791106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.148800</td>\n",
       "      <td>0.440347</td>\n",
       "      <td>0.890271</td>\n",
       "      <td>0.800282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.117100</td>\n",
       "      <td>0.394237</td>\n",
       "      <td>0.894970</td>\n",
       "      <td>0.816988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.096800</td>\n",
       "      <td>0.475081</td>\n",
       "      <td>0.883361</td>\n",
       "      <td>0.794281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.478023</td>\n",
       "      <td>0.893311</td>\n",
       "      <td>0.804918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.486449</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.812552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.066600</td>\n",
       "      <td>0.550159</td>\n",
       "      <td>0.886401</td>\n",
       "      <td>0.795357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.585553</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.818599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.530426</td>\n",
       "      <td>0.900498</td>\n",
       "      <td>0.824327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.549873</td>\n",
       "      <td>0.898563</td>\n",
       "      <td>0.824468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.558683</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.830059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.034200</td>\n",
       "      <td>0.541354</td>\n",
       "      <td>0.902985</td>\n",
       "      <td>0.827520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.553018</td>\n",
       "      <td>0.904920</td>\n",
       "      <td>0.830825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.591496</td>\n",
       "      <td>0.900774</td>\n",
       "      <td>0.826937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.588170</td>\n",
       "      <td>0.897181</td>\n",
       "      <td>0.819150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.620550</td>\n",
       "      <td>0.895246</td>\n",
       "      <td>0.810303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.611764</td>\n",
       "      <td>0.901327</td>\n",
       "      <td>0.823768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.594758</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.826520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.598442</td>\n",
       "      <td>0.903261</td>\n",
       "      <td>0.828641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.610747</td>\n",
       "      <td>0.904643</td>\n",
       "      <td>0.833483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.616473</td>\n",
       "      <td>0.898839</td>\n",
       "      <td>0.818437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.018900</td>\n",
       "      <td>0.619538</td>\n",
       "      <td>0.899668</td>\n",
       "      <td>0.819356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.618863</td>\n",
       "      <td>0.901603</td>\n",
       "      <td>0.821551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.620708</td>\n",
       "      <td>0.902156</td>\n",
       "      <td>0.823283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1ae60735964e4fa48cb3fb2512b867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae315c7a4c2249cd98402b2d0cd94acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>0.819237</td>\n",
       "      <td>0.637957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.495800</td>\n",
       "      <td>0.398133</td>\n",
       "      <td>0.859867</td>\n",
       "      <td>0.726142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.384600</td>\n",
       "      <td>0.384782</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.766381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.388294</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.785387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.357009</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.807194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.167200</td>\n",
       "      <td>0.387873</td>\n",
       "      <td>0.891929</td>\n",
       "      <td>0.820447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>0.453893</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.807048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.462731</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.810357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.510053</td>\n",
       "      <td>0.895246</td>\n",
       "      <td>0.827243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.493667</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.821179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.510951</td>\n",
       "      <td>0.891929</td>\n",
       "      <td>0.826513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.575823</td>\n",
       "      <td>0.881150</td>\n",
       "      <td>0.796050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.541925</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.831378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.521297</td>\n",
       "      <td>0.899668</td>\n",
       "      <td>0.841252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.573934</td>\n",
       "      <td>0.896904</td>\n",
       "      <td>0.832255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.571771</td>\n",
       "      <td>0.897734</td>\n",
       "      <td>0.835099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.536367</td>\n",
       "      <td>0.901603</td>\n",
       "      <td>0.834196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.575037</td>\n",
       "      <td>0.898839</td>\n",
       "      <td>0.830844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.602524</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.835174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.601118</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.836231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.605465</td>\n",
       "      <td>0.901327</td>\n",
       "      <td>0.841257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.611550</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.843221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.619238</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.840358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.620143</td>\n",
       "      <td>0.899116</td>\n",
       "      <td>0.833510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.628591</td>\n",
       "      <td>0.898286</td>\n",
       "      <td>0.833417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.634144</td>\n",
       "      <td>0.900774</td>\n",
       "      <td>0.835619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.630259</td>\n",
       "      <td>0.900774</td>\n",
       "      <td>0.836528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.630136</td>\n",
       "      <td>0.898839</td>\n",
       "      <td>0.832808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4844ac8202ef43b3bdf58f7cb9919921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c8b46960034f6ab1daaaca4fb44c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>0.819237</td>\n",
       "      <td>0.637957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.495800</td>\n",
       "      <td>0.398133</td>\n",
       "      <td>0.859867</td>\n",
       "      <td>0.726142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.384600</td>\n",
       "      <td>0.384782</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.766381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.388294</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.785387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.357009</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.807194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.167200</td>\n",
       "      <td>0.387873</td>\n",
       "      <td>0.891929</td>\n",
       "      <td>0.820447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>0.453893</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.807048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.462731</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.810357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.510053</td>\n",
       "      <td>0.895246</td>\n",
       "      <td>0.827243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.493667</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.821179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.510951</td>\n",
       "      <td>0.891929</td>\n",
       "      <td>0.826513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.575823</td>\n",
       "      <td>0.881150</td>\n",
       "      <td>0.796050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.541925</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.831378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.521297</td>\n",
       "      <td>0.899668</td>\n",
       "      <td>0.841252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.573934</td>\n",
       "      <td>0.896904</td>\n",
       "      <td>0.832255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.571771</td>\n",
       "      <td>0.897734</td>\n",
       "      <td>0.835099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.536367</td>\n",
       "      <td>0.901603</td>\n",
       "      <td>0.834196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.575037</td>\n",
       "      <td>0.898839</td>\n",
       "      <td>0.830844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.602524</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.835174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.601118</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.836231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.605465</td>\n",
       "      <td>0.901327</td>\n",
       "      <td>0.841257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.611550</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.843221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.619238</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.840358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.620143</td>\n",
       "      <td>0.899116</td>\n",
       "      <td>0.833510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.628591</td>\n",
       "      <td>0.898286</td>\n",
       "      <td>0.833417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.634144</td>\n",
       "      <td>0.900774</td>\n",
       "      <td>0.835619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.630259</td>\n",
       "      <td>0.900774</td>\n",
       "      <td>0.836528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.630136</td>\n",
       "      <td>0.898839</td>\n",
       "      <td>0.832808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faisalq/SaudiBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at faisalq/SaudiBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5734b13142e24bf393cae0d320765785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd5dcb9cb10450e983b3ba02e06ee06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.496423</td>\n",
       "      <td>0.819237</td>\n",
       "      <td>0.637957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.495800</td>\n",
       "      <td>0.398133</td>\n",
       "      <td>0.859867</td>\n",
       "      <td>0.726142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.384600</td>\n",
       "      <td>0.384782</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.766381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.224800</td>\n",
       "      <td>0.388294</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.785387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.214900</td>\n",
       "      <td>0.357009</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.807194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.167200</td>\n",
       "      <td>0.387873</td>\n",
       "      <td>0.891929</td>\n",
       "      <td>0.820447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>0.453893</td>\n",
       "      <td>0.889165</td>\n",
       "      <td>0.807048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.462731</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.810357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.510053</td>\n",
       "      <td>0.895246</td>\n",
       "      <td>0.827243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.074900</td>\n",
       "      <td>0.493667</td>\n",
       "      <td>0.893035</td>\n",
       "      <td>0.821179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.510951</td>\n",
       "      <td>0.891929</td>\n",
       "      <td>0.826513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.575823</td>\n",
       "      <td>0.881150</td>\n",
       "      <td>0.796050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.541925</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.831378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.521297</td>\n",
       "      <td>0.899668</td>\n",
       "      <td>0.841252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.573934</td>\n",
       "      <td>0.896904</td>\n",
       "      <td>0.832255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.571771</td>\n",
       "      <td>0.897734</td>\n",
       "      <td>0.835099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.536367</td>\n",
       "      <td>0.901603</td>\n",
       "      <td>0.834196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.575037</td>\n",
       "      <td>0.898839</td>\n",
       "      <td>0.830844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.602524</td>\n",
       "      <td>0.900221</td>\n",
       "      <td>0.835174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.601118</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.836231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.605465</td>\n",
       "      <td>0.901327</td>\n",
       "      <td>0.841257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.611550</td>\n",
       "      <td>0.905473</td>\n",
       "      <td>0.843221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.619238</td>\n",
       "      <td>0.901879</td>\n",
       "      <td>0.840358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.620143</td>\n",
       "      <td>0.899116</td>\n",
       "      <td>0.833510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.628591</td>\n",
       "      <td>0.898286</td>\n",
       "      <td>0.833417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.634144</td>\n",
       "      <td>0.900774</td>\n",
       "      <td>0.835619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.630259</td>\n",
       "      <td>0.900774</td>\n",
       "      <td>0.836528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.630136</td>\n",
       "      <td>0.898839</td>\n",
       "      <td>0.832808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 14469\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 3618\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fd6fb3fbbf45a795dd58e2ea588553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c54f5128d2c47a5b4f5ab22cd5bcc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:32, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.128400</td>\n",
       "      <td>1.105300</td>\n",
       "      <td>0.482034</td>\n",
       "      <td>0.250425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.073100</td>\n",
       "      <td>1.006594</td>\n",
       "      <td>0.563018</td>\n",
       "      <td>0.193819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.040100</td>\n",
       "      <td>1.002104</td>\n",
       "      <td>0.556661</td>\n",
       "      <td>0.276359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.038200</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.560254</td>\n",
       "      <td>0.271248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.005700</td>\n",
       "      <td>1.004147</td>\n",
       "      <td>0.549751</td>\n",
       "      <td>0.275969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.024100</td>\n",
       "      <td>0.984106</td>\n",
       "      <td>0.574350</td>\n",
       "      <td>0.304976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.002000</td>\n",
       "      <td>0.997178</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.991300</td>\n",
       "      <td>0.965805</td>\n",
       "      <td>0.567441</td>\n",
       "      <td>0.241017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>1.013663</td>\n",
       "      <td>0.576009</td>\n",
       "      <td>0.286966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.954230</td>\n",
       "      <td>0.575456</td>\n",
       "      <td>0.332636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.968700</td>\n",
       "      <td>0.965207</td>\n",
       "      <td>0.579049</td>\n",
       "      <td>0.310464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.925100</td>\n",
       "      <td>0.954958</td>\n",
       "      <td>0.580155</td>\n",
       "      <td>0.340377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.935500</td>\n",
       "      <td>0.978014</td>\n",
       "      <td>0.579602</td>\n",
       "      <td>0.354946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.936500</td>\n",
       "      <td>0.982155</td>\n",
       "      <td>0.587617</td>\n",
       "      <td>0.346768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.974620</td>\n",
       "      <td>0.571034</td>\n",
       "      <td>0.375086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>0.956174</td>\n",
       "      <td>0.596462</td>\n",
       "      <td>0.361233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.949244</td>\n",
       "      <td>0.588170</td>\n",
       "      <td>0.366409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.864900</td>\n",
       "      <td>0.949324</td>\n",
       "      <td>0.589829</td>\n",
       "      <td>0.358910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.885700</td>\n",
       "      <td>0.947281</td>\n",
       "      <td>0.590658</td>\n",
       "      <td>0.356780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.869900</td>\n",
       "      <td>0.953951</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.379320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.838500</td>\n",
       "      <td>0.928812</td>\n",
       "      <td>0.597568</td>\n",
       "      <td>0.359896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.842100</td>\n",
       "      <td>0.949468</td>\n",
       "      <td>0.597291</td>\n",
       "      <td>0.394926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.949526</td>\n",
       "      <td>0.596739</td>\n",
       "      <td>0.382809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.790900</td>\n",
       "      <td>0.952166</td>\n",
       "      <td>0.596462</td>\n",
       "      <td>0.423469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.810400</td>\n",
       "      <td>0.925196</td>\n",
       "      <td>0.612769</td>\n",
       "      <td>0.387832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.783400</td>\n",
       "      <td>0.952985</td>\n",
       "      <td>0.606965</td>\n",
       "      <td>0.436673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.753900</td>\n",
       "      <td>0.948510</td>\n",
       "      <td>0.610282</td>\n",
       "      <td>0.432864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.756200</td>\n",
       "      <td>0.938002</td>\n",
       "      <td>0.613322</td>\n",
       "      <td>0.427927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26da94ee50814cf8bdf88a8dee87ab0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9964090878542bea5f508b4f295bc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.128400</td>\n",
       "      <td>1.105300</td>\n",
       "      <td>0.482034</td>\n",
       "      <td>0.250425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.073100</td>\n",
       "      <td>1.006594</td>\n",
       "      <td>0.563018</td>\n",
       "      <td>0.193819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.040100</td>\n",
       "      <td>1.002104</td>\n",
       "      <td>0.556661</td>\n",
       "      <td>0.276359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.038200</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.560254</td>\n",
       "      <td>0.271248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.005700</td>\n",
       "      <td>1.004147</td>\n",
       "      <td>0.549751</td>\n",
       "      <td>0.275969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.024100</td>\n",
       "      <td>0.984106</td>\n",
       "      <td>0.574350</td>\n",
       "      <td>0.304976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.002000</td>\n",
       "      <td>0.997178</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.991300</td>\n",
       "      <td>0.965805</td>\n",
       "      <td>0.567441</td>\n",
       "      <td>0.241017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>1.013663</td>\n",
       "      <td>0.576009</td>\n",
       "      <td>0.286966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.954230</td>\n",
       "      <td>0.575456</td>\n",
       "      <td>0.332636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.968700</td>\n",
       "      <td>0.965207</td>\n",
       "      <td>0.579049</td>\n",
       "      <td>0.310464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.925100</td>\n",
       "      <td>0.954958</td>\n",
       "      <td>0.580155</td>\n",
       "      <td>0.340377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.935500</td>\n",
       "      <td>0.978014</td>\n",
       "      <td>0.579602</td>\n",
       "      <td>0.354946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.936500</td>\n",
       "      <td>0.982155</td>\n",
       "      <td>0.587617</td>\n",
       "      <td>0.346768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.974620</td>\n",
       "      <td>0.571034</td>\n",
       "      <td>0.375086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>0.956174</td>\n",
       "      <td>0.596462</td>\n",
       "      <td>0.361233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.949244</td>\n",
       "      <td>0.588170</td>\n",
       "      <td>0.366409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.864900</td>\n",
       "      <td>0.949324</td>\n",
       "      <td>0.589829</td>\n",
       "      <td>0.358910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.885700</td>\n",
       "      <td>0.947281</td>\n",
       "      <td>0.590658</td>\n",
       "      <td>0.356780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.869900</td>\n",
       "      <td>0.953951</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.379320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.838500</td>\n",
       "      <td>0.928812</td>\n",
       "      <td>0.597568</td>\n",
       "      <td>0.359896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.842100</td>\n",
       "      <td>0.949468</td>\n",
       "      <td>0.597291</td>\n",
       "      <td>0.394926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.949526</td>\n",
       "      <td>0.596739</td>\n",
       "      <td>0.382809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.790900</td>\n",
       "      <td>0.952166</td>\n",
       "      <td>0.596462</td>\n",
       "      <td>0.423469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.810400</td>\n",
       "      <td>0.925196</td>\n",
       "      <td>0.612769</td>\n",
       "      <td>0.387832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.783400</td>\n",
       "      <td>0.952985</td>\n",
       "      <td>0.606965</td>\n",
       "      <td>0.436673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.753900</td>\n",
       "      <td>0.948510</td>\n",
       "      <td>0.610282</td>\n",
       "      <td>0.432864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.756200</td>\n",
       "      <td>0.938002</td>\n",
       "      <td>0.613322</td>\n",
       "      <td>0.427927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5d07d6d7254e9e9b32f30ab9f6bcd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988e36afe56244baacbdecee5108675f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.128400</td>\n",
       "      <td>1.105300</td>\n",
       "      <td>0.482034</td>\n",
       "      <td>0.250425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.073100</td>\n",
       "      <td>1.006594</td>\n",
       "      <td>0.563018</td>\n",
       "      <td>0.193819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.040100</td>\n",
       "      <td>1.002104</td>\n",
       "      <td>0.556661</td>\n",
       "      <td>0.276359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.038200</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.560254</td>\n",
       "      <td>0.271248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.005700</td>\n",
       "      <td>1.004147</td>\n",
       "      <td>0.549751</td>\n",
       "      <td>0.275969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.024100</td>\n",
       "      <td>0.984106</td>\n",
       "      <td>0.574350</td>\n",
       "      <td>0.304976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.002000</td>\n",
       "      <td>0.997178</td>\n",
       "      <td>0.544500</td>\n",
       "      <td>0.290100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.991300</td>\n",
       "      <td>0.965805</td>\n",
       "      <td>0.567441</td>\n",
       "      <td>0.241017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>1.013663</td>\n",
       "      <td>0.576009</td>\n",
       "      <td>0.286966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.955600</td>\n",
       "      <td>0.954230</td>\n",
       "      <td>0.575456</td>\n",
       "      <td>0.332636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.968700</td>\n",
       "      <td>0.965207</td>\n",
       "      <td>0.579049</td>\n",
       "      <td>0.310464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.925100</td>\n",
       "      <td>0.954958</td>\n",
       "      <td>0.580155</td>\n",
       "      <td>0.340377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.935500</td>\n",
       "      <td>0.978014</td>\n",
       "      <td>0.579602</td>\n",
       "      <td>0.354946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.936500</td>\n",
       "      <td>0.982155</td>\n",
       "      <td>0.587617</td>\n",
       "      <td>0.346768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.915900</td>\n",
       "      <td>0.974620</td>\n",
       "      <td>0.571034</td>\n",
       "      <td>0.375086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.912400</td>\n",
       "      <td>0.956174</td>\n",
       "      <td>0.596462</td>\n",
       "      <td>0.361233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>0.949244</td>\n",
       "      <td>0.588170</td>\n",
       "      <td>0.366409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.864900</td>\n",
       "      <td>0.949324</td>\n",
       "      <td>0.589829</td>\n",
       "      <td>0.358910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.885700</td>\n",
       "      <td>0.947281</td>\n",
       "      <td>0.590658</td>\n",
       "      <td>0.356780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.869900</td>\n",
       "      <td>0.953951</td>\n",
       "      <td>0.597015</td>\n",
       "      <td>0.379320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.838500</td>\n",
       "      <td>0.928812</td>\n",
       "      <td>0.597568</td>\n",
       "      <td>0.359896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.842100</td>\n",
       "      <td>0.949468</td>\n",
       "      <td>0.597291</td>\n",
       "      <td>0.394926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.949526</td>\n",
       "      <td>0.596739</td>\n",
       "      <td>0.382809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.790900</td>\n",
       "      <td>0.952166</td>\n",
       "      <td>0.596462</td>\n",
       "      <td>0.423469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.810400</td>\n",
       "      <td>0.925196</td>\n",
       "      <td>0.612769</td>\n",
       "      <td>0.387832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.783400</td>\n",
       "      <td>0.952985</td>\n",
       "      <td>0.606965</td>\n",
       "      <td>0.436673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.753900</td>\n",
       "      <td>0.948510</td>\n",
       "      <td>0.610282</td>\n",
       "      <td>0.432864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.756200</td>\n",
       "      <td>0.938002</td>\n",
       "      <td>0.613322</td>\n",
       "      <td>0.427927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2559176bbb3548e286756570be800f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd76998f6d0f4583a4238fb56ce8ec1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.130300</td>\n",
       "      <td>1.068016</td>\n",
       "      <td>0.557490</td>\n",
       "      <td>0.178971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>1.023155</td>\n",
       "      <td>0.557490</td>\n",
       "      <td>0.178971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.058900</td>\n",
       "      <td>1.001812</td>\n",
       "      <td>0.565229</td>\n",
       "      <td>0.253576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.030100</td>\n",
       "      <td>1.044184</td>\n",
       "      <td>0.564953</td>\n",
       "      <td>0.223448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.019300</td>\n",
       "      <td>0.997346</td>\n",
       "      <td>0.551410</td>\n",
       "      <td>0.277117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.984896</td>\n",
       "      <td>0.567717</td>\n",
       "      <td>0.289845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.001300</td>\n",
       "      <td>0.983463</td>\n",
       "      <td>0.563847</td>\n",
       "      <td>0.289048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.985500</td>\n",
       "      <td>0.992035</td>\n",
       "      <td>0.566335</td>\n",
       "      <td>0.259580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.985300</td>\n",
       "      <td>0.968239</td>\n",
       "      <td>0.570481</td>\n",
       "      <td>0.280416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.968600</td>\n",
       "      <td>0.979761</td>\n",
       "      <td>0.554174</td>\n",
       "      <td>0.297559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.966600</td>\n",
       "      <td>0.958102</td>\n",
       "      <td>0.585683</td>\n",
       "      <td>0.344874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.975414</td>\n",
       "      <td>0.583472</td>\n",
       "      <td>0.334632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.957436</td>\n",
       "      <td>0.590381</td>\n",
       "      <td>0.366298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.951614</td>\n",
       "      <td>0.582642</td>\n",
       "      <td>0.326703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.923600</td>\n",
       "      <td>0.979307</td>\n",
       "      <td>0.589276</td>\n",
       "      <td>0.363704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.922100</td>\n",
       "      <td>0.947958</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.306437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.908900</td>\n",
       "      <td>0.949621</td>\n",
       "      <td>0.577944</td>\n",
       "      <td>0.335260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.883800</td>\n",
       "      <td>0.955541</td>\n",
       "      <td>0.586235</td>\n",
       "      <td>0.349305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.954733</td>\n",
       "      <td>0.593422</td>\n",
       "      <td>0.347152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.944849</td>\n",
       "      <td>0.598121</td>\n",
       "      <td>0.376181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.943997</td>\n",
       "      <td>0.598950</td>\n",
       "      <td>0.382607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.859200</td>\n",
       "      <td>0.950066</td>\n",
       "      <td>0.592869</td>\n",
       "      <td>0.358240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.936743</td>\n",
       "      <td>0.599502</td>\n",
       "      <td>0.379945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.949235</td>\n",
       "      <td>0.601990</td>\n",
       "      <td>0.367413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.959943</td>\n",
       "      <td>0.595909</td>\n",
       "      <td>0.389737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.818700</td>\n",
       "      <td>0.937808</td>\n",
       "      <td>0.601714</td>\n",
       "      <td>0.408805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.817700</td>\n",
       "      <td>0.951734</td>\n",
       "      <td>0.600332</td>\n",
       "      <td>0.412844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.946098</td>\n",
       "      <td>0.599779</td>\n",
       "      <td>0.394651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063058d0bc8c4e19a3eda8fb2fd3264b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32e62d3663c469dbb9cf30ca40e3193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.130300</td>\n",
       "      <td>1.068016</td>\n",
       "      <td>0.557490</td>\n",
       "      <td>0.178971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>1.023155</td>\n",
       "      <td>0.557490</td>\n",
       "      <td>0.178971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.058900</td>\n",
       "      <td>1.001812</td>\n",
       "      <td>0.565229</td>\n",
       "      <td>0.253576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.030100</td>\n",
       "      <td>1.044184</td>\n",
       "      <td>0.564953</td>\n",
       "      <td>0.223448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.019300</td>\n",
       "      <td>0.997346</td>\n",
       "      <td>0.551410</td>\n",
       "      <td>0.277117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.984896</td>\n",
       "      <td>0.567717</td>\n",
       "      <td>0.289845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.001300</td>\n",
       "      <td>0.983463</td>\n",
       "      <td>0.563847</td>\n",
       "      <td>0.289048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.985500</td>\n",
       "      <td>0.992035</td>\n",
       "      <td>0.566335</td>\n",
       "      <td>0.259580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.985300</td>\n",
       "      <td>0.968239</td>\n",
       "      <td>0.570481</td>\n",
       "      <td>0.280416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.968600</td>\n",
       "      <td>0.979761</td>\n",
       "      <td>0.554174</td>\n",
       "      <td>0.297559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.966600</td>\n",
       "      <td>0.958102</td>\n",
       "      <td>0.585683</td>\n",
       "      <td>0.344874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.975414</td>\n",
       "      <td>0.583472</td>\n",
       "      <td>0.334632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.957436</td>\n",
       "      <td>0.590381</td>\n",
       "      <td>0.366298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.951614</td>\n",
       "      <td>0.582642</td>\n",
       "      <td>0.326703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.923600</td>\n",
       "      <td>0.979307</td>\n",
       "      <td>0.589276</td>\n",
       "      <td>0.363704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.922100</td>\n",
       "      <td>0.947958</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.306437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.908900</td>\n",
       "      <td>0.949621</td>\n",
       "      <td>0.577944</td>\n",
       "      <td>0.335260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.883800</td>\n",
       "      <td>0.955541</td>\n",
       "      <td>0.586235</td>\n",
       "      <td>0.349305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.954733</td>\n",
       "      <td>0.593422</td>\n",
       "      <td>0.347152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.944849</td>\n",
       "      <td>0.598121</td>\n",
       "      <td>0.376181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.943997</td>\n",
       "      <td>0.598950</td>\n",
       "      <td>0.382607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.859200</td>\n",
       "      <td>0.950066</td>\n",
       "      <td>0.592869</td>\n",
       "      <td>0.358240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.936743</td>\n",
       "      <td>0.599502</td>\n",
       "      <td>0.379945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.949235</td>\n",
       "      <td>0.601990</td>\n",
       "      <td>0.367413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.959943</td>\n",
       "      <td>0.595909</td>\n",
       "      <td>0.389737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.818700</td>\n",
       "      <td>0.937808</td>\n",
       "      <td>0.601714</td>\n",
       "      <td>0.408805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.817700</td>\n",
       "      <td>0.951734</td>\n",
       "      <td>0.600332</td>\n",
       "      <td>0.412844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.946098</td>\n",
       "      <td>0.599779</td>\n",
       "      <td>0.394651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b5e2174c854f76addd7b3f75c95d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d350c7aabab84a62b14211b9934917c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.130300</td>\n",
       "      <td>1.068016</td>\n",
       "      <td>0.557490</td>\n",
       "      <td>0.178971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.085000</td>\n",
       "      <td>1.023155</td>\n",
       "      <td>0.557490</td>\n",
       "      <td>0.178971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.058900</td>\n",
       "      <td>1.001812</td>\n",
       "      <td>0.565229</td>\n",
       "      <td>0.253576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.030100</td>\n",
       "      <td>1.044184</td>\n",
       "      <td>0.564953</td>\n",
       "      <td>0.223448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.019300</td>\n",
       "      <td>0.997346</td>\n",
       "      <td>0.551410</td>\n",
       "      <td>0.277117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.984896</td>\n",
       "      <td>0.567717</td>\n",
       "      <td>0.289845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.001300</td>\n",
       "      <td>0.983463</td>\n",
       "      <td>0.563847</td>\n",
       "      <td>0.289048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.985500</td>\n",
       "      <td>0.992035</td>\n",
       "      <td>0.566335</td>\n",
       "      <td>0.259580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.985300</td>\n",
       "      <td>0.968239</td>\n",
       "      <td>0.570481</td>\n",
       "      <td>0.280416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.968600</td>\n",
       "      <td>0.979761</td>\n",
       "      <td>0.554174</td>\n",
       "      <td>0.297559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.966600</td>\n",
       "      <td>0.958102</td>\n",
       "      <td>0.585683</td>\n",
       "      <td>0.344874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.934500</td>\n",
       "      <td>0.975414</td>\n",
       "      <td>0.583472</td>\n",
       "      <td>0.334632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.957436</td>\n",
       "      <td>0.590381</td>\n",
       "      <td>0.366298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>0.951614</td>\n",
       "      <td>0.582642</td>\n",
       "      <td>0.326703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.923600</td>\n",
       "      <td>0.979307</td>\n",
       "      <td>0.589276</td>\n",
       "      <td>0.363704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.922100</td>\n",
       "      <td>0.947958</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.306437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.908900</td>\n",
       "      <td>0.949621</td>\n",
       "      <td>0.577944</td>\n",
       "      <td>0.335260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.883800</td>\n",
       "      <td>0.955541</td>\n",
       "      <td>0.586235</td>\n",
       "      <td>0.349305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.871700</td>\n",
       "      <td>0.954733</td>\n",
       "      <td>0.593422</td>\n",
       "      <td>0.347152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.893900</td>\n",
       "      <td>0.944849</td>\n",
       "      <td>0.598121</td>\n",
       "      <td>0.376181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.943997</td>\n",
       "      <td>0.598950</td>\n",
       "      <td>0.382607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.859200</td>\n",
       "      <td>0.950066</td>\n",
       "      <td>0.592869</td>\n",
       "      <td>0.358240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.844800</td>\n",
       "      <td>0.936743</td>\n",
       "      <td>0.599502</td>\n",
       "      <td>0.379945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.949235</td>\n",
       "      <td>0.601990</td>\n",
       "      <td>0.367413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.959943</td>\n",
       "      <td>0.595909</td>\n",
       "      <td>0.389737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.818700</td>\n",
       "      <td>0.937808</td>\n",
       "      <td>0.601714</td>\n",
       "      <td>0.408805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.817700</td>\n",
       "      <td>0.951734</td>\n",
       "      <td>0.600332</td>\n",
       "      <td>0.412844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.946098</td>\n",
       "      <td>0.599779</td>\n",
       "      <td>0.394651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218b1d827cf94449b8f31ed29e638fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3156073df0ff472f8f746a496596f9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.114600</td>\n",
       "      <td>1.051322</td>\n",
       "      <td>0.525981</td>\n",
       "      <td>0.252828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.063800</td>\n",
       "      <td>1.007896</td>\n",
       "      <td>0.558596</td>\n",
       "      <td>0.285346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.049500</td>\n",
       "      <td>1.035597</td>\n",
       "      <td>0.488944</td>\n",
       "      <td>0.269117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.018500</td>\n",
       "      <td>0.988897</td>\n",
       "      <td>0.566888</td>\n",
       "      <td>0.276663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.018600</td>\n",
       "      <td>0.988837</td>\n",
       "      <td>0.546711</td>\n",
       "      <td>0.274806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.988200</td>\n",
       "      <td>0.971189</td>\n",
       "      <td>0.558872</td>\n",
       "      <td>0.281794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>1.040122</td>\n",
       "      <td>0.566059</td>\n",
       "      <td>0.244673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.985800</td>\n",
       "      <td>0.964334</td>\n",
       "      <td>0.585406</td>\n",
       "      <td>0.316109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.964590</td>\n",
       "      <td>0.583195</td>\n",
       "      <td>0.313883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.975270</td>\n",
       "      <td>0.556661</td>\n",
       "      <td>0.327899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>0.986202</td>\n",
       "      <td>0.566335</td>\n",
       "      <td>0.302225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.984039</td>\n",
       "      <td>0.571310</td>\n",
       "      <td>0.247084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.937400</td>\n",
       "      <td>0.980730</td>\n",
       "      <td>0.572968</td>\n",
       "      <td>0.256014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>0.959741</td>\n",
       "      <td>0.578773</td>\n",
       "      <td>0.377641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.937800</td>\n",
       "      <td>0.930751</td>\n",
       "      <td>0.588723</td>\n",
       "      <td>0.344118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.896500</td>\n",
       "      <td>0.933356</td>\n",
       "      <td>0.587341</td>\n",
       "      <td>0.314685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.927979</td>\n",
       "      <td>0.589552</td>\n",
       "      <td>0.383629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.878900</td>\n",
       "      <td>0.923910</td>\n",
       "      <td>0.595633</td>\n",
       "      <td>0.375156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.885100</td>\n",
       "      <td>0.930821</td>\n",
       "      <td>0.597291</td>\n",
       "      <td>0.365216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.879600</td>\n",
       "      <td>0.913895</td>\n",
       "      <td>0.601161</td>\n",
       "      <td>0.367380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.857300</td>\n",
       "      <td>0.931266</td>\n",
       "      <td>0.599226</td>\n",
       "      <td>0.372832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.833400</td>\n",
       "      <td>0.917761</td>\n",
       "      <td>0.603096</td>\n",
       "      <td>0.348496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.826400</td>\n",
       "      <td>0.922380</td>\n",
       "      <td>0.608071</td>\n",
       "      <td>0.426955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.924135</td>\n",
       "      <td>0.600884</td>\n",
       "      <td>0.397068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.817900</td>\n",
       "      <td>0.914025</td>\n",
       "      <td>0.605860</td>\n",
       "      <td>0.382591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.778900</td>\n",
       "      <td>0.930696</td>\n",
       "      <td>0.602543</td>\n",
       "      <td>0.439308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.767700</td>\n",
       "      <td>0.931061</td>\n",
       "      <td>0.601990</td>\n",
       "      <td>0.409781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.791500</td>\n",
       "      <td>0.926629</td>\n",
       "      <td>0.605860</td>\n",
       "      <td>0.422213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f089387dd9453d92939ae680622a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e183fdc6d3475fb694bbe0889d20c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:32, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.114600</td>\n",
       "      <td>1.051322</td>\n",
       "      <td>0.525981</td>\n",
       "      <td>0.252828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.063800</td>\n",
       "      <td>1.007896</td>\n",
       "      <td>0.558596</td>\n",
       "      <td>0.285346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.049500</td>\n",
       "      <td>1.035597</td>\n",
       "      <td>0.488944</td>\n",
       "      <td>0.269117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.018500</td>\n",
       "      <td>0.988897</td>\n",
       "      <td>0.566888</td>\n",
       "      <td>0.276663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.018600</td>\n",
       "      <td>0.988837</td>\n",
       "      <td>0.546711</td>\n",
       "      <td>0.274806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.988200</td>\n",
       "      <td>0.971189</td>\n",
       "      <td>0.558872</td>\n",
       "      <td>0.281794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>1.040122</td>\n",
       "      <td>0.566059</td>\n",
       "      <td>0.244673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.985800</td>\n",
       "      <td>0.964334</td>\n",
       "      <td>0.585406</td>\n",
       "      <td>0.316109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.964590</td>\n",
       "      <td>0.583195</td>\n",
       "      <td>0.313883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.975270</td>\n",
       "      <td>0.556661</td>\n",
       "      <td>0.327899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>0.986202</td>\n",
       "      <td>0.566335</td>\n",
       "      <td>0.302225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.984039</td>\n",
       "      <td>0.571310</td>\n",
       "      <td>0.247084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.937400</td>\n",
       "      <td>0.980730</td>\n",
       "      <td>0.572968</td>\n",
       "      <td>0.256014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>0.959741</td>\n",
       "      <td>0.578773</td>\n",
       "      <td>0.377641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.937800</td>\n",
       "      <td>0.930751</td>\n",
       "      <td>0.588723</td>\n",
       "      <td>0.344118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.896500</td>\n",
       "      <td>0.933356</td>\n",
       "      <td>0.587341</td>\n",
       "      <td>0.314685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.927979</td>\n",
       "      <td>0.589552</td>\n",
       "      <td>0.383629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.878900</td>\n",
       "      <td>0.923910</td>\n",
       "      <td>0.595633</td>\n",
       "      <td>0.375156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.885100</td>\n",
       "      <td>0.930821</td>\n",
       "      <td>0.597291</td>\n",
       "      <td>0.365216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.879600</td>\n",
       "      <td>0.913895</td>\n",
       "      <td>0.601161</td>\n",
       "      <td>0.367380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.857300</td>\n",
       "      <td>0.931266</td>\n",
       "      <td>0.599226</td>\n",
       "      <td>0.372832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.833400</td>\n",
       "      <td>0.917761</td>\n",
       "      <td>0.603096</td>\n",
       "      <td>0.348496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.826400</td>\n",
       "      <td>0.922380</td>\n",
       "      <td>0.608071</td>\n",
       "      <td>0.426955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.924135</td>\n",
       "      <td>0.600884</td>\n",
       "      <td>0.397068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.817900</td>\n",
       "      <td>0.914025</td>\n",
       "      <td>0.605860</td>\n",
       "      <td>0.382591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.778900</td>\n",
       "      <td>0.930696</td>\n",
       "      <td>0.602543</td>\n",
       "      <td>0.439308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.767700</td>\n",
       "      <td>0.931061</td>\n",
       "      <td>0.601990</td>\n",
       "      <td>0.409781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.791500</td>\n",
       "      <td>0.926629</td>\n",
       "      <td>0.605860</td>\n",
       "      <td>0.422213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tunis-ai/TunBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at tunis-ai/TunBERT and are newly initialized: ['classifier.bias', 'classifier.weight', 'embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f504286ccded4e3bab6af180f9911c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23a6ff879df4317a8372b24320141d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:32, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.114600</td>\n",
       "      <td>1.051322</td>\n",
       "      <td>0.525981</td>\n",
       "      <td>0.252828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.063800</td>\n",
       "      <td>1.007896</td>\n",
       "      <td>0.558596</td>\n",
       "      <td>0.285346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.049500</td>\n",
       "      <td>1.035597</td>\n",
       "      <td>0.488944</td>\n",
       "      <td>0.269117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.018500</td>\n",
       "      <td>0.988897</td>\n",
       "      <td>0.566888</td>\n",
       "      <td>0.276663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.018600</td>\n",
       "      <td>0.988837</td>\n",
       "      <td>0.546711</td>\n",
       "      <td>0.274806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.988200</td>\n",
       "      <td>0.971189</td>\n",
       "      <td>0.558872</td>\n",
       "      <td>0.281794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.981100</td>\n",
       "      <td>1.040122</td>\n",
       "      <td>0.566059</td>\n",
       "      <td>0.244673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.985800</td>\n",
       "      <td>0.964334</td>\n",
       "      <td>0.585406</td>\n",
       "      <td>0.316109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.964590</td>\n",
       "      <td>0.583195</td>\n",
       "      <td>0.313883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.972500</td>\n",
       "      <td>0.975270</td>\n",
       "      <td>0.556661</td>\n",
       "      <td>0.327899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.965300</td>\n",
       "      <td>0.986202</td>\n",
       "      <td>0.566335</td>\n",
       "      <td>0.302225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.956000</td>\n",
       "      <td>0.984039</td>\n",
       "      <td>0.571310</td>\n",
       "      <td>0.247084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.937400</td>\n",
       "      <td>0.980730</td>\n",
       "      <td>0.572968</td>\n",
       "      <td>0.256014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>0.959741</td>\n",
       "      <td>0.578773</td>\n",
       "      <td>0.377641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.937800</td>\n",
       "      <td>0.930751</td>\n",
       "      <td>0.588723</td>\n",
       "      <td>0.344118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.896500</td>\n",
       "      <td>0.933356</td>\n",
       "      <td>0.587341</td>\n",
       "      <td>0.314685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.927979</td>\n",
       "      <td>0.589552</td>\n",
       "      <td>0.383629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.878900</td>\n",
       "      <td>0.923910</td>\n",
       "      <td>0.595633</td>\n",
       "      <td>0.375156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.885100</td>\n",
       "      <td>0.930821</td>\n",
       "      <td>0.597291</td>\n",
       "      <td>0.365216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.879600</td>\n",
       "      <td>0.913895</td>\n",
       "      <td>0.601161</td>\n",
       "      <td>0.367380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.857300</td>\n",
       "      <td>0.931266</td>\n",
       "      <td>0.599226</td>\n",
       "      <td>0.372832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.833400</td>\n",
       "      <td>0.917761</td>\n",
       "      <td>0.603096</td>\n",
       "      <td>0.348496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.826400</td>\n",
       "      <td>0.922380</td>\n",
       "      <td>0.608071</td>\n",
       "      <td>0.426955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.924135</td>\n",
       "      <td>0.600884</td>\n",
       "      <td>0.397068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.817900</td>\n",
       "      <td>0.914025</td>\n",
       "      <td>0.605860</td>\n",
       "      <td>0.382591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.778900</td>\n",
       "      <td>0.930696</td>\n",
       "      <td>0.602543</td>\n",
       "      <td>0.439308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.767700</td>\n",
       "      <td>0.931061</td>\n",
       "      <td>0.601990</td>\n",
       "      <td>0.409781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.791500</td>\n",
       "      <td>0.926629</td>\n",
       "      <td>0.605860</td>\n",
       "      <td>0.422213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 14469\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 3618\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a42325f3c7cd491b99a1dfc109b15358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b63cf26978c4a71a918fc571a6c0bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.711400</td>\n",
       "      <td>0.600578</td>\n",
       "      <td>0.770315</td>\n",
       "      <td>0.555480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.482046</td>\n",
       "      <td>0.821448</td>\n",
       "      <td>0.614642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.461653</td>\n",
       "      <td>0.844389</td>\n",
       "      <td>0.693453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>0.474056</td>\n",
       "      <td>0.844666</td>\n",
       "      <td>0.691889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.422411</td>\n",
       "      <td>0.864013</td>\n",
       "      <td>0.746237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.520346</td>\n",
       "      <td>0.867883</td>\n",
       "      <td>0.765875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.542095</td>\n",
       "      <td>0.861802</td>\n",
       "      <td>0.758092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.544756</td>\n",
       "      <td>0.868988</td>\n",
       "      <td>0.776356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.569597</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.770083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.585998</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.777491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.608538</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.758625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.627558</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.784552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.666354</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.777489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.599527</td>\n",
       "      <td>0.870923</td>\n",
       "      <td>0.779395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.711309</td>\n",
       "      <td>0.870647</td>\n",
       "      <td>0.779496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.716540</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>0.783039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.684562</td>\n",
       "      <td>0.875069</td>\n",
       "      <td>0.789972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.738221</td>\n",
       "      <td>0.875898</td>\n",
       "      <td>0.786121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.732654</td>\n",
       "      <td>0.878386</td>\n",
       "      <td>0.795609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.752473</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.792288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.781425</td>\n",
       "      <td>0.878109</td>\n",
       "      <td>0.794317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.756063</td>\n",
       "      <td>0.879768</td>\n",
       "      <td>0.796543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.733900</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.797894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.779088</td>\n",
       "      <td>0.882532</td>\n",
       "      <td>0.801190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.779638</td>\n",
       "      <td>0.878939</td>\n",
       "      <td>0.791320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.785832</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.793924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.788211</td>\n",
       "      <td>0.878939</td>\n",
       "      <td>0.794471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.787501</td>\n",
       "      <td>0.880321</td>\n",
       "      <td>0.796106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a953a4461474749954d0633259cc392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f83e64be894e8ea43bf8086d2e7a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.711400</td>\n",
       "      <td>0.600578</td>\n",
       "      <td>0.770315</td>\n",
       "      <td>0.555480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.482046</td>\n",
       "      <td>0.821448</td>\n",
       "      <td>0.614642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.461653</td>\n",
       "      <td>0.844389</td>\n",
       "      <td>0.693453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>0.474056</td>\n",
       "      <td>0.844666</td>\n",
       "      <td>0.691889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.422411</td>\n",
       "      <td>0.864013</td>\n",
       "      <td>0.746237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.520346</td>\n",
       "      <td>0.867883</td>\n",
       "      <td>0.765875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.542095</td>\n",
       "      <td>0.861802</td>\n",
       "      <td>0.758092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.544756</td>\n",
       "      <td>0.868988</td>\n",
       "      <td>0.776356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.569597</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.770083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.585998</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.777491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.608538</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.758625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.627558</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.784552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.666354</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.777489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.599527</td>\n",
       "      <td>0.870923</td>\n",
       "      <td>0.779395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.711309</td>\n",
       "      <td>0.870647</td>\n",
       "      <td>0.779496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.716540</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>0.783039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.684562</td>\n",
       "      <td>0.875069</td>\n",
       "      <td>0.789972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.738221</td>\n",
       "      <td>0.875898</td>\n",
       "      <td>0.786121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.732654</td>\n",
       "      <td>0.878386</td>\n",
       "      <td>0.795609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.752473</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.792288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.781425</td>\n",
       "      <td>0.878109</td>\n",
       "      <td>0.794317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.756063</td>\n",
       "      <td>0.879768</td>\n",
       "      <td>0.796543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.733900</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.797894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.779088</td>\n",
       "      <td>0.882532</td>\n",
       "      <td>0.801190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.779638</td>\n",
       "      <td>0.878939</td>\n",
       "      <td>0.791320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.785832</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.793924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.788211</td>\n",
       "      <td>0.878939</td>\n",
       "      <td>0.794471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.787501</td>\n",
       "      <td>0.880321</td>\n",
       "      <td>0.796106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f901ad5e82a34fc2a34b110be8d88132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3be23b80551403a849d444a8a712fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.711400</td>\n",
       "      <td>0.600578</td>\n",
       "      <td>0.770315</td>\n",
       "      <td>0.555480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.562800</td>\n",
       "      <td>0.482046</td>\n",
       "      <td>0.821448</td>\n",
       "      <td>0.614642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.454000</td>\n",
       "      <td>0.461653</td>\n",
       "      <td>0.844389</td>\n",
       "      <td>0.693453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.272800</td>\n",
       "      <td>0.474056</td>\n",
       "      <td>0.844666</td>\n",
       "      <td>0.691889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.422411</td>\n",
       "      <td>0.864013</td>\n",
       "      <td>0.746237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.520346</td>\n",
       "      <td>0.867883</td>\n",
       "      <td>0.765875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.542095</td>\n",
       "      <td>0.861802</td>\n",
       "      <td>0.758092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.112600</td>\n",
       "      <td>0.544756</td>\n",
       "      <td>0.868988</td>\n",
       "      <td>0.776356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.569597</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.770083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.585998</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.777491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.072100</td>\n",
       "      <td>0.608538</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.758625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.041700</td>\n",
       "      <td>0.627558</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.784552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.666354</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.777489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.599527</td>\n",
       "      <td>0.870923</td>\n",
       "      <td>0.779395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.711309</td>\n",
       "      <td>0.870647</td>\n",
       "      <td>0.779496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.716540</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>0.783039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.684562</td>\n",
       "      <td>0.875069</td>\n",
       "      <td>0.789972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.738221</td>\n",
       "      <td>0.875898</td>\n",
       "      <td>0.786121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.732654</td>\n",
       "      <td>0.878386</td>\n",
       "      <td>0.795609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.752473</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.792288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.781425</td>\n",
       "      <td>0.878109</td>\n",
       "      <td>0.794317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.756063</td>\n",
       "      <td>0.879768</td>\n",
       "      <td>0.796543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.733900</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.797894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.779088</td>\n",
       "      <td>0.882532</td>\n",
       "      <td>0.801190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.779638</td>\n",
       "      <td>0.878939</td>\n",
       "      <td>0.791320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.785832</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.793924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.788211</td>\n",
       "      <td>0.878939</td>\n",
       "      <td>0.794471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.787501</td>\n",
       "      <td>0.880321</td>\n",
       "      <td>0.796106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53887e607694ef48f7b695cfee9b55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca20154fb454fe4aad2ee9851e96674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.586255</td>\n",
       "      <td>0.785793</td>\n",
       "      <td>0.563649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.495278</td>\n",
       "      <td>0.811222</td>\n",
       "      <td>0.647517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.518481</td>\n",
       "      <td>0.830569</td>\n",
       "      <td>0.708879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.527058</td>\n",
       "      <td>0.841349</td>\n",
       "      <td>0.697358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.462947</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.750712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.856551</td>\n",
       "      <td>0.747783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.588174</td>\n",
       "      <td>0.855998</td>\n",
       "      <td>0.739370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.743429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.634824</td>\n",
       "      <td>0.860420</td>\n",
       "      <td>0.753684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.628436</td>\n",
       "      <td>0.862078</td>\n",
       "      <td>0.742832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.755608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.715528</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.755428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.742124</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.756408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.702541</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.774590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.753397</td>\n",
       "      <td>0.864566</td>\n",
       "      <td>0.756242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.733549</td>\n",
       "      <td>0.866501</td>\n",
       "      <td>0.770675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.726015</td>\n",
       "      <td>0.869818</td>\n",
       "      <td>0.777642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.767376</td>\n",
       "      <td>0.862355</td>\n",
       "      <td>0.765011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.770796</td>\n",
       "      <td>0.862631</td>\n",
       "      <td>0.767126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.781398</td>\n",
       "      <td>0.867883</td>\n",
       "      <td>0.771514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.807938</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.773316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.793425</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.772637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.798197</td>\n",
       "      <td>0.868712</td>\n",
       "      <td>0.778879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.835809</td>\n",
       "      <td>0.869818</td>\n",
       "      <td>0.783594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.861116</td>\n",
       "      <td>0.865395</td>\n",
       "      <td>0.774288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.869971</td>\n",
       "      <td>0.864842</td>\n",
       "      <td>0.769963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.858660</td>\n",
       "      <td>0.864290</td>\n",
       "      <td>0.771666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.857753</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.772073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee952bdc96b44faa35a4fd58b8269d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbebd7fbba444e6a870ad49ab6164f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.586255</td>\n",
       "      <td>0.785793</td>\n",
       "      <td>0.563649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.495278</td>\n",
       "      <td>0.811222</td>\n",
       "      <td>0.647517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.518481</td>\n",
       "      <td>0.830569</td>\n",
       "      <td>0.708879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.527058</td>\n",
       "      <td>0.841349</td>\n",
       "      <td>0.697358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.462947</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.750712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.856551</td>\n",
       "      <td>0.747783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.588174</td>\n",
       "      <td>0.855998</td>\n",
       "      <td>0.739370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.743429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.634824</td>\n",
       "      <td>0.860420</td>\n",
       "      <td>0.753684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.628436</td>\n",
       "      <td>0.862078</td>\n",
       "      <td>0.742832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.755608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.715528</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.755428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.742124</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.756408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.702541</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.774590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.753397</td>\n",
       "      <td>0.864566</td>\n",
       "      <td>0.756242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.733549</td>\n",
       "      <td>0.866501</td>\n",
       "      <td>0.770675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.726015</td>\n",
       "      <td>0.869818</td>\n",
       "      <td>0.777642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.767376</td>\n",
       "      <td>0.862355</td>\n",
       "      <td>0.765011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.770796</td>\n",
       "      <td>0.862631</td>\n",
       "      <td>0.767126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.781398</td>\n",
       "      <td>0.867883</td>\n",
       "      <td>0.771514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.807938</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.773316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.793425</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.772637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.798197</td>\n",
       "      <td>0.868712</td>\n",
       "      <td>0.778879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.835809</td>\n",
       "      <td>0.869818</td>\n",
       "      <td>0.783594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.861116</td>\n",
       "      <td>0.865395</td>\n",
       "      <td>0.774288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.869971</td>\n",
       "      <td>0.864842</td>\n",
       "      <td>0.769963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.858660</td>\n",
       "      <td>0.864290</td>\n",
       "      <td>0.771666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.857753</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.772073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42265e2cda2548b9960caaa4c001ff1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c833005eba43db9fbfecbc0a4f703a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>0.586255</td>\n",
       "      <td>0.785793</td>\n",
       "      <td>0.563649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.551500</td>\n",
       "      <td>0.495278</td>\n",
       "      <td>0.811222</td>\n",
       "      <td>0.647517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.518481</td>\n",
       "      <td>0.830569</td>\n",
       "      <td>0.708879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.254000</td>\n",
       "      <td>0.527058</td>\n",
       "      <td>0.841349</td>\n",
       "      <td>0.697358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.462947</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.750712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.183700</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.856551</td>\n",
       "      <td>0.747783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.588174</td>\n",
       "      <td>0.855998</td>\n",
       "      <td>0.739370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.545600</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.743429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.107400</td>\n",
       "      <td>0.634824</td>\n",
       "      <td>0.860420</td>\n",
       "      <td>0.753684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.079400</td>\n",
       "      <td>0.628436</td>\n",
       "      <td>0.862078</td>\n",
       "      <td>0.742832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.667619</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.755608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.715528</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.755428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.053100</td>\n",
       "      <td>0.742124</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.756408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.051900</td>\n",
       "      <td>0.702541</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.774590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.753397</td>\n",
       "      <td>0.864566</td>\n",
       "      <td>0.756242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.733549</td>\n",
       "      <td>0.866501</td>\n",
       "      <td>0.770675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.726015</td>\n",
       "      <td>0.869818</td>\n",
       "      <td>0.777642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.767376</td>\n",
       "      <td>0.862355</td>\n",
       "      <td>0.765011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.770796</td>\n",
       "      <td>0.862631</td>\n",
       "      <td>0.767126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.781398</td>\n",
       "      <td>0.867883</td>\n",
       "      <td>0.771514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.807938</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.773316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.793425</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.772637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.798197</td>\n",
       "      <td>0.868712</td>\n",
       "      <td>0.778879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.835809</td>\n",
       "      <td>0.869818</td>\n",
       "      <td>0.783594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.861116</td>\n",
       "      <td>0.865395</td>\n",
       "      <td>0.774288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.869971</td>\n",
       "      <td>0.864842</td>\n",
       "      <td>0.769963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.858660</td>\n",
       "      <td>0.864290</td>\n",
       "      <td>0.771666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.016300</td>\n",
       "      <td>0.857753</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.772073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60fbf1c968854bf087bcb4e4b5ea421c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f76b3a426e411db3d4b829ec593bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.703900</td>\n",
       "      <td>0.600609</td>\n",
       "      <td>0.772803</td>\n",
       "      <td>0.557633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.557500</td>\n",
       "      <td>0.488068</td>\n",
       "      <td>0.819237</td>\n",
       "      <td>0.635259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.470338</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.702779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.446147</td>\n",
       "      <td>0.847430</td>\n",
       "      <td>0.726399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>0.491177</td>\n",
       "      <td>0.854892</td>\n",
       "      <td>0.755281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.538926</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.756566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.569967</td>\n",
       "      <td>0.861526</td>\n",
       "      <td>0.769099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.530933</td>\n",
       "      <td>0.863737</td>\n",
       "      <td>0.773078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.084100</td>\n",
       "      <td>0.625041</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.785844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.663580</td>\n",
       "      <td>0.866224</td>\n",
       "      <td>0.772008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.648221</td>\n",
       "      <td>0.868988</td>\n",
       "      <td>0.785963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.701218</td>\n",
       "      <td>0.874240</td>\n",
       "      <td>0.801615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.702314</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.796761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.652429</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.793119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.682986</td>\n",
       "      <td>0.869265</td>\n",
       "      <td>0.799161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.689475</td>\n",
       "      <td>0.871752</td>\n",
       "      <td>0.799447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.736321</td>\n",
       "      <td>0.864842</td>\n",
       "      <td>0.775372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.754111</td>\n",
       "      <td>0.871752</td>\n",
       "      <td>0.795253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.792880</td>\n",
       "      <td>0.867883</td>\n",
       "      <td>0.792295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.786904</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.787151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.784946</td>\n",
       "      <td>0.873411</td>\n",
       "      <td>0.797056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.777606</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.807656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.765389</td>\n",
       "      <td>0.872029</td>\n",
       "      <td>0.794599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.793472</td>\n",
       "      <td>0.872305</td>\n",
       "      <td>0.794368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.821342</td>\n",
       "      <td>0.870094</td>\n",
       "      <td>0.791701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.820252</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.793384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.817383</td>\n",
       "      <td>0.871752</td>\n",
       "      <td>0.796499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.820518</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.796085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38068cf85c08406785de2ee46c68899b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4c53d8f8f64295bd8f660d9d0261c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.703900</td>\n",
       "      <td>0.600609</td>\n",
       "      <td>0.772803</td>\n",
       "      <td>0.557633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.557500</td>\n",
       "      <td>0.488068</td>\n",
       "      <td>0.819237</td>\n",
       "      <td>0.635259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.470338</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.702779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.446147</td>\n",
       "      <td>0.847430</td>\n",
       "      <td>0.726399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>0.491177</td>\n",
       "      <td>0.854892</td>\n",
       "      <td>0.755281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.538926</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.756566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.569967</td>\n",
       "      <td>0.861526</td>\n",
       "      <td>0.769099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.530933</td>\n",
       "      <td>0.863737</td>\n",
       "      <td>0.773078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.084100</td>\n",
       "      <td>0.625041</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.785844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.663580</td>\n",
       "      <td>0.866224</td>\n",
       "      <td>0.772008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.648221</td>\n",
       "      <td>0.868988</td>\n",
       "      <td>0.785963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.701218</td>\n",
       "      <td>0.874240</td>\n",
       "      <td>0.801615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.702314</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.796761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.652429</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.793119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.682986</td>\n",
       "      <td>0.869265</td>\n",
       "      <td>0.799161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.689475</td>\n",
       "      <td>0.871752</td>\n",
       "      <td>0.799447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.736321</td>\n",
       "      <td>0.864842</td>\n",
       "      <td>0.775372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.754111</td>\n",
       "      <td>0.871752</td>\n",
       "      <td>0.795253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.792880</td>\n",
       "      <td>0.867883</td>\n",
       "      <td>0.792295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.786904</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.787151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.784946</td>\n",
       "      <td>0.873411</td>\n",
       "      <td>0.797056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.777606</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.807656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.765389</td>\n",
       "      <td>0.872029</td>\n",
       "      <td>0.794599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.793472</td>\n",
       "      <td>0.872305</td>\n",
       "      <td>0.794368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.821342</td>\n",
       "      <td>0.870094</td>\n",
       "      <td>0.791701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.820252</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.793384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.817383</td>\n",
       "      <td>0.871752</td>\n",
       "      <td>0.796499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.820518</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.796085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alger-ia/dziribert, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9761d8c8194959b6ee754398ea59d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371bbdec2159481f868f0869a3da6259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.703900</td>\n",
       "      <td>0.600609</td>\n",
       "      <td>0.772803</td>\n",
       "      <td>0.557633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.557500</td>\n",
       "      <td>0.488068</td>\n",
       "      <td>0.819237</td>\n",
       "      <td>0.635259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.463700</td>\n",
       "      <td>0.470338</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.702779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.446147</td>\n",
       "      <td>0.847430</td>\n",
       "      <td>0.726399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.255600</td>\n",
       "      <td>0.491177</td>\n",
       "      <td>0.854892</td>\n",
       "      <td>0.755281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>0.538926</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.756566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.569967</td>\n",
       "      <td>0.861526</td>\n",
       "      <td>0.769099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.118600</td>\n",
       "      <td>0.530933</td>\n",
       "      <td>0.863737</td>\n",
       "      <td>0.773078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.084100</td>\n",
       "      <td>0.625041</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.785844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.663580</td>\n",
       "      <td>0.866224</td>\n",
       "      <td>0.772008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.648221</td>\n",
       "      <td>0.868988</td>\n",
       "      <td>0.785963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.701218</td>\n",
       "      <td>0.874240</td>\n",
       "      <td>0.801615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.702314</td>\n",
       "      <td>0.873134</td>\n",
       "      <td>0.796761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.652429</td>\n",
       "      <td>0.866777</td>\n",
       "      <td>0.793119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.682986</td>\n",
       "      <td>0.869265</td>\n",
       "      <td>0.799161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.689475</td>\n",
       "      <td>0.871752</td>\n",
       "      <td>0.799447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.033900</td>\n",
       "      <td>0.736321</td>\n",
       "      <td>0.864842</td>\n",
       "      <td>0.775372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.754111</td>\n",
       "      <td>0.871752</td>\n",
       "      <td>0.795253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.792880</td>\n",
       "      <td>0.867883</td>\n",
       "      <td>0.792295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.786904</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.787151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.784946</td>\n",
       "      <td>0.873411</td>\n",
       "      <td>0.797056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.022600</td>\n",
       "      <td>0.777606</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.807656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.765389</td>\n",
       "      <td>0.872029</td>\n",
       "      <td>0.794599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.793472</td>\n",
       "      <td>0.872305</td>\n",
       "      <td>0.794368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.821342</td>\n",
       "      <td>0.870094</td>\n",
       "      <td>0.791701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.820252</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.793384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.817383</td>\n",
       "      <td>0.871752</td>\n",
       "      <td>0.796499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.820518</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.796085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 14469\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 3618\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ad3b9218ec4f8189aafb62921b98e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b71e42306c4c81b175edd13ce23eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.644586</td>\n",
       "      <td>0.759812</td>\n",
       "      <td>0.540449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.592800</td>\n",
       "      <td>0.502976</td>\n",
       "      <td>0.820066</td>\n",
       "      <td>0.625149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.515620</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.701419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.486978</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.668491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.315600</td>\n",
       "      <td>0.459927</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.728115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.531814</td>\n",
       "      <td>0.865119</td>\n",
       "      <td>0.766917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.564025</td>\n",
       "      <td>0.857103</td>\n",
       "      <td>0.744811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.540360</td>\n",
       "      <td>0.864290</td>\n",
       "      <td>0.756202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.587121</td>\n",
       "      <td>0.864566</td>\n",
       "      <td>0.767554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.571209</td>\n",
       "      <td>0.865395</td>\n",
       "      <td>0.767198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.551106</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.783731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.631367</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.787737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.659262</td>\n",
       "      <td>0.880321</td>\n",
       "      <td>0.799177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.617206</td>\n",
       "      <td>0.872305</td>\n",
       "      <td>0.780274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.682634</td>\n",
       "      <td>0.881426</td>\n",
       "      <td>0.807595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.732262</td>\n",
       "      <td>0.870094</td>\n",
       "      <td>0.778626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.671066</td>\n",
       "      <td>0.875069</td>\n",
       "      <td>0.792072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.750981</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.792188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.747133</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.796644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.728955</td>\n",
       "      <td>0.883914</td>\n",
       "      <td>0.810420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.772790</td>\n",
       "      <td>0.881426</td>\n",
       "      <td>0.803659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.768424</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.800325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.758536</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.809366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.792672</td>\n",
       "      <td>0.879215</td>\n",
       "      <td>0.795357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.790547</td>\n",
       "      <td>0.883637</td>\n",
       "      <td>0.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.796086</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.806811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.795205</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>0.805984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.795783</td>\n",
       "      <td>0.882532</td>\n",
       "      <td>0.806285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c0e71b7bb545c587eeeec43a3f365b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0dec14a8d84121bb09b08d4ac8f50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.644586</td>\n",
       "      <td>0.759812</td>\n",
       "      <td>0.540449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.592800</td>\n",
       "      <td>0.502976</td>\n",
       "      <td>0.820066</td>\n",
       "      <td>0.625149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.515620</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.701419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.486978</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.668491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.315600</td>\n",
       "      <td>0.459927</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.728115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.531814</td>\n",
       "      <td>0.865119</td>\n",
       "      <td>0.766917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.564025</td>\n",
       "      <td>0.857103</td>\n",
       "      <td>0.744811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.540360</td>\n",
       "      <td>0.864290</td>\n",
       "      <td>0.756202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.587121</td>\n",
       "      <td>0.864566</td>\n",
       "      <td>0.767554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.571209</td>\n",
       "      <td>0.865395</td>\n",
       "      <td>0.767198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.551106</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.783731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.631367</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.787737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.659262</td>\n",
       "      <td>0.880321</td>\n",
       "      <td>0.799177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.617206</td>\n",
       "      <td>0.872305</td>\n",
       "      <td>0.780274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.682634</td>\n",
       "      <td>0.881426</td>\n",
       "      <td>0.807595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.732262</td>\n",
       "      <td>0.870094</td>\n",
       "      <td>0.778626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.671066</td>\n",
       "      <td>0.875069</td>\n",
       "      <td>0.792072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.750981</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.792188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.747133</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.796644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.728955</td>\n",
       "      <td>0.883914</td>\n",
       "      <td>0.810420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.772790</td>\n",
       "      <td>0.881426</td>\n",
       "      <td>0.803659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.768424</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.800325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.758536</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.809366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.792672</td>\n",
       "      <td>0.879215</td>\n",
       "      <td>0.795357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.790547</td>\n",
       "      <td>0.883637</td>\n",
       "      <td>0.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.796086</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.806811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.795205</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>0.805984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.795783</td>\n",
       "      <td>0.882532</td>\n",
       "      <td>0.806285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85895dfbacc84e38ba8cfecea39e1531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ee90588b64479a8715cceafa9c21b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.644586</td>\n",
       "      <td>0.759812</td>\n",
       "      <td>0.540449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.592800</td>\n",
       "      <td>0.502976</td>\n",
       "      <td>0.820066</td>\n",
       "      <td>0.625149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.515620</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.701419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.339200</td>\n",
       "      <td>0.486978</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.668491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.315600</td>\n",
       "      <td>0.459927</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.728115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.531814</td>\n",
       "      <td>0.865119</td>\n",
       "      <td>0.766917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.154700</td>\n",
       "      <td>0.564025</td>\n",
       "      <td>0.857103</td>\n",
       "      <td>0.744811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.540360</td>\n",
       "      <td>0.864290</td>\n",
       "      <td>0.756202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.138100</td>\n",
       "      <td>0.587121</td>\n",
       "      <td>0.864566</td>\n",
       "      <td>0.767554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.571209</td>\n",
       "      <td>0.865395</td>\n",
       "      <td>0.767198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.098000</td>\n",
       "      <td>0.551106</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.783731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.631367</td>\n",
       "      <td>0.870370</td>\n",
       "      <td>0.787737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.053300</td>\n",
       "      <td>0.659262</td>\n",
       "      <td>0.880321</td>\n",
       "      <td>0.799177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.617206</td>\n",
       "      <td>0.872305</td>\n",
       "      <td>0.780274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.682634</td>\n",
       "      <td>0.881426</td>\n",
       "      <td>0.807595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.732262</td>\n",
       "      <td>0.870094</td>\n",
       "      <td>0.778626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.671066</td>\n",
       "      <td>0.875069</td>\n",
       "      <td>0.792072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.750981</td>\n",
       "      <td>0.875622</td>\n",
       "      <td>0.792188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.747133</td>\n",
       "      <td>0.877833</td>\n",
       "      <td>0.796644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.728955</td>\n",
       "      <td>0.883914</td>\n",
       "      <td>0.810420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.772790</td>\n",
       "      <td>0.881426</td>\n",
       "      <td>0.803659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.768424</td>\n",
       "      <td>0.880597</td>\n",
       "      <td>0.800325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.758536</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.809366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.792672</td>\n",
       "      <td>0.879215</td>\n",
       "      <td>0.795357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.790547</td>\n",
       "      <td>0.883637</td>\n",
       "      <td>0.812000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.796086</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.806811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018400</td>\n",
       "      <td>0.795205</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>0.805984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015600</td>\n",
       "      <td>0.795783</td>\n",
       "      <td>0.882532</td>\n",
       "      <td>0.806285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe419183e6e347cfb97a7bd2d7c1af66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d033e8b9516a4ee198468252d1f9eed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.629014</td>\n",
       "      <td>0.748203</td>\n",
       "      <td>0.560024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>0.546622</td>\n",
       "      <td>0.788004</td>\n",
       "      <td>0.613108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.540346</td>\n",
       "      <td>0.818132</td>\n",
       "      <td>0.643522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.499583</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.686208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.467603</td>\n",
       "      <td>0.847706</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.538493</td>\n",
       "      <td>0.852405</td>\n",
       "      <td>0.739853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.535641</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.752220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.523262</td>\n",
       "      <td>0.860144</td>\n",
       "      <td>0.756778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.586605</td>\n",
       "      <td>0.862078</td>\n",
       "      <td>0.754908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.566985</td>\n",
       "      <td>0.868712</td>\n",
       "      <td>0.774088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.592429</td>\n",
       "      <td>0.857380</td>\n",
       "      <td>0.763382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.668178</td>\n",
       "      <td>0.859038</td>\n",
       "      <td>0.759532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.753698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.750209</td>\n",
       "      <td>0.866224</td>\n",
       "      <td>0.772953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.780726</td>\n",
       "      <td>0.869265</td>\n",
       "      <td>0.779015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.802290</td>\n",
       "      <td>0.861802</td>\n",
       "      <td>0.764736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.748479</td>\n",
       "      <td>0.864013</td>\n",
       "      <td>0.772845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.827308</td>\n",
       "      <td>0.861526</td>\n",
       "      <td>0.762998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.823559</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.764552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.821602</td>\n",
       "      <td>0.866501</td>\n",
       "      <td>0.766828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.836425</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.770871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.823030</td>\n",
       "      <td>0.866501</td>\n",
       "      <td>0.772744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.839116</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.773370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.868288</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.775794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.851050</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.771668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.874726</td>\n",
       "      <td>0.862078</td>\n",
       "      <td>0.765570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.864160</td>\n",
       "      <td>0.867330</td>\n",
       "      <td>0.774467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.868140</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.773807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e24e4314c64673bda74b207a8c6766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2493e3d1a6034dbdbf9d534ed01c4d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.629014</td>\n",
       "      <td>0.748203</td>\n",
       "      <td>0.560024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>0.546622</td>\n",
       "      <td>0.788004</td>\n",
       "      <td>0.613108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.540346</td>\n",
       "      <td>0.818132</td>\n",
       "      <td>0.643522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.499583</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.686208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.467603</td>\n",
       "      <td>0.847706</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.538493</td>\n",
       "      <td>0.852405</td>\n",
       "      <td>0.739853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.535641</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.752220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.523262</td>\n",
       "      <td>0.860144</td>\n",
       "      <td>0.756778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.586605</td>\n",
       "      <td>0.862078</td>\n",
       "      <td>0.754908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.566985</td>\n",
       "      <td>0.868712</td>\n",
       "      <td>0.774088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.592429</td>\n",
       "      <td>0.857380</td>\n",
       "      <td>0.763382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.668178</td>\n",
       "      <td>0.859038</td>\n",
       "      <td>0.759532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.753698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.750209</td>\n",
       "      <td>0.866224</td>\n",
       "      <td>0.772953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.780726</td>\n",
       "      <td>0.869265</td>\n",
       "      <td>0.779015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.802290</td>\n",
       "      <td>0.861802</td>\n",
       "      <td>0.764736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.748479</td>\n",
       "      <td>0.864013</td>\n",
       "      <td>0.772845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.827308</td>\n",
       "      <td>0.861526</td>\n",
       "      <td>0.762998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.823559</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.764552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.821602</td>\n",
       "      <td>0.866501</td>\n",
       "      <td>0.766828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.836425</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.770871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.823030</td>\n",
       "      <td>0.866501</td>\n",
       "      <td>0.772744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.839116</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.773370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.868288</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.775794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.851050</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.771668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.874726</td>\n",
       "      <td>0.862078</td>\n",
       "      <td>0.765570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.864160</td>\n",
       "      <td>0.867330</td>\n",
       "      <td>0.774467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.868140</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.773807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e16e8917b24618a7b5bf0350e109b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1983b022f3d34b5bbd61bcca288ab7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.629014</td>\n",
       "      <td>0.748203</td>\n",
       "      <td>0.560024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.605400</td>\n",
       "      <td>0.546622</td>\n",
       "      <td>0.788004</td>\n",
       "      <td>0.613108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.515700</td>\n",
       "      <td>0.540346</td>\n",
       "      <td>0.818132</td>\n",
       "      <td>0.643522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.499583</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.686208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.467603</td>\n",
       "      <td>0.847706</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.253800</td>\n",
       "      <td>0.538493</td>\n",
       "      <td>0.852405</td>\n",
       "      <td>0.739853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.535641</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.752220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.170400</td>\n",
       "      <td>0.523262</td>\n",
       "      <td>0.860144</td>\n",
       "      <td>0.756778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.586605</td>\n",
       "      <td>0.862078</td>\n",
       "      <td>0.754908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.103800</td>\n",
       "      <td>0.566985</td>\n",
       "      <td>0.868712</td>\n",
       "      <td>0.774088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.100900</td>\n",
       "      <td>0.592429</td>\n",
       "      <td>0.857380</td>\n",
       "      <td>0.763382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.067300</td>\n",
       "      <td>0.668178</td>\n",
       "      <td>0.859038</td>\n",
       "      <td>0.759532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.082300</td>\n",
       "      <td>0.672654</td>\n",
       "      <td>0.857656</td>\n",
       "      <td>0.753698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.750209</td>\n",
       "      <td>0.866224</td>\n",
       "      <td>0.772953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.780726</td>\n",
       "      <td>0.869265</td>\n",
       "      <td>0.779015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.802290</td>\n",
       "      <td>0.861802</td>\n",
       "      <td>0.764736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.748479</td>\n",
       "      <td>0.864013</td>\n",
       "      <td>0.772845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.031900</td>\n",
       "      <td>0.827308</td>\n",
       "      <td>0.861526</td>\n",
       "      <td>0.762998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.823559</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.764552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.821602</td>\n",
       "      <td>0.866501</td>\n",
       "      <td>0.766828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.836425</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.770871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>0.823030</td>\n",
       "      <td>0.866501</td>\n",
       "      <td>0.772744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.839116</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.773370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.868288</td>\n",
       "      <td>0.868159</td>\n",
       "      <td>0.775794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.851050</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.771668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.874726</td>\n",
       "      <td>0.862078</td>\n",
       "      <td>0.765570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.021800</td>\n",
       "      <td>0.864160</td>\n",
       "      <td>0.867330</td>\n",
       "      <td>0.774467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.868140</td>\n",
       "      <td>0.867054</td>\n",
       "      <td>0.773807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f263d8a351496a8f3135874a9ce0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883fcee809c24392961cdd3fbfe90661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.752300</td>\n",
       "      <td>0.617449</td>\n",
       "      <td>0.766722</td>\n",
       "      <td>0.545938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>0.508300</td>\n",
       "      <td>0.810116</td>\n",
       "      <td>0.611556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.497192</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.676153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.340200</td>\n",
       "      <td>0.458867</td>\n",
       "      <td>0.840520</td>\n",
       "      <td>0.730011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.443054</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.755401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.586779</td>\n",
       "      <td>0.840243</td>\n",
       "      <td>0.742603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.164900</td>\n",
       "      <td>0.547708</td>\n",
       "      <td>0.857933</td>\n",
       "      <td>0.774150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.560919</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.777570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.634350</td>\n",
       "      <td>0.863184</td>\n",
       "      <td>0.786736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.596851</td>\n",
       "      <td>0.874240</td>\n",
       "      <td>0.795472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.603894</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.797104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.586406</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>0.803479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.680973</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.803217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.644812</td>\n",
       "      <td>0.864566</td>\n",
       "      <td>0.789943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.646245</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.816701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.707959</td>\n",
       "      <td>0.878662</td>\n",
       "      <td>0.811517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.695191</td>\n",
       "      <td>0.878386</td>\n",
       "      <td>0.816018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.703872</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.811956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.752051</td>\n",
       "      <td>0.877280</td>\n",
       "      <td>0.813156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.749524</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>0.806146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.722176</td>\n",
       "      <td>0.878662</td>\n",
       "      <td>0.810158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.752044</td>\n",
       "      <td>0.876175</td>\n",
       "      <td>0.813030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.808391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.780891</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.809238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.772951</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.813822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.789573</td>\n",
       "      <td>0.880321</td>\n",
       "      <td>0.812137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.785298</td>\n",
       "      <td>0.881426</td>\n",
       "      <td>0.814439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.783478</td>\n",
       "      <td>0.881979</td>\n",
       "      <td>0.812949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a7027fe7c644518ec0a9dd1104cdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb18ec5a0c0e45a082132e741ae23178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:43, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.752300</td>\n",
       "      <td>0.617449</td>\n",
       "      <td>0.766722</td>\n",
       "      <td>0.545938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>0.508300</td>\n",
       "      <td>0.810116</td>\n",
       "      <td>0.611556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.497192</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.676153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.340200</td>\n",
       "      <td>0.458867</td>\n",
       "      <td>0.840520</td>\n",
       "      <td>0.730011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.443054</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.755401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.586779</td>\n",
       "      <td>0.840243</td>\n",
       "      <td>0.742603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.164900</td>\n",
       "      <td>0.547708</td>\n",
       "      <td>0.857933</td>\n",
       "      <td>0.774150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.560919</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.777570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.634350</td>\n",
       "      <td>0.863184</td>\n",
       "      <td>0.786736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.596851</td>\n",
       "      <td>0.874240</td>\n",
       "      <td>0.795472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.603894</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.797104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.586406</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>0.803479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.680973</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.803217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.644812</td>\n",
       "      <td>0.864566</td>\n",
       "      <td>0.789943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.646245</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.816701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.707959</td>\n",
       "      <td>0.878662</td>\n",
       "      <td>0.811517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.695191</td>\n",
       "      <td>0.878386</td>\n",
       "      <td>0.816018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.703872</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.811956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.752051</td>\n",
       "      <td>0.877280</td>\n",
       "      <td>0.813156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.749524</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>0.806146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.722176</td>\n",
       "      <td>0.878662</td>\n",
       "      <td>0.810158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.752044</td>\n",
       "      <td>0.876175</td>\n",
       "      <td>0.813030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.808391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.780891</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.809238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.772951</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.813822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.789573</td>\n",
       "      <td>0.880321</td>\n",
       "      <td>0.812137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.785298</td>\n",
       "      <td>0.881426</td>\n",
       "      <td>0.814439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.783478</td>\n",
       "      <td>0.881979</td>\n",
       "      <td>0.812949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SI2M-Lab/DarijaBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ffbdae81724065b9351e4fa5682588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b207a47e79349cb97f1a511639776a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.752300</td>\n",
       "      <td>0.617449</td>\n",
       "      <td>0.766722</td>\n",
       "      <td>0.545938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.609800</td>\n",
       "      <td>0.508300</td>\n",
       "      <td>0.810116</td>\n",
       "      <td>0.611556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.502200</td>\n",
       "      <td>0.497192</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.676153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.340200</td>\n",
       "      <td>0.458867</td>\n",
       "      <td>0.840520</td>\n",
       "      <td>0.730011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.443054</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.755401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.586779</td>\n",
       "      <td>0.840243</td>\n",
       "      <td>0.742603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.164900</td>\n",
       "      <td>0.547708</td>\n",
       "      <td>0.857933</td>\n",
       "      <td>0.774150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.560919</td>\n",
       "      <td>0.863460</td>\n",
       "      <td>0.777570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.634350</td>\n",
       "      <td>0.863184</td>\n",
       "      <td>0.786736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.596851</td>\n",
       "      <td>0.874240</td>\n",
       "      <td>0.795472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.603894</td>\n",
       "      <td>0.871476</td>\n",
       "      <td>0.797104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.586406</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>0.803479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.680973</td>\n",
       "      <td>0.871200</td>\n",
       "      <td>0.803217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.644812</td>\n",
       "      <td>0.864566</td>\n",
       "      <td>0.789943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.646245</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.816701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.707959</td>\n",
       "      <td>0.878662</td>\n",
       "      <td>0.811517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.695191</td>\n",
       "      <td>0.878386</td>\n",
       "      <td>0.816018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.703872</td>\n",
       "      <td>0.876727</td>\n",
       "      <td>0.811956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.752051</td>\n",
       "      <td>0.877280</td>\n",
       "      <td>0.813156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.749524</td>\n",
       "      <td>0.872858</td>\n",
       "      <td>0.806146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.722176</td>\n",
       "      <td>0.878662</td>\n",
       "      <td>0.810158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.752044</td>\n",
       "      <td>0.876175</td>\n",
       "      <td>0.813030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.766637</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.808391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>0.780891</td>\n",
       "      <td>0.879491</td>\n",
       "      <td>0.809238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.023600</td>\n",
       "      <td>0.772951</td>\n",
       "      <td>0.880873</td>\n",
       "      <td>0.813822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.789573</td>\n",
       "      <td>0.880321</td>\n",
       "      <td>0.812137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.785298</td>\n",
       "      <td>0.881426</td>\n",
       "      <td>0.814439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.783478</td>\n",
       "      <td>0.881979</td>\n",
       "      <td>0.812949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 14469\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 3618\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5be63cb9cb42e181d0299ad8a6dd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a791ba5cc249abb58a18ba706f6b3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 02:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.819800</td>\n",
       "      <td>0.746652</td>\n",
       "      <td>0.699834</td>\n",
       "      <td>0.489843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.573233</td>\n",
       "      <td>0.782200</td>\n",
       "      <td>0.562663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.555200</td>\n",
       "      <td>0.575076</td>\n",
       "      <td>0.802377</td>\n",
       "      <td>0.613517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.563116</td>\n",
       "      <td>0.813709</td>\n",
       "      <td>0.670368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.509953</td>\n",
       "      <td>0.821725</td>\n",
       "      <td>0.695722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.674763</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.699631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.590788</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.724480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.596050</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.735458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.699458</td>\n",
       "      <td>0.841625</td>\n",
       "      <td>0.735070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.708202</td>\n",
       "      <td>0.840243</td>\n",
       "      <td>0.746524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.757528</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.735468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.798554</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>0.747210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.801877</td>\n",
       "      <td>0.849917</td>\n",
       "      <td>0.759717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.842454</td>\n",
       "      <td>0.747774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.844214</td>\n",
       "      <td>0.844942</td>\n",
       "      <td>0.750551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.894040</td>\n",
       "      <td>0.844389</td>\n",
       "      <td>0.747096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.926676</td>\n",
       "      <td>0.844666</td>\n",
       "      <td>0.755213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.922512</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.755726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.966355</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.761542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.937843</td>\n",
       "      <td>0.846324</td>\n",
       "      <td>0.749444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.985468</td>\n",
       "      <td>0.847706</td>\n",
       "      <td>0.756421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.992288</td>\n",
       "      <td>0.843836</td>\n",
       "      <td>0.752495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.968195</td>\n",
       "      <td>0.847982</td>\n",
       "      <td>0.762203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>1.005859</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.762580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>0.847706</td>\n",
       "      <td>0.759356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>1.004106</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.761295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>1.007853</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.757789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>1.010027</td>\n",
       "      <td>0.849088</td>\n",
       "      <td>0.758631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab35bf97a4d4452abd2456e32e525c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4808bb307a4e2e9fa907dbb8bad88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 02:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.819800</td>\n",
       "      <td>0.746652</td>\n",
       "      <td>0.699834</td>\n",
       "      <td>0.489843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.573233</td>\n",
       "      <td>0.782200</td>\n",
       "      <td>0.562663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.555200</td>\n",
       "      <td>0.575076</td>\n",
       "      <td>0.802377</td>\n",
       "      <td>0.613517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.563116</td>\n",
       "      <td>0.813709</td>\n",
       "      <td>0.670368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.509953</td>\n",
       "      <td>0.821725</td>\n",
       "      <td>0.695722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.674763</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.699631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.590788</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.724480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.596050</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.735458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.699458</td>\n",
       "      <td>0.841625</td>\n",
       "      <td>0.735070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.708202</td>\n",
       "      <td>0.840243</td>\n",
       "      <td>0.746524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.757528</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.735468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.798554</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>0.747210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.801877</td>\n",
       "      <td>0.849917</td>\n",
       "      <td>0.759717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.842454</td>\n",
       "      <td>0.747774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.844214</td>\n",
       "      <td>0.844942</td>\n",
       "      <td>0.750551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.894040</td>\n",
       "      <td>0.844389</td>\n",
       "      <td>0.747096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.926676</td>\n",
       "      <td>0.844666</td>\n",
       "      <td>0.755213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.922512</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.755726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.966355</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.761542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.937843</td>\n",
       "      <td>0.846324</td>\n",
       "      <td>0.749444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.985468</td>\n",
       "      <td>0.847706</td>\n",
       "      <td>0.756421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.992288</td>\n",
       "      <td>0.843836</td>\n",
       "      <td>0.752495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.968195</td>\n",
       "      <td>0.847982</td>\n",
       "      <td>0.762203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>1.005859</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.762580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>0.847706</td>\n",
       "      <td>0.759356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>1.004106</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.761295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>1.007853</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.757789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>1.010027</td>\n",
       "      <td>0.849088</td>\n",
       "      <td>0.758631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f498f08b5f4d039d77f56ae25733d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bd387ae76a418ebe5d45c144f08f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 02:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.819800</td>\n",
       "      <td>0.746652</td>\n",
       "      <td>0.699834</td>\n",
       "      <td>0.489843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.573233</td>\n",
       "      <td>0.782200</td>\n",
       "      <td>0.562663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.555200</td>\n",
       "      <td>0.575076</td>\n",
       "      <td>0.802377</td>\n",
       "      <td>0.613517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.373800</td>\n",
       "      <td>0.563116</td>\n",
       "      <td>0.813709</td>\n",
       "      <td>0.670368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.350500</td>\n",
       "      <td>0.509953</td>\n",
       "      <td>0.821725</td>\n",
       "      <td>0.695722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.674763</td>\n",
       "      <td>0.820896</td>\n",
       "      <td>0.699631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.163300</td>\n",
       "      <td>0.590788</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.724480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.596050</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.735458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.126300</td>\n",
       "      <td>0.699458</td>\n",
       "      <td>0.841625</td>\n",
       "      <td>0.735070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.091900</td>\n",
       "      <td>0.708202</td>\n",
       "      <td>0.840243</td>\n",
       "      <td>0.746524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.086200</td>\n",
       "      <td>0.757528</td>\n",
       "      <td>0.836374</td>\n",
       "      <td>0.735468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.063500</td>\n",
       "      <td>0.798554</td>\n",
       "      <td>0.839690</td>\n",
       "      <td>0.747210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.064400</td>\n",
       "      <td>0.801877</td>\n",
       "      <td>0.849917</td>\n",
       "      <td>0.759717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.842454</td>\n",
       "      <td>0.747774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.844214</td>\n",
       "      <td>0.844942</td>\n",
       "      <td>0.750551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.046700</td>\n",
       "      <td>0.894040</td>\n",
       "      <td>0.844389</td>\n",
       "      <td>0.747096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.038800</td>\n",
       "      <td>0.926676</td>\n",
       "      <td>0.844666</td>\n",
       "      <td>0.755213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.922512</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.755726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.966355</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.761542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.027500</td>\n",
       "      <td>0.937843</td>\n",
       "      <td>0.846324</td>\n",
       "      <td>0.749444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.985468</td>\n",
       "      <td>0.847706</td>\n",
       "      <td>0.756421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.992288</td>\n",
       "      <td>0.843836</td>\n",
       "      <td>0.752495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.968195</td>\n",
       "      <td>0.847982</td>\n",
       "      <td>0.762203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>1.005859</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.762580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.999236</td>\n",
       "      <td>0.847706</td>\n",
       "      <td>0.759356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>1.004106</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.761295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.014800</td>\n",
       "      <td>1.007853</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.757789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>1.010027</td>\n",
       "      <td>0.849088</td>\n",
       "      <td>0.758631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5a2762238843c2825f6d4ae5453944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c95aa19c4146e38cfbf6452a448c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 02:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.836200</td>\n",
       "      <td>0.657731</td>\n",
       "      <td>0.743228</td>\n",
       "      <td>0.528768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.647000</td>\n",
       "      <td>0.582813</td>\n",
       "      <td>0.762576</td>\n",
       "      <td>0.558009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>0.575741</td>\n",
       "      <td>0.801548</td>\n",
       "      <td>0.634628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.537475</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>0.683191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.355900</td>\n",
       "      <td>0.531958</td>\n",
       "      <td>0.825594</td>\n",
       "      <td>0.696716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.612635</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.730897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.603896</td>\n",
       "      <td>0.834163</td>\n",
       "      <td>0.726331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.638092</td>\n",
       "      <td>0.840243</td>\n",
       "      <td>0.723232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.693474</td>\n",
       "      <td>0.846324</td>\n",
       "      <td>0.731046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.723149</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.684198</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.754419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.771868</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.753876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.782405</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.748773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.749798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.866529</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.754116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.867112</td>\n",
       "      <td>0.845495</td>\n",
       "      <td>0.747615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.853249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.751424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.863311</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.763608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.895053</td>\n",
       "      <td>0.851575</td>\n",
       "      <td>0.754598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.911136</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.750491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.958383</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.748237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.928609</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.751202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.930312</td>\n",
       "      <td>0.850470</td>\n",
       "      <td>0.752720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.956002</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.765586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.954401</td>\n",
       "      <td>0.851299</td>\n",
       "      <td>0.752470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.950326</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.753967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.949906</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.757536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.956876</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.758939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f6ba3ad634405fb52b83a86ae030d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4049f045b41e41dca33b7b03e83209c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 02:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.836200</td>\n",
       "      <td>0.657731</td>\n",
       "      <td>0.743228</td>\n",
       "      <td>0.528768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.647000</td>\n",
       "      <td>0.582813</td>\n",
       "      <td>0.762576</td>\n",
       "      <td>0.558009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>0.575741</td>\n",
       "      <td>0.801548</td>\n",
       "      <td>0.634628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.537475</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>0.683191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.355900</td>\n",
       "      <td>0.531958</td>\n",
       "      <td>0.825594</td>\n",
       "      <td>0.696716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.612635</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.730897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.603896</td>\n",
       "      <td>0.834163</td>\n",
       "      <td>0.726331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.638092</td>\n",
       "      <td>0.840243</td>\n",
       "      <td>0.723232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.693474</td>\n",
       "      <td>0.846324</td>\n",
       "      <td>0.731046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.723149</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.684198</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.754419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.771868</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.753876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.782405</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.748773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.749798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.866529</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.754116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.867112</td>\n",
       "      <td>0.845495</td>\n",
       "      <td>0.747615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.853249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.751424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.863311</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.763608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.895053</td>\n",
       "      <td>0.851575</td>\n",
       "      <td>0.754598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.911136</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.750491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.958383</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.748237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.928609</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.751202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.930312</td>\n",
       "      <td>0.850470</td>\n",
       "      <td>0.752720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.956002</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.765586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.954401</td>\n",
       "      <td>0.851299</td>\n",
       "      <td>0.752470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.950326</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.753967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.949906</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.757536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.956876</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.758939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9974e68f92450982a7b4908dd61532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b264f4ee59d14473a8f9896be4f5ed0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 02:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.836200</td>\n",
       "      <td>0.657731</td>\n",
       "      <td>0.743228</td>\n",
       "      <td>0.528768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.647000</td>\n",
       "      <td>0.582813</td>\n",
       "      <td>0.762576</td>\n",
       "      <td>0.558009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.555900</td>\n",
       "      <td>0.575741</td>\n",
       "      <td>0.801548</td>\n",
       "      <td>0.634628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.537475</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>0.683191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.355900</td>\n",
       "      <td>0.531958</td>\n",
       "      <td>0.825594</td>\n",
       "      <td>0.696716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.257000</td>\n",
       "      <td>0.612635</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.730897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.170200</td>\n",
       "      <td>0.603896</td>\n",
       "      <td>0.834163</td>\n",
       "      <td>0.726331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.638092</td>\n",
       "      <td>0.840243</td>\n",
       "      <td>0.723232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.131100</td>\n",
       "      <td>0.693474</td>\n",
       "      <td>0.846324</td>\n",
       "      <td>0.731046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.723149</td>\n",
       "      <td>0.845771</td>\n",
       "      <td>0.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.102200</td>\n",
       "      <td>0.684198</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.754419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.057400</td>\n",
       "      <td>0.771868</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.753876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.782405</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.748773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.804952</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.749798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.866529</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.754116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.867112</td>\n",
       "      <td>0.845495</td>\n",
       "      <td>0.747615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.853249</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.751424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.863311</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.763608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.895053</td>\n",
       "      <td>0.851575</td>\n",
       "      <td>0.754598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.911136</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.750491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.022100</td>\n",
       "      <td>0.958383</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.748237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.928609</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.751202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.930312</td>\n",
       "      <td>0.850470</td>\n",
       "      <td>0.752720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.956002</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.765586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.954401</td>\n",
       "      <td>0.851299</td>\n",
       "      <td>0.752470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.950326</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.753967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.018500</td>\n",
       "      <td>0.949906</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.757536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.956876</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.758939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfc1e2d93fd48169e87620eaa965609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f54cee0b7ca4050b09732a44f19f843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 02:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>0.669563</td>\n",
       "      <td>0.743505</td>\n",
       "      <td>0.525139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.674900</td>\n",
       "      <td>0.573422</td>\n",
       "      <td>0.778331</td>\n",
       "      <td>0.560691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.550400</td>\n",
       "      <td>0.587434</td>\n",
       "      <td>0.803206</td>\n",
       "      <td>0.622295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.544628</td>\n",
       "      <td>0.810392</td>\n",
       "      <td>0.657408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.508276</td>\n",
       "      <td>0.821448</td>\n",
       "      <td>0.694255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>0.568622</td>\n",
       "      <td>0.827253</td>\n",
       "      <td>0.725262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.569177</td>\n",
       "      <td>0.839138</td>\n",
       "      <td>0.740612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.565436</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.735704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.671431</td>\n",
       "      <td>0.842731</td>\n",
       "      <td>0.756899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.673081</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.759423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.690701</td>\n",
       "      <td>0.839138</td>\n",
       "      <td>0.745730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.717844</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.769739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.774020</td>\n",
       "      <td>0.853787</td>\n",
       "      <td>0.772962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.776656</td>\n",
       "      <td>0.849364</td>\n",
       "      <td>0.768752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.766150</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.779841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.796022</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.776543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.828011</td>\n",
       "      <td>0.847153</td>\n",
       "      <td>0.767048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.873002</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.772052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.887512</td>\n",
       "      <td>0.852405</td>\n",
       "      <td>0.775433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.904333</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.785324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.907176</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.773872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.888456</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.781074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.907704</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.773818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.913379</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.773363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.929473</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.775876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.936981</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.772645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.924415</td>\n",
       "      <td>0.855445</td>\n",
       "      <td>0.782254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.924245</td>\n",
       "      <td>0.855998</td>\n",
       "      <td>0.778156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27301cbcbec94f02a1491da562f3a9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8e9b86daf1423fa3e8e9136893f5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 02:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>0.669563</td>\n",
       "      <td>0.743505</td>\n",
       "      <td>0.525139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.674900</td>\n",
       "      <td>0.573422</td>\n",
       "      <td>0.778331</td>\n",
       "      <td>0.560691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.550400</td>\n",
       "      <td>0.587434</td>\n",
       "      <td>0.803206</td>\n",
       "      <td>0.622295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.544628</td>\n",
       "      <td>0.810392</td>\n",
       "      <td>0.657408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.508276</td>\n",
       "      <td>0.821448</td>\n",
       "      <td>0.694255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>0.568622</td>\n",
       "      <td>0.827253</td>\n",
       "      <td>0.725262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.569177</td>\n",
       "      <td>0.839138</td>\n",
       "      <td>0.740612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.565436</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.735704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.671431</td>\n",
       "      <td>0.842731</td>\n",
       "      <td>0.756899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.673081</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.759423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.690701</td>\n",
       "      <td>0.839138</td>\n",
       "      <td>0.745730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.717844</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.769739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.774020</td>\n",
       "      <td>0.853787</td>\n",
       "      <td>0.772962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.776656</td>\n",
       "      <td>0.849364</td>\n",
       "      <td>0.768752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.766150</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.779841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.796022</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.776543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.828011</td>\n",
       "      <td>0.847153</td>\n",
       "      <td>0.767048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.873002</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.772052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.887512</td>\n",
       "      <td>0.852405</td>\n",
       "      <td>0.775433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.904333</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.785324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.907176</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.773872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.888456</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.781074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.907704</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.773818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.913379</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.773363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.929473</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.775876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.936981</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.772645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.924415</td>\n",
       "      <td>0.855445</td>\n",
       "      <td>0.782254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.924245</td>\n",
       "      <td>0.855998</td>\n",
       "      <td>0.778156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorRoBERTa, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at otmangi/MorRoBERTa and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8344a6e3a224a0c98ba87592ad41117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbef53fd48c401f9ff191ef7d4d9628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 02:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.796800</td>\n",
       "      <td>0.669563</td>\n",
       "      <td>0.743505</td>\n",
       "      <td>0.525139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.674900</td>\n",
       "      <td>0.573422</td>\n",
       "      <td>0.778331</td>\n",
       "      <td>0.560691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.550400</td>\n",
       "      <td>0.587434</td>\n",
       "      <td>0.803206</td>\n",
       "      <td>0.622295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.544628</td>\n",
       "      <td>0.810392</td>\n",
       "      <td>0.657408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.371300</td>\n",
       "      <td>0.508276</td>\n",
       "      <td>0.821448</td>\n",
       "      <td>0.694255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.283600</td>\n",
       "      <td>0.568622</td>\n",
       "      <td>0.827253</td>\n",
       "      <td>0.725262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.569177</td>\n",
       "      <td>0.839138</td>\n",
       "      <td>0.740612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.565436</td>\n",
       "      <td>0.834992</td>\n",
       "      <td>0.735704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.671431</td>\n",
       "      <td>0.842731</td>\n",
       "      <td>0.756899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.673081</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.759423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.101900</td>\n",
       "      <td>0.690701</td>\n",
       "      <td>0.839138</td>\n",
       "      <td>0.745730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.717844</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.769739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.056300</td>\n",
       "      <td>0.774020</td>\n",
       "      <td>0.853787</td>\n",
       "      <td>0.772962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.776656</td>\n",
       "      <td>0.849364</td>\n",
       "      <td>0.768752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.766150</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.779841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.796022</td>\n",
       "      <td>0.854063</td>\n",
       "      <td>0.776543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.828011</td>\n",
       "      <td>0.847153</td>\n",
       "      <td>0.767048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.873002</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.772052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.028600</td>\n",
       "      <td>0.887512</td>\n",
       "      <td>0.852405</td>\n",
       "      <td>0.775433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.904333</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.785324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.023700</td>\n",
       "      <td>0.907176</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.773872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.888456</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.781074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.907704</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.773818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.913379</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.773363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.929473</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.775876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.936981</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.772645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.924415</td>\n",
       "      <td>0.855445</td>\n",
       "      <td>0.782254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.924245</td>\n",
       "      <td>0.855998</td>\n",
       "      <td>0.778156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 14469\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tweets', 'label'],\n",
       "        num_rows: 3618\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054e2ad11fcb4c0ab5dc2f0db34821ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c61c9a6d8c74bd29b86f3bfbebb201a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.744800</td>\n",
       "      <td>0.650289</td>\n",
       "      <td>0.754008</td>\n",
       "      <td>0.539908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.551362</td>\n",
       "      <td>0.786622</td>\n",
       "      <td>0.591084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.527800</td>\n",
       "      <td>0.545632</td>\n",
       "      <td>0.809287</td>\n",
       "      <td>0.653217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.551414</td>\n",
       "      <td>0.808458</td>\n",
       "      <td>0.662893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.504788</td>\n",
       "      <td>0.833610</td>\n",
       "      <td>0.713393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.656534</td>\n",
       "      <td>0.826700</td>\n",
       "      <td>0.709231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.681012</td>\n",
       "      <td>0.828911</td>\n",
       "      <td>0.713838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.611834</td>\n",
       "      <td>0.847153</td>\n",
       "      <td>0.746882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.674931</td>\n",
       "      <td>0.847430</td>\n",
       "      <td>0.754907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.708920</td>\n",
       "      <td>0.849917</td>\n",
       "      <td>0.757129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.706635</td>\n",
       "      <td>0.841902</td>\n",
       "      <td>0.741917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.782703</td>\n",
       "      <td>0.852405</td>\n",
       "      <td>0.761701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.756120</td>\n",
       "      <td>0.847430</td>\n",
       "      <td>0.754920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.782596</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.748565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.829727</td>\n",
       "      <td>0.855998</td>\n",
       "      <td>0.772678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.765903</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.761989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.786897</td>\n",
       "      <td>0.854892</td>\n",
       "      <td>0.765371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.858279</td>\n",
       "      <td>0.851299</td>\n",
       "      <td>0.760726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.880430</td>\n",
       "      <td>0.853234</td>\n",
       "      <td>0.764144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.882884</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.756801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.927675</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.769150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.902461</td>\n",
       "      <td>0.849641</td>\n",
       "      <td>0.752077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.895020</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.772828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.912524</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.771248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.925470</td>\n",
       "      <td>0.855169</td>\n",
       "      <td>0.763973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.940868</td>\n",
       "      <td>0.855169</td>\n",
       "      <td>0.770513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.928092</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.768659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.931988</td>\n",
       "      <td>0.855445</td>\n",
       "      <td>0.763561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6ae7d230644054b01e857493f20f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e4308a8b9448cc9805c2746f2a0363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.744800</td>\n",
       "      <td>0.650289</td>\n",
       "      <td>0.754008</td>\n",
       "      <td>0.539908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.551362</td>\n",
       "      <td>0.786622</td>\n",
       "      <td>0.591084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.527800</td>\n",
       "      <td>0.545632</td>\n",
       "      <td>0.809287</td>\n",
       "      <td>0.653217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.551414</td>\n",
       "      <td>0.808458</td>\n",
       "      <td>0.662893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.504788</td>\n",
       "      <td>0.833610</td>\n",
       "      <td>0.713393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.656534</td>\n",
       "      <td>0.826700</td>\n",
       "      <td>0.709231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.681012</td>\n",
       "      <td>0.828911</td>\n",
       "      <td>0.713838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.611834</td>\n",
       "      <td>0.847153</td>\n",
       "      <td>0.746882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.674931</td>\n",
       "      <td>0.847430</td>\n",
       "      <td>0.754907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.708920</td>\n",
       "      <td>0.849917</td>\n",
       "      <td>0.757129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.706635</td>\n",
       "      <td>0.841902</td>\n",
       "      <td>0.741917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.782703</td>\n",
       "      <td>0.852405</td>\n",
       "      <td>0.761701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.756120</td>\n",
       "      <td>0.847430</td>\n",
       "      <td>0.754920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.782596</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.748565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.829727</td>\n",
       "      <td>0.855998</td>\n",
       "      <td>0.772678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.765903</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.761989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.786897</td>\n",
       "      <td>0.854892</td>\n",
       "      <td>0.765371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.858279</td>\n",
       "      <td>0.851299</td>\n",
       "      <td>0.760726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.880430</td>\n",
       "      <td>0.853234</td>\n",
       "      <td>0.764144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.882884</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.756801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.927675</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.769150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.902461</td>\n",
       "      <td>0.849641</td>\n",
       "      <td>0.752077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.895020</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.772828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.912524</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.771248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.925470</td>\n",
       "      <td>0.855169</td>\n",
       "      <td>0.763973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.940868</td>\n",
       "      <td>0.855169</td>\n",
       "      <td>0.770513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.928092</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.768659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.931988</td>\n",
       "      <td>0.855445</td>\n",
       "      <td>0.763561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d25400938a45ae8cda586f282f87d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3ef546887341cca7a1e8c6678aa258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.744800</td>\n",
       "      <td>0.650289</td>\n",
       "      <td>0.754008</td>\n",
       "      <td>0.539908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.611700</td>\n",
       "      <td>0.551362</td>\n",
       "      <td>0.786622</td>\n",
       "      <td>0.591084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.527800</td>\n",
       "      <td>0.545632</td>\n",
       "      <td>0.809287</td>\n",
       "      <td>0.653217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.551414</td>\n",
       "      <td>0.808458</td>\n",
       "      <td>0.662893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.504788</td>\n",
       "      <td>0.833610</td>\n",
       "      <td>0.713393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.656534</td>\n",
       "      <td>0.826700</td>\n",
       "      <td>0.709231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.146500</td>\n",
       "      <td>0.681012</td>\n",
       "      <td>0.828911</td>\n",
       "      <td>0.713838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.150500</td>\n",
       "      <td>0.611834</td>\n",
       "      <td>0.847153</td>\n",
       "      <td>0.746882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.674931</td>\n",
       "      <td>0.847430</td>\n",
       "      <td>0.754907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.708920</td>\n",
       "      <td>0.849917</td>\n",
       "      <td>0.757129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.706635</td>\n",
       "      <td>0.841902</td>\n",
       "      <td>0.741917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.064100</td>\n",
       "      <td>0.782703</td>\n",
       "      <td>0.852405</td>\n",
       "      <td>0.761701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.064700</td>\n",
       "      <td>0.756120</td>\n",
       "      <td>0.847430</td>\n",
       "      <td>0.754920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.782596</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.748565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.038400</td>\n",
       "      <td>0.829727</td>\n",
       "      <td>0.855998</td>\n",
       "      <td>0.772678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.055600</td>\n",
       "      <td>0.765903</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.761989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.786897</td>\n",
       "      <td>0.854892</td>\n",
       "      <td>0.765371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.858279</td>\n",
       "      <td>0.851299</td>\n",
       "      <td>0.760726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.880430</td>\n",
       "      <td>0.853234</td>\n",
       "      <td>0.764144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.034300</td>\n",
       "      <td>0.882884</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.756801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.927675</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.769150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.902461</td>\n",
       "      <td>0.849641</td>\n",
       "      <td>0.752077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.895020</td>\n",
       "      <td>0.856274</td>\n",
       "      <td>0.772828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.912524</td>\n",
       "      <td>0.858485</td>\n",
       "      <td>0.771248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.925470</td>\n",
       "      <td>0.855169</td>\n",
       "      <td>0.763973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.940868</td>\n",
       "      <td>0.855169</td>\n",
       "      <td>0.770513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.024500</td>\n",
       "      <td>0.928092</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.768659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.931988</td>\n",
       "      <td>0.855445</td>\n",
       "      <td>0.763561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61380789b695440a8a8d73b3a79d5efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022286a20e1e4eabae76ebbf8a98abd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.635242</td>\n",
       "      <td>0.757048</td>\n",
       "      <td>0.543654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>0.571682</td>\n",
       "      <td>0.788281</td>\n",
       "      <td>0.643635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.550547</td>\n",
       "      <td>0.811222</td>\n",
       "      <td>0.663756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.560315</td>\n",
       "      <td>0.818684</td>\n",
       "      <td>0.699381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.319300</td>\n",
       "      <td>0.520224</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.707680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.630333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.729101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.597412</td>\n",
       "      <td>0.836926</td>\n",
       "      <td>0.727135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.600733</td>\n",
       "      <td>0.843836</td>\n",
       "      <td>0.743420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.734350</td>\n",
       "      <td>0.835268</td>\n",
       "      <td>0.724047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.689851</td>\n",
       "      <td>0.841072</td>\n",
       "      <td>0.741800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.767695</td>\n",
       "      <td>0.841625</td>\n",
       "      <td>0.744107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.744754</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.757648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.784815</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.749291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.865244</td>\n",
       "      <td>0.836926</td>\n",
       "      <td>0.736244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.832670</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.766593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.863118</td>\n",
       "      <td>0.846048</td>\n",
       "      <td>0.749129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.804534</td>\n",
       "      <td>0.846048</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.881196</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.756387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.884064</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.767141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.902658</td>\n",
       "      <td>0.845495</td>\n",
       "      <td>0.754105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.944625</td>\n",
       "      <td>0.844389</td>\n",
       "      <td>0.746309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.931960</td>\n",
       "      <td>0.851575</td>\n",
       "      <td>0.762686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.918587</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.763223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.924388</td>\n",
       "      <td>0.849088</td>\n",
       "      <td>0.764520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.940603</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.755270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.946995</td>\n",
       "      <td>0.850470</td>\n",
       "      <td>0.759917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.948630</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.761993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.949060</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.761919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5217602074a4cf2a6927461ff5c2366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f182f8406c4256bdb87f4efff67e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.635242</td>\n",
       "      <td>0.757048</td>\n",
       "      <td>0.543654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>0.571682</td>\n",
       "      <td>0.788281</td>\n",
       "      <td>0.643635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.550547</td>\n",
       "      <td>0.811222</td>\n",
       "      <td>0.663756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.560315</td>\n",
       "      <td>0.818684</td>\n",
       "      <td>0.699381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.319300</td>\n",
       "      <td>0.520224</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.707680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.630333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.729101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.597412</td>\n",
       "      <td>0.836926</td>\n",
       "      <td>0.727135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.600733</td>\n",
       "      <td>0.843836</td>\n",
       "      <td>0.743420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.734350</td>\n",
       "      <td>0.835268</td>\n",
       "      <td>0.724047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.689851</td>\n",
       "      <td>0.841072</td>\n",
       "      <td>0.741800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.767695</td>\n",
       "      <td>0.841625</td>\n",
       "      <td>0.744107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.744754</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.757648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.784815</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.749291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.865244</td>\n",
       "      <td>0.836926</td>\n",
       "      <td>0.736244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.832670</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.766593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.863118</td>\n",
       "      <td>0.846048</td>\n",
       "      <td>0.749129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.804534</td>\n",
       "      <td>0.846048</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.881196</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.756387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.884064</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.767141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.902658</td>\n",
       "      <td>0.845495</td>\n",
       "      <td>0.754105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.944625</td>\n",
       "      <td>0.844389</td>\n",
       "      <td>0.746309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.931960</td>\n",
       "      <td>0.851575</td>\n",
       "      <td>0.762686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.918587</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.763223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.924388</td>\n",
       "      <td>0.849088</td>\n",
       "      <td>0.764520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.940603</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.755270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.946995</td>\n",
       "      <td>0.850470</td>\n",
       "      <td>0.759917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.948630</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.761993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.949060</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.761919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b16c701296423b8bad1d16fde9f364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df47bb5dc7e04bdaa9044f2ae4289d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.635242</td>\n",
       "      <td>0.757048</td>\n",
       "      <td>0.543654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.609300</td>\n",
       "      <td>0.571682</td>\n",
       "      <td>0.788281</td>\n",
       "      <td>0.643635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.550547</td>\n",
       "      <td>0.811222</td>\n",
       "      <td>0.663756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.314500</td>\n",
       "      <td>0.560315</td>\n",
       "      <td>0.818684</td>\n",
       "      <td>0.699381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.319300</td>\n",
       "      <td>0.520224</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.707680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.235600</td>\n",
       "      <td>0.630333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.729101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.597412</td>\n",
       "      <td>0.836926</td>\n",
       "      <td>0.727135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.600733</td>\n",
       "      <td>0.843836</td>\n",
       "      <td>0.743420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.734350</td>\n",
       "      <td>0.835268</td>\n",
       "      <td>0.724047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.689851</td>\n",
       "      <td>0.841072</td>\n",
       "      <td>0.741800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.767695</td>\n",
       "      <td>0.841625</td>\n",
       "      <td>0.744107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.744754</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.757648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.784815</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.749291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.865244</td>\n",
       "      <td>0.836926</td>\n",
       "      <td>0.736244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.832670</td>\n",
       "      <td>0.850193</td>\n",
       "      <td>0.766593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.863118</td>\n",
       "      <td>0.846048</td>\n",
       "      <td>0.749129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.804534</td>\n",
       "      <td>0.846048</td>\n",
       "      <td>0.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.881196</td>\n",
       "      <td>0.850746</td>\n",
       "      <td>0.756387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.884064</td>\n",
       "      <td>0.852681</td>\n",
       "      <td>0.767141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.902658</td>\n",
       "      <td>0.845495</td>\n",
       "      <td>0.754105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.027300</td>\n",
       "      <td>0.944625</td>\n",
       "      <td>0.844389</td>\n",
       "      <td>0.746309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.931960</td>\n",
       "      <td>0.851575</td>\n",
       "      <td>0.762686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.918587</td>\n",
       "      <td>0.848811</td>\n",
       "      <td>0.763223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.924388</td>\n",
       "      <td>0.849088</td>\n",
       "      <td>0.764520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.026300</td>\n",
       "      <td>0.940603</td>\n",
       "      <td>0.848259</td>\n",
       "      <td>0.755270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.026200</td>\n",
       "      <td>0.946995</td>\n",
       "      <td>0.850470</td>\n",
       "      <td>0.759917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.948630</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.761993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.949060</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.761919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881eef5373394b3898ce340aec84324f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ec47b22e3c54a698f08a2b347e1dd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.745300</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.751797</td>\n",
       "      <td>0.530414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.559690</td>\n",
       "      <td>0.785793</td>\n",
       "      <td>0.618274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.517305</td>\n",
       "      <td>0.810945</td>\n",
       "      <td>0.673343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.531577</td>\n",
       "      <td>0.816750</td>\n",
       "      <td>0.704208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.490225</td>\n",
       "      <td>0.830846</td>\n",
       "      <td>0.734873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.577135</td>\n",
       "      <td>0.836650</td>\n",
       "      <td>0.757835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.601515</td>\n",
       "      <td>0.846324</td>\n",
       "      <td>0.766156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.581894</td>\n",
       "      <td>0.839414</td>\n",
       "      <td>0.756904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>0.691071</td>\n",
       "      <td>0.842731</td>\n",
       "      <td>0.762634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.705849</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.757788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.688606</td>\n",
       "      <td>0.847153</td>\n",
       "      <td>0.769212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.858123</td>\n",
       "      <td>0.837479</td>\n",
       "      <td>0.743606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.807751</td>\n",
       "      <td>0.850470</td>\n",
       "      <td>0.781750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.767865</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.778864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.861917</td>\n",
       "      <td>0.849641</td>\n",
       "      <td>0.780982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.841585</td>\n",
       "      <td>0.849088</td>\n",
       "      <td>0.784782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.786707</td>\n",
       "      <td>0.855169</td>\n",
       "      <td>0.785818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.853872</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.783792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.883960</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.776586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.915139</td>\n",
       "      <td>0.853234</td>\n",
       "      <td>0.779167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.928057</td>\n",
       "      <td>0.851299</td>\n",
       "      <td>0.771001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.920030</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.777688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.926020</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.780641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.938773</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.783712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.952778</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.779747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.947806</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.772794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.958003</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.778624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.960989</td>\n",
       "      <td>0.854616</td>\n",
       "      <td>0.781801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88423e51721e4667815760f70511f27b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4784a04ed91f4b1c871c8268167ef56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.745300</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.751797</td>\n",
       "      <td>0.530414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.559690</td>\n",
       "      <td>0.785793</td>\n",
       "      <td>0.618274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.517305</td>\n",
       "      <td>0.810945</td>\n",
       "      <td>0.673343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.531577</td>\n",
       "      <td>0.816750</td>\n",
       "      <td>0.704208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.490225</td>\n",
       "      <td>0.830846</td>\n",
       "      <td>0.734873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.577135</td>\n",
       "      <td>0.836650</td>\n",
       "      <td>0.757835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.601515</td>\n",
       "      <td>0.846324</td>\n",
       "      <td>0.766156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.581894</td>\n",
       "      <td>0.839414</td>\n",
       "      <td>0.756904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>0.691071</td>\n",
       "      <td>0.842731</td>\n",
       "      <td>0.762634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.705849</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.757788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.688606</td>\n",
       "      <td>0.847153</td>\n",
       "      <td>0.769212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.858123</td>\n",
       "      <td>0.837479</td>\n",
       "      <td>0.743606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.807751</td>\n",
       "      <td>0.850470</td>\n",
       "      <td>0.781750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.767865</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.778864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.861917</td>\n",
       "      <td>0.849641</td>\n",
       "      <td>0.780982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.841585</td>\n",
       "      <td>0.849088</td>\n",
       "      <td>0.784782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.786707</td>\n",
       "      <td>0.855169</td>\n",
       "      <td>0.785818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.853872</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.783792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.883960</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.776586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.915139</td>\n",
       "      <td>0.853234</td>\n",
       "      <td>0.779167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.928057</td>\n",
       "      <td>0.851299</td>\n",
       "      <td>0.771001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.920030</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.777688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.926020</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.780641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.938773</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.783712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.952778</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.779747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.947806</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.772794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.958003</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.778624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.960989</td>\n",
       "      <td>0.854616</td>\n",
       "      <td>0.781801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "otmangi/MorrBERT, try:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at otmangi/MorrBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f3245230db430fb33f2749a43d17a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14469 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc65e4e50704b7aa1669e0e92946987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2270' max='2270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2270/2270 03:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.745300</td>\n",
       "      <td>0.642980</td>\n",
       "      <td>0.751797</td>\n",
       "      <td>0.530414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.632300</td>\n",
       "      <td>0.559690</td>\n",
       "      <td>0.785793</td>\n",
       "      <td>0.618274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.517305</td>\n",
       "      <td>0.810945</td>\n",
       "      <td>0.673343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.531577</td>\n",
       "      <td>0.816750</td>\n",
       "      <td>0.704208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.314100</td>\n",
       "      <td>0.490225</td>\n",
       "      <td>0.830846</td>\n",
       "      <td>0.734873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.577135</td>\n",
       "      <td>0.836650</td>\n",
       "      <td>0.757835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.136400</td>\n",
       "      <td>0.601515</td>\n",
       "      <td>0.846324</td>\n",
       "      <td>0.766156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.149700</td>\n",
       "      <td>0.581894</td>\n",
       "      <td>0.839414</td>\n",
       "      <td>0.756904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.109500</td>\n",
       "      <td>0.691071</td>\n",
       "      <td>0.842731</td>\n",
       "      <td>0.762634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.082100</td>\n",
       "      <td>0.705849</td>\n",
       "      <td>0.840796</td>\n",
       "      <td>0.757788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.688606</td>\n",
       "      <td>0.847153</td>\n",
       "      <td>0.769212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.858123</td>\n",
       "      <td>0.837479</td>\n",
       "      <td>0.743606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.064600</td>\n",
       "      <td>0.807751</td>\n",
       "      <td>0.850470</td>\n",
       "      <td>0.781750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.067100</td>\n",
       "      <td>0.767865</td>\n",
       "      <td>0.854339</td>\n",
       "      <td>0.778864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.054200</td>\n",
       "      <td>0.861917</td>\n",
       "      <td>0.849641</td>\n",
       "      <td>0.780982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.841585</td>\n",
       "      <td>0.849088</td>\n",
       "      <td>0.784782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>0.786707</td>\n",
       "      <td>0.855169</td>\n",
       "      <td>0.785818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.041500</td>\n",
       "      <td>0.853872</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.783792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.883960</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.776586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.915139</td>\n",
       "      <td>0.853234</td>\n",
       "      <td>0.779167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.027800</td>\n",
       "      <td>0.928057</td>\n",
       "      <td>0.851299</td>\n",
       "      <td>0.771001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.032600</td>\n",
       "      <td>0.920030</td>\n",
       "      <td>0.852957</td>\n",
       "      <td>0.777688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.926020</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.780641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.938773</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.783712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.952778</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.779747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.947806</td>\n",
       "      <td>0.848535</td>\n",
       "      <td>0.772794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.958003</td>\n",
       "      <td>0.852128</td>\n",
       "      <td>0.778624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.960989</td>\n",
       "      <td>0.854616</td>\n",
       "      <td>0.781801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SI2M-Lab/DarijaBERT</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.816701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alger-ia/dziribert</td>\n",
       "      <td>0.877557</td>\n",
       "      <td>0.807656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>faisalq/EgyBERT</td>\n",
       "      <td>0.893588</td>\n",
       "      <td>0.826880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>faisalq/SaudiBERT</td>\n",
       "      <td>0.908513</td>\n",
       "      <td>0.847212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>otmangi/MorRoBERTa</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>0.785324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>otmangi/MorrBERT</td>\n",
       "      <td>0.855169</td>\n",
       "      <td>0.785818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tunis-ai/TunBERT</td>\n",
       "      <td>0.602543</td>\n",
       "      <td>0.439308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model  Accuracy        F1\n",
       "0   SI2M-Lab/DarijaBERT  0.877557  0.816701\n",
       "3    alger-ia/dziribert  0.877557  0.807656\n",
       "6       faisalq/EgyBERT  0.893588  0.826880\n",
       "9     faisalq/SaudiBERT  0.908513  0.847212\n",
       "12   otmangi/MorRoBERTa  0.855721  0.785324\n",
       "15     otmangi/MorrBERT  0.855169  0.785818\n",
       "18     tunis-ai/TunBERT  0.602543  0.439308"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "fname = 'MAC_2'\n",
    "log_file = fname + '.txt'\n",
    "\n",
    "with open(log_file, 'w') as f:\n",
    "    f.write('Model,Accuracy,F1\\n')\n",
    "\n",
    "\n",
    "df = pd.read_csv('datasets/MAC corpus.csv', encoding='utf-8', engine='python') #, quotechar=\"'\"  , quoting=3\n",
    "\n",
    "\n",
    "      \n",
    "display(df.columns)\n",
    "display(df[:4])\n",
    "\n",
    "df['tweets'] = df['tweets'].astype('str')\n",
    "\n",
    "classes = set(df['type'].values)\n",
    "display(classes)\n",
    "\n",
    "c = df['type'].value_counts()\n",
    "display(c)\n",
    "\n",
    "df['type'] = df['type'].astype('category')\n",
    "df['label'] = df['type'].cat.codes\n",
    "\n",
    "df = df[['tweets', 'label']]\n",
    "classes_num = len(classes)\n",
    "display(classes_num)\n",
    "display(len(df))\n",
    "\n",
    "\n",
    "# ds = Dataset.from_pandas(df)\n",
    "# ds = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "# display(ds)\n",
    "\n",
    "max_sequence_length = 128\n",
    "\n",
    "\n",
    "models = [ \n",
    "        'faisalq/EgyBERT',            \n",
    "    'faisalq/SaudiBERT',            \n",
    "    'tunis-ai/TunBERT',\n",
    "    'alger-ia/dziribert',\n",
    "    'SI2M-Lab/DarijaBERT',\n",
    "    'otmangi/MorRoBERTa',\n",
    "    'otmangi/MorrBERT'\n",
    "            \n",
    "]\n",
    "\n",
    "\n",
    "seeds = [0, 1, 42]\n",
    "\n",
    "for model_name in models:\n",
    "    for seed in seeds:\n",
    "        ds = Dataset.from_pandas(df)\n",
    "        ds = ds.train_test_split(test_size=0.2, seed = seed)\n",
    "        if seed==0:\n",
    "            display(ds)\n",
    "        for i in range(3):\n",
    "            print(f'{model_name}, try:{i}')\n",
    "                  \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_name,\n",
    "                                                                  num_labels=classes_num).to('cuda')                                                 \n",
    "            dataset_train = ds['train']\n",
    "            dataset_validation = ds['test']                                                    \n",
    "            \n",
    "          \n",
    "    \n",
    "            def preprocess_function(examples):\n",
    "                return tokenizer(examples['tweets'], truncation=True, padding=\"max_length\",\n",
    "                                max_length=max_sequence_length)\n",
    "            \n",
    "            \n",
    "            dataset_train = dataset_train.map(preprocess_function, batched=True) # , batched=True\n",
    "            dataset_validation = dataset_validation.map(preprocess_function, batched=True)  # , batched=True\n",
    "            \n",
    "           \n",
    "            \n",
    "            def compute_metrics(eval_pred):\n",
    "                logits, labels = eval_pred\n",
    "                predictions = np.argmax(logits, axis=-1)    \n",
    "                acc = accuracy_score(labels, predictions)        \n",
    "                f1 = f1_score(labels, predictions, average='macro')   \n",
    "                with open(log_file, 'a') as f:\n",
    "                    f.write(f'{model_name},{acc},{f1}\\n')\n",
    "                return {'accuracy': acc, 'f1_score': f1}\n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "            epochs = 10\n",
    "            save_steps = 10000 #save checkpoint every 10000 steps\n",
    "            batch_size = 64\n",
    "            \n",
    "            training_args = TrainingArguments(\n",
    "                output_dir = 'bert/',\n",
    "                overwrite_output_dir=True,\n",
    "                num_train_epochs = epochs,\n",
    "                per_device_train_batch_size = batch_size,\n",
    "                per_device_eval_batch_size = batch_size,\n",
    "                save_steps = save_steps,\n",
    "                save_total_limit = 1, #only save the last 5 checkpoints\n",
    "                fp16=True,\n",
    "                learning_rate = 5e-5,  # 5e-5 is the default\n",
    "                logging_steps = 80, #50_000\n",
    "                evaluation_strategy = 'steps',\n",
    "                # evaluate_during_training = True,\n",
    "                eval_steps = 80\n",
    "                \n",
    "            )\n",
    "            \n",
    "            trainer = Trainer(\n",
    "                model = model,\n",
    "                args = training_args,\n",
    "                # data_collator=data_collator,\n",
    "                train_dataset=dataset_train,\n",
    "                eval_dataset=dataset_validation,\n",
    "                compute_metrics = compute_metrics\n",
    "            )\n",
    "            \n",
    "            \n",
    "            trainer.train()\n",
    "\n",
    "\n",
    "results = pd.read_csv(log_file)\n",
    "\n",
    "best_results = results.groupby('Model', as_index=False)['F1'].max()\n",
    "\n",
    "best_results = pd.merge(best_results, results, on=['Model', 'F1'])\n",
    "best_results = best_results[['Model', 'Accuracy', 'F1']]\n",
    "best_results = best_results.drop_duplicates()\n",
    "best_results.to_csv(f'{fname}.csv')\n",
    "display(best_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213ac86-934f-4e82-a949-0bcdcae2188d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220784d6-b06d-4429-adb8-0026654f9d49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203fa6b-c9d7-44a4-b501-a67bfd3e4ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8794b705-31a1-45d7-8e88-4017a9c282aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
